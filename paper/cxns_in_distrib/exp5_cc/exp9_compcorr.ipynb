{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-13T17:07:57.248372Z",
     "start_time": "2025-02-13T17:07:57.198400Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:07:57.352512Z",
     "start_time": "2025-02-13T17:07:57.331079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from proj.cxs_are_revealed.paper.data_config import Exp2Cogs\n",
    "# overall plan\n",
    "\n",
    "# dataset\n",
    "# - label which indexes\n",
    "\n",
    "# model -> get fills at position (deal with multitoken)\n",
    "# get likely fills + probability mass\n",
    "# sum probability mass on comp adjectives"
   ],
   "id": "e747138a2dcec070",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#",
   "id": "831d4dcd189cb673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:09.548500Z",
     "start_time": "2025-02-13T17:27:09.523731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read in cogs data for comp corr\n",
    "# note that we already processed in exp8_cogs\n",
    "from proj.cxs_are_revealed.paper.cxns_in_distrib.exp2_cogs import read_csv_row_by_row, get_all_data_clean, CogsEntry\n",
    "\n",
    "csv_data_clean = read_csv_row_by_row(Exp2Cogs.cogs_parsed_final)\n",
    "print(csv_data_clean[0])\n",
    "all_cogs_clean = get_all_data_clean(csv_data_clean, fix_punct_in_comp_corr=False)"
   ],
   "id": "c8276e25cb467182",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let Alone', 'Most wives are too bloody old, let alone mothers.', '[6, 7]', \"[(0, 'Most'), (1, 'wives'), (2, 'are'), (3, 'too'), (4, 'bloody'), (5, 'old,'), (6, 'let'), (7, 'alone'), (8, 'mothers.'), (9, '(FN'), (10, 'Construction)')]\", '']\n",
      "in sent Though punctuated by frequent flash-backs to the period before, during and just after the war, temporal progression in the present is clearly marked by the development of two narrative lines which weave their ways in and out of the novel., idx 33 != way\n",
      "note expected one error message bc one word is 'ways' instead of 'way'\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:10.115908Z",
     "start_time": "2025-02-13T17:27:10.086369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comp_corrs = [c for c in all_cogs_clean if c.cx_type == 'Comparative Correlative' ]\n",
    "print(len(comp_corrs))\n",
    "print(comp_corrs[0])"
   ],
   "id": "3e864d00ca582992",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "CogsEntry(id=156, cx_type='Comparative Correlative', sent='The harder they come, the harder they fall.', tgt_words=[0, 4], tgt_word_offsets=[(0, 3), (22, 25)])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:11.809542Z",
     "start_time": "2025-02-13T17:27:10.520948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from proj.cxs_are_revealed.paper.cxns_in_distrib.exp5_cc.exp9_cc_utils import is_comparative\n",
    "\n",
    "is_comparative(\"harder\")"
   ],
   "id": "70fc08d4afcfb33e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:12.059919Z",
     "start_time": "2025-02-13T17:27:11.842735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from proj.cxs_are_revealed.paper.cxns_in_distrib.exp5_cc.exp9_cc_utils import check_cc\n",
    "\n",
    "# check that after each \"the\" we do in fact have a comparative\n",
    "# this is also checked below; we wrote code twice to extract the comparative adjectives\n",
    "def check_all():\n",
    "    for cc in comp_corrs:\n",
    "        check_cc(cc)\n",
    "check_all()\n",
    "\n"
   ],
   "id": "37df670b99fcb83d",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:12.091784Z",
     "start_time": "2025-02-13T17:27:12.070645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lib.common.mlm_singleton import init_singleton_scorer\n",
    "mlm_scorer = init_singleton_scorer('roberta-large', output_attentions=False)\n",
    "\n"
   ],
   "id": "8b6df6be79394811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: singleton already initialized; will not re-init\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:12.131156Z",
     "start_time": "2025-02-13T17:27:12.102077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# todo\n",
    "# gpt o1 generated; probably duplicates some of our code somewhere else\n",
    "# goes from a sentence to list of words with char offsets\n",
    "@dataclass\n",
    "class WordAlignment:\n",
    "    word: str\n",
    "    word_idx: int\n",
    "    token_idxs: List[int]\n",
    "    tokens: List[str]\n",
    "    char_offset: Tuple[int, int]\n",
    "\n",
    "def get_word_token_alignments(sentence: str, tokenizer: PreTrainedTokenizerFast) -> List[WordAlignment]:\n",
    "    \"\"\"\n",
    "    Splits `sentence` on spaces and obtains token alignments via the provided fast tokenizer.\n",
    "    Returns a list of WordAlignment objects with character offsets, token indices, and token strings.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    token_ids = encoding[\"input_ids\"]\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(t_id) for t_id in token_ids]\n",
    "\n",
    "    word_alignments: List[WordAlignment] = []\n",
    "    words = sentence.split(\" \")\n",
    "\n",
    "    current_char_pos = 0\n",
    "    token_idx = 0\n",
    "\n",
    "    for w_idx, word in enumerate(words):\n",
    "        start_char = current_char_pos\n",
    "        end_char = start_char + len(word)\n",
    "\n",
    "        # Accumulate tokens that fully map to the current word\n",
    "        current_token_indices: List[int] = []\n",
    "        current_tokens: List[str] = []\n",
    "\n",
    "        while token_idx < len(offsets):\n",
    "            t_start, t_end = offsets[token_idx]\n",
    "            if t_start >= start_char and t_end <= end_char:\n",
    "                current_token_indices.append(token_idx)\n",
    "                current_tokens.append(tokens[token_idx])\n",
    "                token_idx += 1\n",
    "            else:\n",
    "                # If token is beyond the current word boundary, stop checking\n",
    "                if t_end > end_char:\n",
    "                    break\n",
    "                token_idx += 1\n",
    "\n",
    "        word_alignments.append(\n",
    "            WordAlignment(\n",
    "                word=word,\n",
    "                word_idx=w_idx,\n",
    "                token_idxs=current_token_indices,\n",
    "                tokens=current_tokens,\n",
    "                char_offset=(start_char, end_char)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        current_char_pos = end_char + 1  # move past the space\n",
    "\n",
    "    return word_alignments\n",
    "\n",
    "sent = \"This is a test sentence.\"\n",
    "get_word_token_alignments(sent, mlm_scorer.tokenizer)\n"
   ],
   "id": "293e1ce538d37095",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordAlignment(word='This', word_idx=0, token_idxs=[0], tokens=['This'], char_offset=(0, 4)),\n",
       " WordAlignment(word='is', word_idx=1, token_idxs=[1], tokens=['Ġis'], char_offset=(5, 7)),\n",
       " WordAlignment(word='a', word_idx=2, token_idxs=[2], tokens=['Ġa'], char_offset=(8, 9)),\n",
       " WordAlignment(word='test', word_idx=3, token_idxs=[3], tokens=['Ġtest'], char_offset=(10, 14)),\n",
       " WordAlignment(word='sentence.', word_idx=4, token_idxs=[4, 5], tokens=['Ġsentence', '.'], char_offset=(15, 24))]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:15.181728Z",
     "start_time": "2025-02-13T17:27:14.948268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from proj.cxs_are_revealed.paper.cxns_in_distrib.exp3_magpie import WrappedIdiomWord, EntryForProcessing, set_allow_deprecated\n",
    "from transformers import BatchEncoding\n",
    "from typing import List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CCEx:\n",
    "    id: int\n",
    "    sent: str\n",
    "    comp_words_idxs: List[int]\n",
    "    # todo: indexes or words\n",
    "\n",
    "# below adapted from exp8_cogs (which was in turn prev adapted from sth else)\n",
    "@dataclass\n",
    "class CCExOneWord:\n",
    "    # original sentence id in dataset\n",
    "    id: int = field(init=False)\n",
    "    sent: str = field(init=False)\n",
    "\n",
    "    # will be only one for each to make it simple\n",
    "    tgt_word_offsets: List[Tuple[int, int]] = field(default_factory=list)\n",
    "    words: List[str] = field(init=False)\n",
    "\n",
    "set_allow_deprecated()\n",
    "\n",
    "class Wrapper:\n",
    "    \"\"\" Adapted from MAGPIE_Wrapper\"\"\"\n",
    "    def __init__(self, cog_entry: CCExOneWord):\n",
    "        \"\"\"\n",
    "        Steps:\n",
    "        - get the total wordlist (contiguous non space strings)\n",
    "        - get the alphabetic wordlist (candidates for masking)\n",
    "        - get the list of idiom words (those with offsets)\n",
    "            (checks that idiom words do not have spaces in them)\n",
    "\n",
    "        \"\"\"\n",
    "        self.cog_entry = cog_entry\n",
    "\n",
    "        encoding = mlm_scorer.get_batch_encoding_for_sentence(cog_entry.sent)\n",
    "\n",
    "        # idiom token indices\n",
    "        self.idiom_token_list: List[WrappedIdiomWord] = self.get_idiom_token_list(encoding)\n",
    "\n",
    "        # later (other methods)\n",
    "        # produce a list of possible tokens / words to mask from a sentence\n",
    "\n",
    "    def get_idiom_token_list(self, encoding: BatchEncoding):\n",
    "        offset_to_token_list_map: List[WrappedIdiomWord] = []\n",
    "        for offset in self.cog_entry.tgt_word_offsets:\n",
    "            wrapped_idiom = WrappedIdiomWord.wrap_idiom_using_encoding(self.cog_entry, offset, encoding)\n",
    "            offset_to_token_list_map.append(wrapped_idiom)\n",
    "\n",
    "        assert len(offset_to_token_list_map) == len(self.cog_entry.tgt_word_offsets)\n",
    "        return offset_to_token_list_map\n",
    "\n",
    "def get_all_data_clean(all_comp: List[CogsEntry]):\n",
    "    ret_list: List[CCExOneWord] = []\n",
    "    for idx, cc_sent in enumerate(all_comp):\n",
    "        sent_words = cc_sent.sent.split(\" \")\n",
    "        # +1 to go from \"the\" => comparative adj\n",
    "        comparative_words = [sent_words[i+1] for i in cc_sent.tgt_words]\n",
    "        # assert(all([is_comparative_with_context(cc_sent.sent, offset) for offset in cc_sent.tgt_word_offsets]))\n",
    "\n",
    "        sent_word_alignments = get_word_token_alignments(cc_sent.sent, mlm_scorer.tokenizer)\n",
    "        tgts = [sent_word_alignments[i + 1] for i in cc_sent.tgt_words]\n",
    "        for t, w in zip(tgts, comparative_words):\n",
    "            assert t.word == w\n",
    "\n",
    "            cc_ex = CCExOneWord()\n",
    "            ret_list.append(cc_ex)\n",
    "            cc_ex.id = cc_sent.id\n",
    "            cc_ex.sent = cc_sent.sent\n",
    "            cc_ex.tgt_word_offsets = [t.char_offset]\n",
    "            cc_ex.words = [w]\n",
    "\n",
    "            assert cc_sent.sent[t.char_offset[0]:t.char_offset[1]] == w\n",
    "\n",
    "            # make sure original is comparative\n",
    "            assert is_comparative_with_context(cc_sent.sent, t.char_offset)\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "all_cc_exs = get_all_data_clean(comp_corrs)\n",
    "len(all_cc_exs)\n",
    "\n"
   ],
   "id": "7b0fc43fec802d8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:16.128499Z",
     "start_time": "2025-02-13T17:27:16.093494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_wrapped = [Wrapper(c) for c in all_cc_exs]\n",
    "len(all_wrapped)\n",
    "all"
   ],
   "id": "f3f1e4e2b728ece",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function all(iterable, /)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:16.840984Z",
     "start_time": "2025-02-13T17:27:16.813887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(all_wrapped[0].idiom_token_list)\n",
    "print(all_wrapped[0].cog_entry.sent)"
   ],
   "id": "fe9f1c6bde641c43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedIdiomWord(offset=(4, 10), idiom_word_chars='harder', tokens=[2], token_words=['harder'], is_multiple_tokens=False, tokens_do_not_exact_match=False)]\n",
      "The harder they come, the harder they fall.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:30.063703Z",
     "start_time": "2025-02-13T17:27:26.386201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# will calculate the total probability mass for comparative adjectives\n",
    "# 1. order by probability\n",
    "# get prob scores\n",
    "# while total prob is < value, check the next word\n",
    "# sum\n",
    "def score_distrib(cog_entry: CCExOneWord, logits: torch.Tensor, top_p=0.98) -> float:\n",
    "    # get sorted\n",
    "    logit_indices_sorted = torch.argsort(logits, descending=True)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    total_prob_mass = 0\n",
    "    comparative_prob_mass = 0\n",
    "    idx = 0\n",
    "    while total_prob_mass < top_p:\n",
    "        if idx > 100:\n",
    "            print(f\"idx is at 100 for {cog_entry.sent}\")\n",
    "            break\n",
    "        # loop iteration\n",
    "        orig_logit_idx = logit_indices_sorted[idx]\n",
    "        idx += 1    # increment for next\n",
    "        p = probs[orig_logit_idx]\n",
    "        total_prob_mass += p\n",
    "\n",
    "        # also accumulate in our tracker if it's comparative\n",
    "        word = mlm_scorer.tokenizer.decode([orig_logit_idx]).strip(\" \") # generally has a space\n",
    "        orig_sent = cog_entry.sent\n",
    "        sent_with_word = (\n",
    "                orig_sent[:cog_entry.tgt_word_offsets[0][0]] +\n",
    "                word +\n",
    "                orig_sent[cog_entry.tgt_word_offsets[0][1]:])\n",
    "        # print(orig_sent, \"=>\\n\\t\", sent_with_word)\n",
    "        new_offset_start = cog_entry.tgt_word_offsets[0][0]\n",
    "        new_offset_end = new_offset_start + len(word)\n",
    "        if is_comparative_with_context(sent_with_word, (new_offset_start, new_offset_end)):\n",
    "            comparative_prob_mass += p\n",
    "        # else:\n",
    "        #     print(f\"In {cog_entry.sent} \\n\\tnew word {word} not comparative\")\n",
    "\n",
    "    return comparative_prob_mass/ total_prob_mass\n",
    "\n",
    "# test that it does something else for an invalid one\n",
    "def test_distrib():\n",
    "    # sent = \"I want to get better at this game.\"\n",
    "    # sent = \"It is worth noting , however , that the more specific you are the better .\"\n",
    "    sent =  \"The higher up the nicer !\"\n",
    "    sent =  \"The bigger two fight the smaller cats.\"\n",
    "    sent =  \"The the two cats fight, the louder.\"\n",
    "    sent = \"The harder the two cats fight .....\"\n",
    "    sent = \"The harder two fight the cats .....\"\n",
    "    # sent = \"The harder the two cats fight.\"\n",
    "    sent = \"The harder two fight the cats.\"\n",
    "\n",
    "    # sent = \"This is a test.\"\n",
    "\n",
    "    sent_word_alignments = get_word_token_alignments(sent, mlm_scorer.tokenizer)\n",
    "    tgt = sent_word_alignments[1]\n",
    "    print(tgt)\n",
    "    cc_test = CCExOneWord()\n",
    "    # cc_test.id = 0\n",
    "    cc_test.id = 0\n",
    "    cc_test.sent = sent\n",
    "    cc_test.tgt_word_offsets = [tgt.char_offset]\n",
    "    cc_test.words = [\"harder\"]\n",
    "    # print(cc_test)\n",
    "    w = Wrapper(cc_test)\n",
    "    # print(w.idiom_token_list)\n",
    "    score = process_sent_exp8(w)\n",
    "    print(score)\n",
    "\n",
    "\n",
    "    # assert len(cog_entry_wrapped.idiom_token_list) == 1 # we put only one in each\n",
    "    # word_to_mask = cog_entry_wrapped.idiom_token_list[0]\n",
    "test_distrib()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "257db89935feda90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordAlignment(word='harder', word_idx=1, token_idxs=[1], tokens=['Ġharder'], char_offset=(4, 10))\n",
      "idx is at 100 for The harder two fight the cats.\n",
      "tensor(0.0162, device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:30.082311Z",
     "start_time": "2025-02-13T17:27:30.080803Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d6a943a33d19d802",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:38.045653Z",
     "start_time": "2025-02-13T17:27:30.086292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "# copied from exp8_cogs\n",
    "def process_sent_exp8(\n",
    "        cog_entry_wrapped: Wrapper\n",
    ") -> Optional[float]:\n",
    "    cog_entry = cog_entry_wrapped.cog_entry\n",
    "    entry_for_processing = EntryForProcessing(cog_entry.id, cog_entry.sent)\n",
    "\n",
    "    assert len(cog_entry_wrapped.idiom_token_list) == 1 # we put only one in each\n",
    "    word_to_mask = cog_entry_wrapped.idiom_token_list[0]\n",
    "    # print(word_to_mask)\n",
    "\n",
    "    if word_to_mask.is_multiple_tokens:\n",
    "        print(f\"Multi token: {word_to_mask}\\n {cog_entry.sent}\")\n",
    "        return None\n",
    "\n",
    "    assert word_to_mask.tokens_do_not_exact_match is False, f\"{word_to_mask}\"\n",
    "\n",
    "\n",
    "    t_idx = word_to_mask.tokens[0]\n",
    "    inputs_ids = entry_for_processing.get_inputs_with_mask_for_token_at_idx(\n",
    "        t_idx)\n",
    "    # get likely fills\n",
    "    outputs = mlm_scorer.get_model_outputs_for_input(inputs_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # predictions is vocab_len [logit, logit, ... logit]\n",
    "    token_logits = logits[0, t_idx]     # batch, idx, then vocab_len shape\n",
    "\n",
    "    # orig_token_id = entry_for_processing.input_ids[0, t_idx]\n",
    "    score = score_distrib(cog_entry, token_logits)\n",
    "\n",
    "    # print(cog_entry.sent, word_to_mask.idiom_word_chars)\n",
    "    # print(score)\n",
    "\n",
    "    return score\n",
    "\n",
    "def get_all_scores():\n",
    "    all_scores: List[float] = []\n",
    "    # for wrapped in tqdm(all_wrapped):\n",
    "    for wrapped in all_wrapped:\n",
    "        score = process_sent_exp8(wrapped)\n",
    "        all_scores.append(score)\n",
    "    return all_scores\n",
    "\n",
    "all_scores = get_all_scores()\n"
   ],
   "id": "d7e0bae2d6b8ce55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi token: WrappedIdiomWord(offset=(13, 21), idiom_word_chars='merrier.', tokens=[4, 5, 6], token_words=['mer', 'rier', '.'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " The more the merrier.\n",
      "Multi token: WrappedIdiomWord(offset=(29, 37), idiom_word_chars='better,\"', tokens=[9, 10], token_words=['better', ',\"'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " \" The more that is done, the better,\" he told reporters, while promising more money for disaster preparedness.\n",
      "Multi token: WrappedIdiomWord(offset=(29, 36), idiom_word_chars='better.', tokens=[7, 8], token_words=['better', '.'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " The more wealth created, the better.\n",
      "Multi token: WrappedIdiomWord(offset=(53, 60), idiom_word_chars='better.', tokens=[15, 16], token_words=['better', '.'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " All I know, is the more I see her on television, the better.\n",
      "Multi token: WrappedIdiomWord(offset=(53, 59), idiom_word_chars='wetter', tokens=[13, 14], token_words=['wet', 'ter'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " I thought the Rio Grande was often dry--and that the wetter it was, the lower the levels of smuggling both animate and inanimate contraband.\n",
      "Multi token: WrappedIdiomWord(offset=(45, 51), idiom_word_chars='madder', tokens=[13, 14], token_words=['mad', 'der'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " At age 56, the more I think about it and the madder I get, to hell with the SC rulings.\n",
      "idx is at 100 for The higher up the nicer !\n",
      "Multi token: WrappedIdiomWord(offset=(29, 36), idiom_word_chars='better.', tokens=[8, 9], token_words=['better', '.'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " The sooner it is tested, the better.\n",
      "Multi token: WrappedIdiomWord(offset=(4, 10), idiom_word_chars='denser', tokens=[2, 3], token_words=['dens', 'er'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " The denser the matter, the more curvature.\n",
      "Multi token: WrappedIdiomWord(offset=(35, 42), idiom_word_chars='better.', tokens=[9, 10], token_words=['better', '.'], is_multiple_tokens=True, tokens_do_not_exact_match=False)\n",
      " The less elaborate you can be, the better.\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis",
   "id": "5b20c64785c29a47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T17:27:54.705417Z",
     "start_time": "2025-02-13T17:27:54.604900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(all_scores)) # 108, as expected, 54*2\n",
    "\n",
    "# 9 failed\n",
    "print(len([s for s in all_scores if s is None]))\n",
    "\n",
    "# how many have less than perfect score? - 4\n",
    "print(len([s for s in all_scores if s is not None and s < 1]))\n",
    "\n",
    "# look at the ones with score less than 100% (there are 4)\n",
    "low = [(i, s) for i, s in enumerate(all_scores) if s is not None and s < 1]\n",
    "for i,s in low:\n",
    "    print(all_cc_exs[i])\n",
    "    print(s)\n"
   ],
   "id": "fd473c57255b2f8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "9\n",
      "4\n",
      "CCExOneWord(id=190, sent='The higher up the nicer !', tgt_word_offsets=[(18, 23)], words=['nicer'])\n",
      "tensor(0.8603, device='mps:0')\n",
      "CCExOneWord(id=193, sent='It is worth noting , however , that the more specific you are the better .', tgt_word_offsets=[(66, 72)], words=['better'])\n",
      "tensor(0.9961, device='mps:0')\n",
      "CCExOneWord(id=199, sent='The more of them you see, the cheaper it is.', tgt_word_offsets=[(30, 37)], words=['cheaper'])\n",
      "tensor(0.9961, device='mps:0')\n",
      "CCExOneWord(id=202, sent='The sooner it is tested, the better.', tgt_word_offsets=[(4, 10)], words=['sooner'])\n",
      "tensor(0.9903, device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
