{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-22T18:30:54.940067Z",
     "start_time": "2025-04-22T18:30:54.929082Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T18:31:00.707729Z",
     "start_time": "2025-04-22T18:30:54.943472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from proj.cxs_are_revealed.paper.proj_common.npns import get_npn_data, compute_scores\n",
    "from lib.common.mlm_singleton import init_singleton_scorer\n",
    "\n",
    "mlm_scorer = init_singleton_scorer('roberta-large')\n"
   ],
   "id": "fab13390dd528b83",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Initializing roberta-large\n",
      "WARNING:root:INIT\n",
      "WARNING:root:Will use cache for MLMScorer. Use this for analysis.For computation, avoid this; memory will blow up.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T00:50:59.826975Z",
     "start_time": "2025-04-23T00:50:34.878264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from proj.cxs_are_revealed.paper.proj_common.npn_dataset_generation.npn_utils import GPTOutput\n",
    "from typing import List\n",
    "from proj.cxs_are_revealed.paper.data_config import Exp4NPN\n",
    "\n",
    "all_npns: List[GPTOutput] = get_npn_data(\n",
    "    Exp4NPN.npn_gpt_outputs_v2_fixed,\n",
    "    # Exp6NPN.npn_gpt_outputs,\n",
    "    # Exp6NPN.npn_acceptability_ratings_csv,\n",
    "    None,\n",
    "    # None,\n",
    "    output_has_noun_rep=False,\n",
    ")\n",
    "all_npns_target_preps = [\n",
    "    # x for x in all_npns if x.prep in ['upon', 'after']\n",
    "    x for x in all_npns\n",
    "]\n",
    "scores, ct_err, ct_multi, npn_results = compute_scores(\n",
    "    all_npns_target_preps,\n",
    "    allow_case_mismatch=True\n",
    ")\n"
   ],
   "id": "4f5ddc20a5da9ffd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "notify (non error): more than 2 occurrences! asteroid The explorer traveled from asteroid to asteroid, seeking valuable minerals in the vast expanse of the asteroid belt. \n",
      "\n",
      "notify (non error): more than 2 occurrences! asteroid The spacecraft maneuvered through the dense asteroid belt, dodging asteroid upon asteroid in its daring journey. \n",
      "\n",
      "notify (non error): more than 2 occurrences! interior As an interior designer, she meticulously worked through interior after interior, bringing each one to life with her unique style. \n",
      "\n",
      "notify (non error): more than 2 occurrences! tow As the snowstorm raged on, the tow trucks worked relentlessly, pulling out tow after tow from the heaps of snow. \n",
      "\n",
      "notify (non error): more than 2 occurrences! tow The seasoned tow truck driver cleared the pileup on the highway, moving the vehicles tow by tow until the road was clear. \n",
      "\n",
      "notify (non error): more than 2 occurrences! comp As he delved deeper into his programming career, he found himself working on comp upon comp, creating complex systems and coding structures. \n",
      "\n",
      "notify (non error): more than 2 occurrences! may His garden bloomed from May to May, showcasing a year-round explosion of color and fragrance. \n",
      "\n",
      "In filter outputs, printing filtered\n",
      "For the preposition category, it's count where desired NPN string did not occur\n",
      "Counter()\n",
      "after filtering, 392\n",
      "392\n",
      "0 0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T00:51:01.656757Z",
     "start_time": "2025-04-23T00:51:01.618834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import statistics\n",
    "for k in scores.keys():\n",
    "    # print(sorted(map(lambda x: round(x, 2), scores[k])))\n",
    "    print(k)\n",
    "    print(statistics.mean(scores[k]))\n"
   ],
   "id": "111630aff071c6f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "0.526854371359073\n",
      "after\n",
      "0.8940689305765438\n",
      "by\n",
      "0.7048931956308235\n",
      "upon\n",
      "0.9033559047273882\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T16:49:11.748683Z",
     "start_time": "2025-04-23T16:49:11.654211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "from proj.cxs_are_revealed.paper.cxns_in_distrib import load_dict_from_file\n",
    "from collections import defaultdict\n",
    "\n",
    "# infinigram_dict: Dict[str, int] = load_dict_from_file(Exp6NPN.npn_infingram)\n",
    "infinigram_dict: Dict[str, int] = load_dict_from_file(Exp4NPN.npn_roberta_infinigram)\n",
    "ctr = defaultdict(list)\n",
    "for npn_res in npn_results:\n",
    "    phrase = npn_res.orig_gpt.noun + \" \" + npn_res.orig_gpt.prep + \" \" + npn_res.orig_gpt.noun\n",
    "    ct = infinigram_dict[phrase]\n",
    "    if ct > 0: continue\n",
    "    ctr[npn_res.prep].append( npn_res.score)\n",
    "# pp(ctr)\n",
    "\n",
    "for k in ctr.keys():\n",
    "    print(k)\n",
    "    print(statistics.mean(ctr[k]))"
   ],
   "id": "340ce7e95f2495c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upon\n",
      "0.8739478079924764\n",
      "after\n",
      "0.7057107051375299\n",
      "by\n",
      "0.5855863880270794\n",
      "to\n",
      "0.16704497863115234\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eee8100f6f1f7fd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T00:52:08.692176Z",
     "start_time": "2025-04-23T00:52:08.645642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pp\n",
    "\n",
    "# i = 0\n",
    "# for r in all_npns_target_preps:\n",
    "#     prep = r.prep\n",
    "#     if prep in ['upon', 'after']: continue\n",
    "#     for i in range(0,2):\n",
    "#         s = scores[prep][i]\n",
    "#         if s > 0.95: continue\n",
    "#         print(r.output)\n",
    "#         i += 1\n",
    "#         print(s)\n",
    "for res in npn_results:\n",
    "    if res.score < 0.95:\n",
    "        pp(res._asdict())\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "5e4969d1cbcf413d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='They lived their lives real to real, always '\n",
      "                              'being true to themselves and each other.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'they lived their lives real to real , always being true to '\n",
      "                   'themselves and each other .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.04782165586948395,\n",
      " 'fills': [' true', ' close', ' honest', ' real', ' authentic']}\n",
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='They lived their lives real to real, always '\n",
      "                              'being true to themselves and each other.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'they lived their lives real to real , always being true to '\n",
      "                   'themselves and each other .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.011526688002049923,\n",
      " 'fills': [' themselves', ' God', ' life', ' everyone', ' us']}\n",
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He unveiled his collection, real after real, '\n",
      "                              'each one more authentic and fascinating than '\n",
      "                              'the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he unveiled his collection , real after real , each one '\n",
      "                   'more authentic and fascinating than the last .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.6220978498458862,\n",
      " 'fills': [' real', ' one', ' Real', ' right', 'real']}\n",
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He unveiled his collection, real after real, '\n",
      "                              'each one more authentic and fascinating than '\n",
      "                              'the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he unveiled his collection , real after real , each one '\n",
      "                   'more authentic and fascinating than the last .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.6672893762588501,\n",
      " 'fills': [' real', 'real', ' another', ' reality', ' one']}\n",
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As they navigated through the virtual world, '\n",
      "                              'they began to discover its nuances, exploring '\n",
      "                              'it real by real.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as they navigated through the virtual world , they began '\n",
      "                   'to discover its nuances , exploring it real by real .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.5091049075126648,\n",
      " 'fills': [' real', ' one', ' virtual', ' little', ' step']}\n",
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As an avid film collector, he kept stacking '\n",
      "                              'real upon real in his basement, creating a '\n",
      "                              'cinematic labyrinth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as an avid film collector , he kept stacking real upon '\n",
      "                   'real in his basement , creating a cinematic labyrinth .',\n",
      " 'prep': 'upon',\n",
      " 'score': 2.8599527013284387e-06,\n",
      " 'fills': [' film', ' films', ' reel', ' them', ' movies']}\n",
      "{'orig_gpt': GPTOutput(noun='real',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As an avid film collector, he kept stacking '\n",
      "                              'real upon real in his basement, creating a '\n",
      "                              'cinematic labyrinth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as an avid film collector , he kept stacking real upon '\n",
      "                   'real in his basement , creating a cinematic labyrinth .',\n",
      " 'prep': 'upon',\n",
      " 'score': 2.1332737105694832e-06,\n",
      " 'fills': [' film', ' reel', ' movie', ' films', ' video']}\n",
      "{'orig_gpt': GPTOutput(noun='tile',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked meticulously, placing the intricate '\n",
      "                              'mosaic tile to tile until the entire bathroom '\n",
      "                              'floor was covered.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked meticulously , placing the intricate mosaic tile '\n",
      "                   'to tile until the entire bathroom floor was covered .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.09114684164524078,\n",
      " 'fills': [' tiles', ' on', ' next', ' tile', ' pattern']}\n",
      "{'orig_gpt': GPTOutput(noun='tile',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked meticulously, placing the intricate '\n",
      "                              'mosaic tile to tile until the entire bathroom '\n",
      "                              'floor was covered.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked meticulously , placing the intricate mosaic tile '\n",
      "                   'to tile until the entire bathroom floor was covered .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.000563575595151633,\n",
      " 'fills': [' match', ' perfection', ' order', 'pper', ' place']}\n",
      "{'orig_gpt': GPTOutput(noun='tile',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The artist meticulously placed tile upon tile, '\n",
      "                              'creating a stunning mosaic masterpiece.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the artist meticulously placed tile upon tile , creating a '\n",
      "                   'stunning mosaic masterpiece .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.856589138507843,\n",
      " 'fills': [' tile', ' tiles', ' glass', ' stone', ' brick']}\n",
      "{'orig_gpt': GPTOutput(noun='pen',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He laboriously compared each ink color, going '\n",
      "                              'from pen to pen until he found the perfect '\n",
      "                              'shade.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he laboriously compared each ink color , going from pen to '\n",
      "                   'pen until he found the perfect shade .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9178475737571716,\n",
      " 'fills': [' pen', ' pencil', ' brush', ' ink', ' paper']}\n",
      "{'orig_gpt': GPTOutput(noun='pen',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He carefully organized his collection, sorting '\n",
      "                              'them pen by pen according to their type and '\n",
      "                              'age.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he carefully organized his collection , sorting them pen '\n",
      "                   'by pen according to their type and age .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7586814761161804,\n",
      " 'fills': [' pen', ' pens', ' all', ' out', ' alphabet']}\n",
      "{'orig_gpt': GPTOutput(noun='pen',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He carefully organized his collection, sorting '\n",
      "                              'them pen by pen according to their type and '\n",
      "                              'age.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he carefully organized his collection , sorting them pen '\n",
      "                   'by pen according to their type and age .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8783482909202576,\n",
      " 'fills': [' pen', ' pens', ' ink', ' color', ' pencil']}\n",
      "{'orig_gpt': GPTOutput(noun='odds',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He stacked his life, odds to odds, on the '\n",
      "                              \"chance of his company's success.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he stacked his life , odds to odds , on the chance of his '\n",
      "                   \"company 's success .\",\n",
      " 'prep': 'to',\n",
      " 'score': 9.781643893802539e-05,\n",
      " 'fills': [' according', ' contrary', ' playing', ' up', ' relative']}\n",
      "{'orig_gpt': GPTOutput(noun='odds',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He stacked his life, odds to odds, on the '\n",
      "                              \"chance of his company's success.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he stacked his life , odds to odds , on the chance of his '\n",
      "                   \"company 's success .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.005390223581343889,\n",
      " 'fills': [' none', ' nil', ' death', ' nothing', ' perfection']}\n",
      "{'orig_gpt': GPTOutput(noun='odds',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Despite facing odds after odds, the determined '\n",
      "                              'athlete continued to train, unwavering in his '\n",
      "                              'pursuit of excellence.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'despite facing odds after odds , the determined athlete '\n",
      "                   'continued to train , unwavering in his pursuit of '\n",
      "                   'excellence .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.9142917990684509,\n",
      " 'fills': [' odds', ' obstacle', ' obstacles', ' setback', ' hurdles']}\n",
      "{'orig_gpt': GPTOutput(noun='odds',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Despite his consistent losing streak, he '\n",
      "                              'continued to bet, going through odds by odds, '\n",
      "                              'hoping for a big win someday.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'despite his consistent losing streak , he continued to bet '\n",
      "                   ', going through odds by odds , hoping for a big win '\n",
      "                   'someday .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.12758409976959229,\n",
      " 'fills': [' games', ' odds', ' books', ' sports', ' tournaments']}\n",
      "{'orig_gpt': GPTOutput(noun='odds',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Despite his consistent losing streak, he '\n",
      "                              'continued to bet, going through odds by odds, '\n",
      "                              'hoping for a big win someday.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'despite his consistent losing streak , he continued to bet '\n",
      "                   ', going through odds by odds , hoping for a big win '\n",
      "                   'someday .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.21433164179325104,\n",
      " 'fills': [' odds', ' hand', ' day', ' one', ' numbers']}\n",
      "{'orig_gpt': GPTOutput(noun='acronym',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he studied for his medical exam, he had to '\n",
      "                              'learn the meanings and applications of the '\n",
      "                              'material acronym by acronym.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he studied for his medical exam , he had to learn the '\n",
      "                   'meanings and applications of the material acronym by '\n",
      "                   'acronym .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.13127030432224274,\n",
      " 'fills': [' word', ' alphabet', ' acronym', ',', ' letter']}\n",
      "{'orig_gpt': GPTOutput(noun='acronym',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he studied for his medical exam, he had to '\n",
      "                              'learn the meanings and applications of the '\n",
      "                              'material acronym by acronym.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he studied for his medical exam , he had to learn the '\n",
      "                   'meanings and applications of the material acronym by '\n",
      "                   'acronym .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0007313067326322198,\n",
      " 'fills': [' word', ' paragraph', ' one', ' example', ' little']}\n",
      "{'orig_gpt': GPTOutput(noun='legislation',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The government's policies shifted drastically \"\n",
      "                              'from legislation to legislation, causing '\n",
      "                              'confusion among the public.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the government 's policies shifted drastically from \"\n",
      "                   'legislation to legislation , causing confusion among the '\n",
      "                   'public .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0018807919695973396,\n",
      " 'fills': [' rhetoric', ' promises', ' policy', ' consultation', ' advisory']}\n",
      "{'orig_gpt': GPTOutput(noun='legislation',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The government's policies shifted drastically \"\n",
      "                              'from legislation to legislation, causing '\n",
      "                              'confusion among the public.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the government 's policies shifted drastically from \"\n",
      "                   'legislation to legislation , causing confusion among the '\n",
      "                   'public .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.019913895055651665,\n",
      " 'fills': [' policy',\n",
      "           ' rhetoric',\n",
      "           ' implementation',\n",
      "           ' regulation',\n",
      "           ' consultation']}\n",
      "{'orig_gpt': GPTOutput(noun='legislation',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Despite passing legislation after legislation, '\n",
      "                              'the government was unable to effectively '\n",
      "                              \"address the country's economic crisis.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'despite passing legislation after legislation , the '\n",
      "                   'government was unable to effectively address the country '\n",
      "                   \"'s economic crisis .\",\n",
      " 'prep': 'after',\n",
      " 'score': 9.253114694729447e-05,\n",
      " 'fills': [' 2008', ' 2010', ' 2011', ' elections', ' 2009']}\n",
      "{'orig_gpt': GPTOutput(noun='legislation',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The government sought to improve the nation's \"\n",
      "                              'healthcare system, legislation by legislation, '\n",
      "                              'until every citizen had access to affordable '\n",
      "                              'care.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the government sought to improve the nation 's healthcare \"\n",
      "                   'system , legislation by legislation , until every citizen '\n",
      "                   'had access to affordable care .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.20661814510822296,\n",
      " 'fills': [' legislation', ' primarily', ' largely', ' mostly', ' including']}\n",
      "{'orig_gpt': GPTOutput(noun='climax',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In her career as a suspense novelist, she '\n",
      "                              'painstakingly crafted her narratives from '\n",
      "                              'climax to climax, ensuring her readers were '\n",
      "                              'always on the edge of their seats.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in her career as a suspense novelist , she painstakingly '\n",
      "                   'crafted her narratives from climax to climax , ensuring '\n",
      "                   'her readers were always on the edge of their seats .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0011249782983213663,\n",
      " 'fills': [' beginning',\n",
      "           ' inception',\n",
      "           ' introduction',\n",
      "           ' start',\n",
      "           ' conception']}\n",
      "{'orig_gpt': GPTOutput(noun='climax',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In her career as a suspense novelist, she '\n",
      "                              'painstakingly crafted her narratives from '\n",
      "                              'climax to climax, ensuring her readers were '\n",
      "                              'always on the edge of their seats.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in her career as a suspense novelist , she painstakingly '\n",
      "                   'crafted her narratives from climax to climax , ensuring '\n",
      "                   'her readers were always on the edge of their seats .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.22138339281082153,\n",
      " 'fills': [' climax', ' resolution', ' conclusion', ' end', ' finale']}\n",
      "{'orig_gpt': GPTOutput(noun='climax',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the novel progressed, she found herself '\n",
      "                              'drawn in deeper, experiencing the thrilling '\n",
      "                              'story climax by climax.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the novel progressed , she found herself drawn in '\n",
      "                   'deeper , experiencing the thrilling story climax by climax '\n",
      "                   '.',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7686843872070312,\n",
      " 'fills': [' climax', ' followed', ' conclusion', ' gripped', ' moment']}\n",
      "{'orig_gpt': GPTOutput(noun='climax',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the novel progressed, she found herself '\n",
      "                              'drawn in deeper, experiencing the thrilling '\n",
      "                              'story climax by climax.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the novel progressed , she found herself drawn in '\n",
      "                   'deeper , experiencing the thrilling story climax by climax '\n",
      "                   '.',\n",
      " 'prep': 'by',\n",
      " 'score': 0.19421294331550598,\n",
      " 'fills': [' herself', ' climax', ' itself', ' heart', ' surprise']}\n",
      "{'orig_gpt': GPTOutput(noun='stink',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked in the waste management facility, '\n",
      "                              'going from stink to stink without a complaint.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked in the waste management facility , going from '\n",
      "                   'stink to stink without a complaint .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.49274909496307373,\n",
      " 'fills': [' stink', ' clean', ' smell', ' poop', ' shine']}\n",
      "{'orig_gpt': GPTOutput(noun='stink',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked in the waste management facility, '\n",
      "                              'going from stink to stink without a complaint.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked in the waste management facility , going from '\n",
      "                   'stink to stink without a complaint .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.5819262862205505,\n",
      " 'fills': [' stink', ' smell', ' clean', ' shine', ' odor']}\n",
      "{'orig_gpt': GPTOutput(noun='stink',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The garbage piled up in the alley, growing '\n",
      "                              'worse stink by stink, until the city finally '\n",
      "                              'intervened.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the garbage piled up in the alley , growing worse stink by '\n",
      "                   'stink , until the city finally intervened .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.062324944883584976,\n",
      " 'fills': [',', ' day', ' smell', ' stink', ' only']}\n",
      "{'orig_gpt': GPTOutput(noun='stink',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The garbage piled up in the alley, growing '\n",
      "                              'worse stink by stink, until the city finally '\n",
      "                              'intervened.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the garbage piled up in the alley , growing worse stink by '\n",
      "                   'stink , until the city finally intervened .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.06526540964841843,\n",
      " 'fills': [' day', ' stink', ' week', ' night', ' year']}\n",
      "{'orig_gpt': GPTOutput(noun='myriad',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The stars in the night sky, myriad to myriad, '\n",
      "                              'stretched out in an endless expanse of '\n",
      "                              'twinkling light.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the stars in the night sky , myriad to myriad , stretched '\n",
      "                   'out in an endless expanse of twinkling light .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0001059167116181925,\n",
      " 'fills': [' estimated', ' believed', ' equal', ' said', ' thought']}\n",
      "{'orig_gpt': GPTOutput(noun='myriad',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The stars in the night sky, myriad to myriad, '\n",
      "                              'stretched out in an endless expanse of '\n",
      "                              'twinkling light.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the stars in the night sky , myriad to myriad , stretched '\n",
      "                   'out in an endless expanse of twinkling light .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0010085783433169127,\n",
      " 'fills': [' millions', ' infinity', ' thousands', ' count', ' infinite']}\n",
      "{'orig_gpt': GPTOutput(noun='myriad',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The archaeologists continued to uncover, myriad '\n",
      "                              'after myriad, ancient artifacts from the '\n",
      "                              'excavation site.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the archaeologists continued to uncover , myriad after '\n",
      "                   'myriad , ancient artifacts from the excavation site .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.937535285949707,\n",
      " 'fills': [' myriad', ' marvel', ' minute', ' mound', ' one']}\n",
      "{'orig_gpt': GPTOutput(noun='myriad',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The stars in the clear night sky seem to '\n",
      "                              'multiply, myriad by myriad, in a vast celestial '\n",
      "                              'spectacle.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the stars in the clear night sky seem to multiply , myriad '\n",
      "                   'by myriad , in a vast celestial spectacle .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9162865281105042,\n",
      " 'fills': [' myriad', ' innumerable', ' minute', ' multiple', ' million']}\n",
      "{'orig_gpt': GPTOutput(noun='stake',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He built the garden fence meticulously, '\n",
      "                              'positioning it stake to stake with precision '\n",
      "                              'and care.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he built the garden fence meticulously , positioning it '\n",
      "                   'stake to stake with precision and care .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.926015317440033,\n",
      " 'fills': [' stake', ' pillar', ' pin', ' post', ' according']}\n",
      "{'orig_gpt': GPTOutput(noun='stake',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Building the fence, he measured and hammered, '\n",
      "                              'inserting each piece of wood stake by stake, '\n",
      "                              'ensuring a strong and sturdy barrier.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'building the fence , he measured and hammered , inserting '\n",
      "                   'each piece of wood stake by stake , ensuring a strong and '\n",
      "                   'sturdy barrier .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7927773594856262,\n",
      " 'fills': [' stake', ' pin', ' point', ' pillar', ' side']}\n",
      "{'orig_gpt': GPTOutput(noun='stake',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Building the fence, he measured and hammered, '\n",
      "                              'inserting each piece of wood stake by stake, '\n",
      "                              'ensuring a strong and sturdy barrier.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'building the fence , he measured and hammered , inserting '\n",
      "                   'each piece of wood stake by stake , ensuring a strong and '\n",
      "                   'sturdy barrier .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7991821765899658,\n",
      " 'fills': [' stake', ' pin', ' itself', ' hand', ' nail']}\n",
      "{'orig_gpt': GPTOutput(noun='imagination',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='We communicated not through words, but '\n",
      "                              'imagination to imagination, sharing ideas and '\n",
      "                              'concepts in ways language could never express.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'we communicated not through words , but imagination to '\n",
      "                   'imagination , sharing ideas and concepts in ways language '\n",
      "                   'could never express .',\n",
      " 'prep': 'to',\n",
      " 'score': 1.981123205041513e-06,\n",
      " 'fills': [' according', ' through', ' thanks', ' appealed', ' due']}\n",
      "{'orig_gpt': GPTOutput(noun='imagination',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='We communicated not through words, but '\n",
      "                              'imagination to imagination, sharing ideas and '\n",
      "                              'concepts in ways language could never express.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'we communicated not through words , but imagination to '\n",
      "                   'imagination , sharing ideas and concepts in ways language '\n",
      "                   'could never express .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8298884034156799,\n",
      " 'fills': [' imagination', ' mind', ' reality', ' thought', ' them']}\n",
      "{'orig_gpt': GPTOutput(noun='imagination',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The artist transformed the plain canvas into a '\n",
      "                              'vibrant masterpiece, imagination by '\n",
      "                              'imagination, until the entire studio was filled '\n",
      "                              'with his beautiful creations.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the artist transformed the plain canvas into a vibrant '\n",
      "                   'masterpiece , imagination by imagination , until the '\n",
      "                   'entire studio was filled with his beautiful creations .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0002466549340169877,\n",
      " 'fills': [' inspired', ' fueled', ' driven', ' fuelled', ' guided']}\n",
      "{'orig_gpt': GPTOutput(noun='director',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The feedback from the film festival was passed '\n",
      "                              'along, director to director, ensuring everyone '\n",
      "                              'had a chance to improve for the next year.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the feedback from the film festival was passed along , '\n",
      "                   'director to director , ensuring everyone had a chance to '\n",
      "                   'improve for the next year .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9365967512130737,\n",
      " 'fills': [' director', ' direct', ' from', 'director', ' directly']}\n",
      "{'orig_gpt': GPTOutput(noun='rating',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a film critic, I scrutinize each movie I '\n",
      "                              'watch, comparing them meticulously from rating '\n",
      "                              'to rating.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a film critic , i scrutinize each movie i watch , '\n",
      "                   'comparing them meticulously from rating to rating .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.09562017023563385,\n",
      " 'fills': [' plot', ' rating', ' genre', ' review', ' script']}\n",
      "{'orig_gpt': GPTOutput(noun='rating',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a film critic, I scrutinize each movie I '\n",
      "                              'watch, comparing them meticulously from rating '\n",
      "                              'to rating.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a film critic , i scrutinize each movie i watch , '\n",
      "                   'comparing them meticulously from rating to rating .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.6435508131980896,\n",
      " 'fills': [' rating', ' review', ' ratings', ' reviews', ' plot']}\n",
      "{'orig_gpt': GPTOutput(noun='rating',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Despite facing harsh criticism, the movie '\n",
      "                              'continued to receive rating after rating, '\n",
      "                              'solidifying its unexpected popularity among the '\n",
      "                              'audience.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'despite facing harsh criticism , the movie continued to '\n",
      "                   'receive rating after rating , solidifying its unexpected '\n",
      "                   'popularity among the audience .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.9486333131790161,\n",
      " 'fills': [' rating', ' ratings', 'rating', ' ranking', ' award']}\n",
      "{'orig_gpt': GPTOutput(noun='rating',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As she scrolled through the customer feedback, '\n",
      "                              'she meticulously analyzed each comment, rating '\n",
      "                              'by rating, to understand the areas they needed '\n",
      "                              'to improve in their service.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as she scrolled through the customer feedback , she '\n",
      "                   'meticulously analyzed each comment , rating by rating , to '\n",
      "                   'understand the areas they needed to improve in their '\n",
      "                   'service .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8986158967018127,\n",
      " 'fills': [' rating', ' rated', ' ranking', ' ranked', ' ratings']}\n",
      "{'orig_gpt': GPTOutput(noun='joy',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Her life was a constant progression, moving '\n",
      "                              'from joy to joy, each experience more '\n",
      "                              'fulfilling than the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'her life was a constant progression , moving from joy to '\n",
      "                   'joy , each experience more fulfilling than the last .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.08515013754367828,\n",
      " 'fills': [' sorrow', ' pain', ' sadness', ' happiness', ' joy']}\n",
      "{'orig_gpt': GPTOutput(noun='joy',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Her life was a constant progression, moving '\n",
      "                              'from joy to joy, each experience more '\n",
      "                              'fulfilling than the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'her life was a constant progression , moving from joy to '\n",
      "                   'joy , each experience more fulfilling than the last .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.26198992133140564,\n",
      " 'fills': [' happiness', ' joy', ' sorrow', ' pain', ' sadness']}\n",
      "{'orig_gpt': GPTOutput(noun='joy',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As she navigated through her recovery, she '\n",
      "                              'began to heal, joy by joy, reclaiming her life '\n",
      "                              'with each passing day.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as she navigated through her recovery , she began to heal '\n",
      "                   ', joy by joy , reclaiming her life with each passing day .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.00010030517296399921,\n",
      " 'fills': [' surrounded', ' fueled', ' replaced', ' consumed', ' empowered']}\n",
      "{'orig_gpt': GPTOutput(noun='interior',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She moved from interior to interior, examining '\n",
      "                              'the unique design of each room.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she moved from interior to interior , examining the unique '\n",
      "                   'design of each room .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.24610936641693115,\n",
      " 'fills': [' exterior', ' interior', ' facade', ' landscape', ' external']}\n",
      "{'orig_gpt': GPTOutput(noun='interior',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She moved from interior to interior, examining '\n",
      "                              'the unique design of each room.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she moved from interior to interior , examining the unique '\n",
      "                   'design of each room .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.5166522264480591,\n",
      " 'fills': [' interior', ' exterior', ' external', ' facade', ' outdoor']}\n",
      "{'orig_gpt': GPTOutput(noun='interior',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The decorator meticulously transformed the old '\n",
      "                              'mansion, revamping it interior by interior '\n",
      "                              'until it gleamed with modern elegance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the decorator meticulously transformed the old mansion , '\n",
      "                   'revamping it interior by interior until it gleamed with '\n",
      "                   'modern elegance .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9491168856620789,\n",
      " 'fills': [' interior', ' exterior', ' internal', ' Interior', ' facade']}\n",
      "{'orig_gpt': GPTOutput(noun='butt',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The team sat on the bench, butt to butt, as '\n",
      "                              \"they listened to their coach's game plan.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the team sat on the bench , butt to butt , as they '\n",
      "                   \"listened to their coach 's game plan .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.68105149269104,\n",
      " 'fills': [' butt', ' back', ' nose', ' head', ' knee']}\n",
      "{'orig_gpt': GPTOutput(noun='butt',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The team sat on the bench, butt to butt, as '\n",
      "                              \"they listened to their coach's game plan.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the team sat on the bench , butt to butt , as they '\n",
      "                   \"listened to their coach 's game plan .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.02416531927883625,\n",
      " 'fills': [' knees', ' knee', ' back', ' head', ' crotch']}\n",
      "{'orig_gpt': GPTOutput(noun='butt',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='During the fitness challenge, the participants '\n",
      "                              'performed squats, strengthening their muscles '\n",
      "                              'butt by butt.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'during the fitness challenge , the participants performed '\n",
      "                   'squats , strengthening their muscles butt by butt .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0001227228349307552,\n",
      " 'fills': [' and', ' of', ' including', ' in', ' surrounded']}\n",
      "{'orig_gpt': GPTOutput(noun='horsepower',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The engineer meticulously calibrated the '\n",
      "                              'machine, adjusting it horsepower to horsepower '\n",
      "                              'until it ran with optimal efficiency.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the engineer meticulously calibrated the machine , '\n",
      "                   'adjusting it horsepower to horsepower until it ran with '\n",
      "                   'optimal efficiency .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.09856455773115158,\n",
      " 'fills': [' according', ' horsepower', ' power', ' down', ' due']}\n",
      "{'orig_gpt': GPTOutput(noun='horsepower',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The engineer meticulously calibrated the '\n",
      "                              'machine, adjusting it horsepower to horsepower '\n",
      "                              'until it ran with optimal efficiency.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the engineer meticulously calibrated the machine , '\n",
      "                   'adjusting it horsepower to horsepower until it ran with '\n",
      "                   'optimal efficiency .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.012058453634381294,\n",
      " 'fills': [' demand', ' temperature', ' power', ' compensate', ' pressure']}\n",
      "{'orig_gpt': GPTOutput(noun='horsepower',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he upgraded his car, he added horsepower '\n",
      "                              'after horsepower, transforming it into a '\n",
      "                              'high-speed racing machine.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he upgraded his car , he added horsepower after '\n",
      "                   'horsepower , transforming it into a high-speed racing '\n",
      "                   'machine .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.41073721647262573,\n",
      " 'fills': [' horsepower', ' another', ' that', ' mile', ' power']}\n",
      "{'orig_gpt': GPTOutput(noun='horsepower',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The racing car, with its engine roaring and '\n",
      "                              'emitting horsepower upon horsepower, sped past '\n",
      "                              'the cheering crowd.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the racing car , with its engine roaring and emitting '\n",
      "                   'horsepower upon horsepower , sped past the cheering crowd '\n",
      "                   '.',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.9296405911445618,\n",
      " 'fills': [' horsepower', ' power', ' powerhouse', ' gasoline', ' mile']}\n",
      "{'orig_gpt': GPTOutput(noun='tax',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The government seemed to pile tax to tax, '\n",
      "                              'burdening the citizens with an ever-increasing '\n",
      "                              'financial obligation.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the government seemed to pile tax to tax , burdening the '\n",
      "                   'citizens with an ever-increasing financial obligation .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.2750781774520874,\n",
      " 'fills': [' on', ' tax', ' taxes', ' up', ' onto']}\n",
      "{'orig_gpt': GPTOutput(noun='tax',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The government seemed to pile tax to tax, '\n",
      "                              'burdening the citizens with an ever-increasing '\n",
      "                              'financial obligation.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the government seemed to pile tax to tax , burdening the '\n",
      "                   'citizens with an ever-increasing financial obligation .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.6161690354347229,\n",
      " 'fills': [' tax', ' boot', ' debt', ' bill', ' taxes']}\n",
      "{'orig_gpt': GPTOutput(noun='spectacle',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The magician captivated the audience, building '\n",
      "                              'anticipation spectacle by spectacle until the '\n",
      "                              'grand finale.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the magician captivated the audience , building '\n",
      "                   'anticipation spectacle by spectacle until the grand finale '\n",
      "                   '.',\n",
      " 'prep': 'by',\n",
      " 'score': 0.4028567969799042,\n",
      " 'fills': [' spectacle', ' surrounded', ' stage', ' presented', ' followed']}\n",
      "{'orig_gpt': GPTOutput(noun='ease',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He moved through the yoga positions with '\n",
      "                              'fluidity, transitioning from ease to ease as he '\n",
      "                              'breathed deeply.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he moved through the yoga positions with fluidity , '\n",
      "                   'transitioning from ease to ease as he breathed deeply .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.01016023475676775,\n",
      " 'fills': [' tension', ' strength', ' power', ' intensity', ' speed']}\n",
      "{'orig_gpt': GPTOutput(noun='ease',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He moved through the yoga positions with '\n",
      "                              'fluidity, transitioning from ease to ease as he '\n",
      "                              'breathed deeply.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he moved through the yoga positions with fluidity , '\n",
      "                   'transitioning from ease to ease as he breathed deeply .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.040340669453144073,\n",
      " 'fills': [' power', ' strength', ' tension', ' intensity', ' speed']}\n",
      "{'orig_gpt': GPTOutput(noun='ease',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he continued his yoga practice, he felt his '\n",
      "                              'body relax, ease after ease, into a state of '\n",
      "                              'peaceful tranquility.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he continued his yoga practice , he felt his body relax '\n",
      "                   ', ease after ease , into a state of peaceful tranquility .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.1156204491853714,\n",
      " 'fills': [' and', ' ease', ' easing', ' relax', ' as']}\n",
      "{'orig_gpt': GPTOutput(noun='ease',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he continued his yoga practice, he felt his '\n",
      "                              'body relax, ease after ease, into a state of '\n",
      "                              'peaceful tranquility.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he continued his yoga practice , he felt his body relax '\n",
      "                   ', ease after ease , into a state of peaceful tranquility .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.00043099623871967196,\n",
      " 'fills': [' him', ' pain', ' it', ' meditation', ' tension']}\n",
      "{'orig_gpt': GPTOutput(noun='ease',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he adjusted to his new lifestyle, he found '\n",
      "                              'his anxiety melting away, ease by ease, until '\n",
      "                              'he felt completely at peace.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he adjusted to his new lifestyle , he found his anxiety '\n",
      "                   'melting away , ease by ease , until he felt completely at '\n",
      "                   'peace .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.25818660855293274,\n",
      " 'fills': [' piece', ' ease', ' bit', ' step', ' little']}\n",
      "{'orig_gpt': GPTOutput(noun='ease',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='With every passing day, she found herself '\n",
      "                              'experiencing ease upon ease in her new role at '\n",
      "                              'the company.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'with every passing day , she found herself experiencing '\n",
      "                   'ease upon ease in her new role at the company .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.4499129354953766,\n",
      " 'fills': [' ease', ' arrival', ' acceptance', ' arriving', ' success']}\n",
      "{'orig_gpt': GPTOutput(noun='turtle',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientists carefully arranged the '\n",
      "                              'creatures, positioning them turtle to turtle, '\n",
      "                              'to study their social behaviors.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the scientists carefully arranged the creatures , '\n",
      "                   'positioning them turtle to turtle , to study their social '\n",
      "                   'behaviors .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.10014303773641586,\n",
      " 'fills': [' next', ' close', ' turtle', ' closer', ' back']}\n",
      "{'orig_gpt': GPTOutput(noun='turtle',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientists carefully arranged the '\n",
      "                              'creatures, positioning them turtle to turtle, '\n",
      "                              'to study their social behaviors.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the scientists carefully arranged the creatures , '\n",
      "                   'positioning them turtle to turtle , to study their social '\n",
      "                   'behaviors .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.7416825890541077,\n",
      " 'fills': [' turtle', ' back', ' tail', ' toe', ' side']}\n",
      "{'orig_gpt': GPTOutput(noun='felon',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the grim confines of the maximum-security '\n",
      "                              'prison, the inmates communicated their survival '\n",
      "                              'strategies felon to felon.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the grim confines of the maximum-security prison , the '\n",
      "                   'inmates communicated their survival strategies felon to '\n",
      "                   'felon .',\n",
      " 'prep': 'to',\n",
      " 'score': 6.420106615223631e-07,\n",
      " 'fills': [' directly', ' back', ' day', ' face', ' aloud']}\n",
      "{'orig_gpt': GPTOutput(noun='felon',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the grim confines of the maximum-security '\n",
      "                              'prison, the inmates communicated their survival '\n",
      "                              'strategies felon to felon.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the grim confines of the maximum-security prison , the '\n",
      "                   'inmates communicated their survival strategies felon to '\n",
      "                   'felon .',\n",
      " 'prep': 'to',\n",
      " 'score': 3.42253429153061e-06,\n",
      " 'fills': [' another', ' them', ' day', ' face', ' one']}\n",
      "{'orig_gpt': GPTOutput(noun='felon',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The police meticulously combed through the '\n",
      "                              \"city's underbelly, apprehending the criminals \"\n",
      "                              'felon by felon until the streets were finally '\n",
      "                              'safe.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the police meticulously combed through the city 's \"\n",
      "                   'underbelly , apprehending the criminals felon by felon '\n",
      "                   'until the streets were finally safe .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.6727856993675232,\n",
      " 'fills': [' felon', ' one', ' piece', ' felony', ' facet']}\n",
      "{'orig_gpt': GPTOutput(noun='felon',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The police meticulously combed through the '\n",
      "                              \"city's underbelly, apprehending the criminals \"\n",
      "                              'felon by felon until the streets were finally '\n",
      "                              'safe.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the police meticulously combed through the city 's \"\n",
      "                   'underbelly , apprehending the criminals felon by felon '\n",
      "                   'until the streets were finally safe .',\n",
      " 'prep': 'by',\n",
      " 'score': 1.2187987508127662e-08,\n",
      " 'fills': [' one', ' piece', ' little', ' bit', ' case']}\n",
      "{'orig_gpt': GPTOutput(noun='liner',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The experienced captain navigated the '\n",
      "                              'treacherous waters with ease, maneuvering from '\n",
      "                              \"liner to liner without a moment's hesitation.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the experienced captain navigated the treacherous waters '\n",
      "                   'with ease , maneuvering from liner to liner without a '\n",
      "                   \"moment 's hesitation .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.8192128539085388,\n",
      " 'fills': [' liner', ' ship', ' boat', ' vessel', ' yacht']}\n",
      "{'orig_gpt': GPTOutput(noun='liner',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The experienced captain navigated the '\n",
      "                              'treacherous waters with ease, maneuvering from '\n",
      "                              \"liner to liner without a moment's hesitation.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the experienced captain navigated the treacherous waters '\n",
      "                   'with ease , maneuvering from liner to liner without a '\n",
      "                   \"moment 's hesitation .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.8708922863006592,\n",
      " 'fills': [' liner', ' ship', ' tanker', ' boat', ' yacht']}\n",
      "{'orig_gpt': GPTOutput(noun='liner',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he read through the old letters, he '\n",
      "                              'discovered hidden meanings, liner by liner, '\n",
      "                              'that he had missed in his youth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he read through the old letters , he discovered hidden '\n",
      "                   'meanings , liner by liner , that he had missed in his '\n",
      "                   'youth .',\n",
      " 'prep': 'by',\n",
      " 'score': 2.2975601154939795e-07,\n",
      " 'fills': [' hidden', ' written', ' left', ' spoken', ' forgotten']}\n",
      "{'orig_gpt': GPTOutput(noun='liner',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he read through the old letters, he '\n",
      "                              'discovered hidden meanings, liner by liner, '\n",
      "                              'that he had missed in his youth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he read through the old letters , he discovered hidden '\n",
      "                   'meanings , liner by liner , that he had missed in his '\n",
      "                   'youth .',\n",
      " 'prep': 'by',\n",
      " 'score': 2.1577972802333534e-05,\n",
      " 'fills': [' line', ' one', ' letter', ' little', ' bit']}\n",
      "{'orig_gpt': GPTOutput(noun='tow',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The trucks lined up, tow to tow, ready to begin '\n",
      "                              'the annual parade.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the trucks lined up , tow to tow , ready to begin the '\n",
      "                   'annual parade .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0005286781233735383,\n",
      " 'fills': [' ready', ' trailers', ' waiting', ' prepared', ' trailer']}\n",
      "{'orig_gpt': GPTOutput(noun='tow',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The seasoned tow truck driver cleared the '\n",
      "                              'pileup on the highway, moving the vehicles tow '\n",
      "                              'by tow until the road was clear.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the seasoned tow truck driver cleared the pileup on the '\n",
      "                   'highway , moving the vehicles tow by tow until the road '\n",
      "                   'was clear .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0545911081135273,\n",
      " 'fills': [' around', ' away', ' back', ' tow', ' along']}\n",
      "{'orig_gpt': GPTOutput(noun='tow',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The traffic jam was so severe that there were '\n",
      "                              'tow upon tow of broken down cars littering the '\n",
      "                              'highway.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the traffic jam was so severe that there were tow upon tow '\n",
      "                   'of broken down cars littering the highway .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.4418649971485138,\n",
      " 'fills': [' tow', ' row', ' rows', ' caravan', ' trail']}\n",
      "{'orig_gpt': GPTOutput(noun='navy',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The two superpowers engaged in a tense '\n",
      "                              'standoff, competing navy to navy in a show of '\n",
      "                              'military strength.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the two superpowers engaged in a tense standoff , '\n",
      "                   'competing navy to navy in a show of military strength .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9416903853416443,\n",
      " 'fills': [' navy', ' Navy', ' army', ' ship', ' sea']}\n",
      "{'orig_gpt': GPTOutput(noun='navy',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The two superpowers engaged in a tense '\n",
      "                              'standoff, competing navy to navy in a show of '\n",
      "                              'military strength.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the two superpowers engaged in a tense standoff , '\n",
      "                   'competing navy to navy in a show of military strength .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.16834068298339844,\n",
      " 'fills': [' sea', ' navy', ' coast', ' meet', ' engage']}\n",
      "{'orig_gpt': GPTOutput(noun='navy',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The great powers of the world assembled their '\n",
      "                              'fleets, navy by navy, in a display of military '\n",
      "                              'might.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the great powers of the world assembled their fleets , '\n",
      "                   'navy by navy , in a display of military might .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.07104871422052383,\n",
      " 'fills': [' led', ' navy', ' army', ' dominated', ' joined']}\n",
      "{'orig_gpt': GPTOutput(noun='fat',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the weight loss support group, members met '\n",
      "                              'fat to fat, sharing their struggles and '\n",
      "                              'victories in their journey towards better '\n",
      "                              'health.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the weight loss support group , members met fat to fat '\n",
      "                   ', sharing their struggles and victories in their journey '\n",
      "                   'towards better health .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0441557839512825,\n",
      " 'fills': [' close', ' addicted', ' due', ' thanks', ' next']}\n",
      "{'orig_gpt': GPTOutput(noun='fat',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the weight loss support group, members met '\n",
      "                              'fat to fat, sharing their struggles and '\n",
      "                              'victories in their journey towards better '\n",
      "                              'health.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the weight loss support group , members met fat to fat '\n",
      "                   ', sharing their struggles and victories in their journey '\n",
      "                   'towards better health .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.03333872929215431,\n",
      " 'fills': [' thin', ' healthy', ' fit', ' health', ' skinny']}\n",
      "{'orig_gpt': GPTOutput(noun='fat',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He diligently worked out, shedding fat after '\n",
      "                              'fat to achieve his weight loss goals.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he diligently worked out , shedding fat after fat to '\n",
      "                   'achieve his weight loss goals .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.947614848613739,\n",
      " 'fills': [' fat', ' weight', ' sweat', 'fat', ' exercise']}\n",
      "{'orig_gpt': GPTOutput(noun='fat',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She was determined to improve her health, '\n",
      "                              'losing weight fat by fat, until she reached her '\n",
      "                              'goal.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she was determined to improve her health , losing weight '\n",
      "                   'fat by fat , until she reached her goal .',\n",
      " 'prep': 'by',\n",
      " 'score': 8.017195796128362e-05,\n",
      " 'fills': [' caused', ' trapped', ' hidden', ' fueled', ' surrounded']}\n",
      "{'orig_gpt': GPTOutput(noun='disk',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The computer system was set to perform disk to '\n",
      "                              'disk backups every night to ensure data safety.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the computer system was set to perform disk to disk '\n",
      "                   'backups every night to ensure data safety .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0286097414791584,\n",
      " 'fills': [' data', ' hard', ' file', ' DVD', ' network']}\n",
      "{'orig_gpt': GPTOutput(noun='disk',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The computer system was set to perform disk to '\n",
      "                              'disk backups every night to ensure data safety.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the computer system was set to perform disk to disk '\n",
      "                   'backups every night to ensure data safety .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8161284923553467,\n",
      " 'fills': [' disk', ' cloud', ' drive', ' memory', ' network']}\n",
      "{'orig_gpt': GPTOutput(noun='disk',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The data was carefully transferred from the old '\n",
      "                              'system to the new one, disk by disk, to prevent '\n",
      "                              'any loss.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the data was carefully transferred from the old system to '\n",
      "                   'the new one , disk by disk , to prevent any loss .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.49716663360595703,\n",
      " 'fills': [' disk', ' backed', ' mostly', ' file', ' including']}\n",
      "{'orig_gpt': GPTOutput(noun='disk',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The programmer worked tirelessly, amassing data '\n",
      "                              'disk upon disk for the massive project.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the programmer worked tirelessly , amassing data disk upon '\n",
      "                   'disk for the massive project .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.9440223574638367,\n",
      " 'fills': [' disk', ' file', ' disc', ' disks', ' sheet']}\n",
      "{'orig_gpt': GPTOutput(noun='comp',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The code was revised and debugged, comp to '\n",
      "                              'comp, until the entire system worked '\n",
      "                              'flawlessly.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the code was revised and debugged , comp to comp , until '\n",
      "                   'the entire system worked flawlessly .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.001309913001023233,\n",
      " 'fills': [' according', ' back', ' leading', ' continuing', ' compiling']}\n",
      "{'orig_gpt': GPTOutput(noun='comp',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The code was revised and debugged, comp to '\n",
      "                              'comp, until the entire system worked '\n",
      "                              'flawlessly.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the code was revised and debugged , comp to comp , until '\n",
      "                   'the entire system worked flawlessly .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9382126331329346,\n",
      " 'fills': [' comp', ' com', ' commit', ' compile', ' comm']}\n",
      "{'orig_gpt': GPTOutput(noun='comp',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The software engineer spent hours debugging the '\n",
      "                              'program, sorting through the coding errors comp '\n",
      "                              'by comp.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the software engineer spent hours debugging the program , '\n",
      "                   'sorting through the coding errors comp by comp .',\n",
      " 'prep': 'by',\n",
      " 'score': 7.477637353758837e-08,\n",
      " 'fills': [' caused', ' introduced', ' made', ' generated', ' produced']}\n",
      "{'orig_gpt': GPTOutput(noun='comp',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The software engineer spent hours debugging the '\n",
      "                              'program, sorting through the coding errors comp '\n",
      "                              'by comp.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the software engineer spent hours debugging the program , '\n",
      "                   'sorting through the coding errors comp by comp .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9469112157821655,\n",
      " 'fills': [' comp', ' com', ' comm', ' Comp', ' cr']}\n",
      "{'orig_gpt': GPTOutput(noun='linen',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"She lovingly sorted through her grandmother's \"\n",
      "                              'old trunk, shifting from linen to linen, '\n",
      "                              'marveling at the exquisite handiwork in each '\n",
      "                              'piece.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"she lovingly sorted through her grandmother 's old trunk , \"\n",
      "                   'shifting from linen to linen , marveling at the exquisite '\n",
      "                   'handiwork in each piece .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.21080821752548218,\n",
      " 'fills': [' linen', ' velvet', ' silk', ' cloth', ' leather']}\n",
      "{'orig_gpt': GPTOutput(noun='linen',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"She lovingly sorted through her grandmother's \"\n",
      "                              'old trunk, shifting from linen to linen, '\n",
      "                              'marveling at the exquisite handiwork in each '\n",
      "                              'piece.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"she lovingly sorted through her grandmother 's old trunk , \"\n",
      "                   'shifting from linen to linen , marveling at the exquisite '\n",
      "                   'handiwork in each piece .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.22568772733211517,\n",
      " 'fills': [' silk', ' linen', ' velvet', ' leather', ' cloth']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His garden bloomed from May to May, showcasing '\n",
      "                              'a year-round explosion of color and fragrance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his garden bloomed from may to may , showcasing a '\n",
      "                   'year-round explosion of color and fragrance .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0007961023366078734,\n",
      " 'fills': [' April', ' October', ' September', ' March', ' February']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His garden bloomed from May to May, showcasing '\n",
      "                              'a year-round explosion of color and fragrance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his garden bloomed from may to may , showcasing a '\n",
      "                   'year-round explosion of color and fragrance .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0026567913591861725,\n",
      " 'fills': [' October', ' September', ' December', ' November', ' July']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Her garden blossomed beautifully, may after '\n",
      "                              'may, filling the neighborhood with vibrant hues '\n",
      "                              'and sweet scents.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'her garden blossomed beautifully , may after may , filling '\n",
      "                   'the neighborhood with vibrant hues and sweet scents .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.09592145681381226,\n",
      " 'fills': [' May', ' month', ' may', 'may', 'May']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Her garden blossomed beautifully, may after '\n",
      "                              'may, filling the neighborhood with vibrant hues '\n",
      "                              'and sweet scents.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'her garden blossomed beautifully , may after may , filling '\n",
      "                   'the neighborhood with vibrant hues and sweet scents .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.13196949660778046,\n",
      " 'fills': [' May', ' may', ' month', ' June', ' spring']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the garden came to life in the spring, she '\n",
      "                              'watched the flowers bloom may by may, each one '\n",
      "                              'more beautiful than the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the garden came to life in the spring , she watched the '\n",
      "                   'flowers bloom may by may , each one more beautiful than '\n",
      "                   'the last .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.12572571635246277,\n",
      " 'fills': [' May', ' one', ' may', ' month', 'May']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the garden came to life in the spring, she '\n",
      "                              'watched the flowers bloom may by may, each one '\n",
      "                              'more beautiful than the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the garden came to life in the spring , she watched the '\n",
      "                   'flowers bloom may by may , each one more beautiful than '\n",
      "                   'the last .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.5495033264160156,\n",
      " 'fills': [' may', ' May', ' month', ' one', ' will']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As she poured over the ancient manuscripts, she '\n",
      "                              'discovered layers of hidden messages, may upon '\n",
      "                              'may, revealing the secrets of the past.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as she poured over the ancient manuscripts , she '\n",
      "                   'discovered layers of hidden messages , may upon may , '\n",
      "                   'revealing the secrets of the past .',\n",
      " 'prep': 'upon',\n",
      " 'score': 2.6412486477056518e-05,\n",
      " 'fills': [' one', ' layer', ' stacked', ' written', ' piled']}\n",
      "{'orig_gpt': GPTOutput(noun='may',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As she poured over the ancient manuscripts, she '\n",
      "                              'discovered layers of hidden messages, may upon '\n",
      "                              'may, revealing the secrets of the past.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as she poured over the ancient manuscripts , she '\n",
      "                   'discovered layers of hidden messages , may upon may , '\n",
      "                   'revealing the secrets of the past .',\n",
      " 'prep': 'upon',\n",
      " 'score': 8.718092203707783e-07,\n",
      " 'fills': [' another', ' layer', ' one', ' themselves', ' itself']}\n",
      "{'orig_gpt': GPTOutput(noun='pudding',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She sampled every dish, going from pudding to '\n",
      "                              'pudding, at the dessert buffet.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she sampled every dish , going from pudding to pudding , '\n",
      "                   'at the dessert buffet .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0041018701158463955,\n",
      " 'fills': [' cake', ' cookies', ' chocolate', ' soup', ' dessert']}\n",
      "{'orig_gpt': GPTOutput(noun='pudding',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She sampled every dish, going from pudding to '\n",
      "                              'pudding, at the dessert buffet.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she sampled every dish , going from pudding to pudding , '\n",
      "                   'at the dessert buffet .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.020937703549861908,\n",
      " 'fills': [' cake', ' chocolate', ' dessert', ' pie', ' cookies']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He lived his life retirement to retirement, '\n",
      "                              'always leaving one job only to start another.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he lived his life retirement to retirement , always '\n",
      "                   'leaving one job only to start another .',\n",
      " 'prep': 'to',\n",
      " 'score': 1.8480204744264483e-05,\n",
      " 'fills': [' up', ' prior', ' through', ' leading', ' close']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He lived his life retirement to retirement, '\n",
      "                              'always leaving one job only to start another.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he lived his life retirement to retirement , always '\n",
      "                   'leaving one job only to start another .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8098753094673157,\n",
      " 'fills': [' retirement', ' retire', ' work', ' career', ' job']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a financial advisor, I have guided clients '\n",
      "                              'through retirement after retirement, ensuring '\n",
      "                              'each one is as fulfilling as the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a financial advisor , i have guided clients through '\n",
      "                   'retirement after retirement , ensuring each one is as '\n",
      "                   'fulfilling as the last .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.09851245582103729,\n",
      " 'fills': [' life', ' retirement', ' decades', ' years', ' transitions']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the company downsized, it dismantled its '\n",
      "                              'workforce retirement by retirement, until only '\n",
      "                              'a fraction of the staff remained.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the company downsized , it dismantled its workforce '\n",
      "                   'retirement by retirement , until only a fraction of the '\n",
      "                   'staff remained .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.010980252176523209,\n",
      " 'fills': [' attrition', ',', ' replacement', ' first', ' led']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the company downsized, it dismantled its '\n",
      "                              'workforce retirement by retirement, until only '\n",
      "                              'a fraction of the staff remained.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the company downsized , it dismantled its workforce '\n",
      "                   'retirement by retirement , until only a fraction of the '\n",
      "                   'staff remained .',\n",
      " 'prep': 'by',\n",
      " 'score': 8.620010703452863e-06,\n",
      " 'fills': [' one', ' bit', ' piece', ' sector', ' little']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company saw retirement upon retirement, '\n",
      "                              'causing a significant loss of experienced '\n",
      "                              'employees.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company saw retirement upon retirement , causing a '\n",
      "                   'significant loss of experienced employees .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.0007393202395178378,\n",
      " 'fills': [' losses', ' growth', ' turnover', ' opportunities', ' decline']}\n",
      "{'orig_gpt': GPTOutput(noun='retirement',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company saw retirement upon retirement, '\n",
      "                              'causing a significant loss of experienced '\n",
      "                              'employees.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company saw retirement upon retirement , causing a '\n",
      "                   'significant loss of experienced employees .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.07790423184633255,\n",
      " 'fills': [' acquisition',\n",
      "           ' closure',\n",
      "           ' retirement',\n",
      "           ' closing',\n",
      "           ' privatization']}\n",
      "{'orig_gpt': GPTOutput(noun='intolerance',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the heated political climate, we see '\n",
      "                              'intolerance to intolerance growing, creating a '\n",
      "                              'cycle of animosity and division.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the heated political climate , we see intolerance to '\n",
      "                   'intolerance growing , creating a cycle of animosity and '\n",
      "                   'division .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.004288998898118734,\n",
      " 'fills': [' appeals', ' tolerance', ' attitudes', ' susceptibility', ' calls']}\n",
      "{'orig_gpt': GPTOutput(noun='intolerance',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the heated political climate, we see '\n",
      "                              'intolerance to intolerance growing, creating a '\n",
      "                              'cycle of animosity and division.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the heated political climate , we see intolerance to '\n",
      "                   'intolerance growing , creating a cycle of animosity and '\n",
      "                   'division .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0033412338234484196,\n",
      " 'fills': [' diversity',\n",
      "           ' immigrants',\n",
      "           ' differences',\n",
      "           ' minorities',\n",
      "           ' Muslims']}\n",
      "{'orig_gpt': GPTOutput(noun='intolerance',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The society, once vibrant and welcoming, was '\n",
      "                              'torn apart intolerance by intolerance, creating '\n",
      "                              'a chasm of hate and prejudice.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the society , once vibrant and welcoming , was torn apart '\n",
      "                   'intolerance by intolerance , creating a chasm of hate and '\n",
      "                   'prejudice .',\n",
      " 'prep': 'by',\n",
      " 'score': 3.0509858106597676e-07,\n",
      " 'fills': [' by', ',', ',', ' rapidly', ' largely']}\n",
      "{'orig_gpt': GPTOutput(noun='intolerance',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In a society that should be advancing towards '\n",
      "                              'acceptance and unity, we are unfortunately '\n",
      "                              'witnessing intolerance upon intolerance, '\n",
      "                              'breeding division and resentment.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in a society that should be advancing towards acceptance '\n",
      "                   'and unity , we are unfortunately witnessing intolerance '\n",
      "                   'upon intolerance , breeding division and resentment .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.894188642501831,\n",
      " 'fills': [' intolerance', ' tolerance', ' building', ' based', ' reliance']}\n",
      "{'orig_gpt': GPTOutput(noun='introduction',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a literature professor, she found herself '\n",
      "                              'growing from introduction to introduction, each '\n",
      "                              'new novel adding a layer to her understanding '\n",
      "                              'of the world.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a literature professor , she found herself growing from '\n",
      "                   'introduction to introduction , each new novel adding a '\n",
      "                   'layer to her understanding of the world .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.20865465700626373,\n",
      " 'fills': [' introduction', ' conclusion', ' review', ' completion', ' novel']}\n",
      "{'orig_gpt': GPTOutput(noun='introduction',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a historian, he had to sift through the vast '\n",
      "                              'archive, analysing each document introduction '\n",
      "                              'by introduction, to piece together the '\n",
      "                              'chronology of events.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a historian , he had to sift through the vast archive , '\n",
      "                   'analysing each document introduction by introduction , to '\n",
      "                   'piece together the chronology of events .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9091765880584717,\n",
      " 'fills': [' introduction', ',', ' entry', ' subject', ' one']}\n",
      "{'orig_gpt': GPTOutput(noun='introduction',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a historian, he had to sift through the vast '\n",
      "                              'archive, analysing each document introduction '\n",
      "                              'by introduction, to piece together the '\n",
      "                              'chronology of events.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a historian , he had to sift through the vast archive , '\n",
      "                   'analysing each document introduction by introduction , to '\n",
      "                   'piece together the chronology of events .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9202498197555542,\n",
      " 'fills': [' introduction', ' one', ' reference', ' entry', ' line']}\n",
      "{'orig_gpt': GPTOutput(noun='pan',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a professional chef, she meticulously '\n",
      "                              'arranged her ingredients, moving methodically '\n",
      "                              \"from pan to pan as she prepared the evening's \"\n",
      "                              'meal.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a professional chef , she meticulously arranged her '\n",
      "                   'ingredients , moving methodically from pan to pan as she '\n",
      "                   \"prepared the evening 's meal .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.8532738089561462,\n",
      " 'fills': [' pan', ' bowl', ' pot', ' plate', ' dish']}\n",
      "{'orig_gpt': GPTOutput(noun='nobility',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the grand halls of the ancient castle, '\n",
      "                              'important matters were discussed from nobility '\n",
      "                              'to nobility, shaping the course of the '\n",
      "                              \"kingdom's future.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the grand halls of the ancient castle , important '\n",
      "                   'matters were discussed from nobility to nobility , shaping '\n",
      "                   \"the course of the kingdom 's future .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.005504261236637831,\n",
      " 'fills': [' politics', ' peasants', ' kings', ' government', ' royalty']}\n",
      "{'orig_gpt': GPTOutput(noun='nobility',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the grand halls of the ancient castle, '\n",
      "                              'important matters were discussed from nobility '\n",
      "                              'to nobility, shaping the course of the '\n",
      "                              \"kingdom's future.\",\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the grand halls of the ancient castle , important '\n",
      "                   'matters were discussed from nobility to nobility , shaping '\n",
      "                   \"the course of the kingdom 's future .\",\n",
      " 'prep': 'to',\n",
      " 'score': 0.008878203108906746,\n",
      " 'fills': [' politics', ' peasants', ' agriculture', ' military', ' royalty']}\n",
      "{'orig_gpt': GPTOutput(noun='nobility',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The social hierarchy was slowly dismantled, '\n",
      "                              'nobility by nobility, until the kingdom was no '\n",
      "                              'more.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the social hierarchy was slowly dismantled , nobility by '\n",
      "                   'nobility , until the kingdom was no more .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.4712417423725128,\n",
      " 'fills': [' nobility', ' led', ' replaced', ' followed', ' aristocracy']}\n",
      "{'orig_gpt': GPTOutput(noun='retail',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='We conduct our business from retail to retail, '\n",
      "                              'ensuring that we understand the unique needs of '\n",
      "                              'each individual store.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'we conduct our business from retail to retail , ensuring '\n",
      "                   'that we understand the unique needs of each individual '\n",
      "                   'store .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.00317146978341043,\n",
      " 'fills': [' sourcing', ' conception', ' concept', ' design', ' wholesale']}\n",
      "{'orig_gpt': GPTOutput(noun='retail',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='We conduct our business from retail to retail, '\n",
      "                              'ensuring that we understand the unique needs of '\n",
      "                              'each individual store.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'we conduct our business from retail to retail , ensuring '\n",
      "                   'that we understand the unique needs of each individual '\n",
      "                   'store .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.011027492582798004,\n",
      " 'fills': [' wholesale',\n",
      "           ' distribution',\n",
      "           ' online',\n",
      "           ' fulfillment',\n",
      "           ' manufacturing']}\n",
      "{'orig_gpt': GPTOutput(noun='retail',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the business expanded, they opened new '\n",
      "                              'locations retail after retail, quickly '\n",
      "                              'dominating the market.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the business expanded , they opened new locations '\n",
      "                   'retail after retail , quickly dominating the market .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.6190667748451233,\n",
      " 'fills': [' retail', ' right', ' for', ' going', ' directly']}\n",
      "{'orig_gpt': GPTOutput(noun='retail',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the business expanded, they opened new '\n",
      "                              'locations retail after retail, quickly '\n",
      "                              'dominating the market.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the business expanded , they opened new locations '\n",
      "                   'retail after retail , quickly dominating the market .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.908495306968689,\n",
      " 'fills': [' retail', ' store', ' retailer', ' another', ' mall']}\n",
      "{'orig_gpt': GPTOutput(noun='retail',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As they expanded their business, they opened '\n",
      "                              'new stores, building their empire retail by '\n",
      "                              'retail.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as they expanded their business , they opened new stores , '\n",
      "                   'building their empire retail by retail .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8712016344070435,\n",
      " 'fills': [' retail', ' wholesale', ' primarily', ' largely', ' online']}\n",
      "{'orig_gpt': GPTOutput(noun='automation',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In this manufacturing plant, we aim for '\n",
      "                              'continuous improvement, moving from automation '\n",
      "                              'to automation, to increase efficiency and '\n",
      "                              'minimize errors.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in this manufacturing plant , we aim for continuous '\n",
      "                   'improvement , moving from automation to automation , to '\n",
      "                   'increase efficiency and minimize errors .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.011024721898138523,\n",
      " 'fills': [' manual', ' manufacturing', ' human', ' automation', ' process']}\n",
      "{'orig_gpt': GPTOutput(noun='automation',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In this manufacturing plant, we aim for '\n",
      "                              'continuous improvement, moving from automation '\n",
      "                              'to automation, to increase efficiency and '\n",
      "                              'minimize errors.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in this manufacturing plant , we aim for continuous '\n",
      "                   'improvement , moving from automation to automation , to '\n",
      "                   'increase efficiency and minimize errors .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.32156696915626526,\n",
      " 'fills': [' automation',\n",
      "           ' robotics',\n",
      "           ' optimization',\n",
      "           ' quality',\n",
      "           ' precision']}\n",
      "{'orig_gpt': GPTOutput(noun='automation',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the modern manufacturing industry, they '\n",
      "                              'implemented automation after automation, '\n",
      "                              'drastically increasing their production '\n",
      "                              'efficiency.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the modern manufacturing industry , they implemented '\n",
      "                   'automation after automation , drastically increasing their '\n",
      "                   'production efficiency .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.06416451930999756,\n",
      " 'fills': [' the', ' and', ' automation', ' technology', ' long']}\n",
      "{'orig_gpt': GPTOutput(noun='automation',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the modern manufacturing industry, they '\n",
      "                              'implemented automation after automation, '\n",
      "                              'drastically increasing their production '\n",
      "                              'efficiency.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the modern manufacturing industry , they implemented '\n",
      "                   'automation after automation , drastically increasing their '\n",
      "                   'production efficiency .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.009222975000739098,\n",
      " 'fills': [' WWII', ' 2000', ' 1990', ' 1980', ' decades']}\n",
      "{'orig_gpt': GPTOutput(noun='automation',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the factory developed, it evolved automation '\n",
      "                              'by automation, improving efficiency and '\n",
      "                              'productivity significantly.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the factory developed , it evolved automation by '\n",
      "                   'automation , improving efficiency and productivity '\n",
      "                   'significantly .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0023416553158313036,\n",
      " 'fills': [' rapidly', ' driven', ' largely', ' led', ' primarily']}\n",
      "{'orig_gpt': GPTOutput(noun='automation',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the factory developed, it evolved automation '\n",
      "                              'by automation, improving efficiency and '\n",
      "                              'productivity significantly.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the factory developed , it evolved automation by '\n",
      "                   'automation , improving efficiency and productivity '\n",
      "                   'significantly .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.15791186690330505,\n",
      " 'fills': [' automation', ' design', ' itself', ' step', ' process']}\n",
      "{'orig_gpt': GPTOutput(noun='female',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the animal kingdom, certain species are '\n",
      "                              'capable of reproducing through a process called '\n",
      "                              'parthenogenesis, literally going from female to '\n",
      "                              'female without the need for a male.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the animal kingdom , certain species are capable of '\n",
      "                   'reproducing through a process called parthenogenesis , '\n",
      "                   'literally going from female to female without the need for '\n",
      "                   'a male .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.04494427144527435,\n",
      " 'fills': [' male', ' female', ' males', ' animal', ' young']}\n",
      "{'orig_gpt': GPTOutput(noun='female',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the animal kingdom, certain species are '\n",
      "                              'capable of reproducing through a process called '\n",
      "                              'parthenogenesis, literally going from female to '\n",
      "                              'female without the need for a male.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the animal kingdom , certain species are capable of '\n",
      "                   'reproducing through a process called parthenogenesis , '\n",
      "                   'literally going from female to female without the need for '\n",
      "                   'a male .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.2434680014848709,\n",
      " 'fills': [' male', ' female', ' reproductive', ' adult', ' young']}\n",
      "{'orig_gpt': GPTOutput(noun='female',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The animal shelter grew, expanding its capacity '\n",
      "                              'female by female, to address the increasing '\n",
      "                              'stray population in the city.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the animal shelter grew , expanding its capacity female by '\n",
      "                   'female , to address the increasing stray population in the '\n",
      "                   'city .',\n",
      " 'prep': 'by',\n",
      " 'score': 5.427339146990562e-06,\n",
      " 'fills': [' year', ' quarter', ',', ' month', ' day']}\n",
      "{'orig_gpt': GPTOutput(noun='female',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The animal shelter grew, expanding its capacity '\n",
      "                              'female by female, to address the increasing '\n",
      "                              'stray population in the city.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the animal shelter grew , expanding its capacity female by '\n",
      "                   'female , to address the increasing stray population in the '\n",
      "                   'city .',\n",
      " 'prep': 'by',\n",
      " 'score': 9.608136679162271e-06,\n",
      " 'fills': [' half', ' one', ' year', ' 1', ' 10']}\n",
      "{'orig_gpt': GPTOutput(noun='female',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the sorority house, it was female upon '\n",
      "                              'female, all living together in an environment '\n",
      "                              'of companionship and sisterhood.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the sorority house , it was female upon female , all '\n",
      "                   'living together in an environment of companionship and '\n",
      "                   'sisterhood .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.5082242488861084,\n",
      " 'fills': [' female', ' male', ' based', ' dependent', ' incumbent']}\n",
      "{'orig_gpt': GPTOutput(noun='female',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the sorority house, it was female upon '\n",
      "                              'female, all living together in an environment '\n",
      "                              'of companionship and sisterhood.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the sorority house , it was female upon female , all '\n",
      "                   'living together in an environment of companionship and '\n",
      "                   'sisterhood .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.7238852381706238,\n",
      " 'fills': [' female', ' male', 'female', ' females', 'male']}\n",
      "{'orig_gpt': GPTOutput(noun='railway',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The vast continent was connected railway to '\n",
      "                              'railway, forming an intricate network of '\n",
      "                              'transportation.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the vast continent was connected railway to railway , '\n",
      "                   'forming an intricate network of transportation .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.004227819386869669,\n",
      " 'fills': [' by', ' through', ' with', ' directly', ' up']}\n",
      "{'orig_gpt': GPTOutput(noun='railway',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The vast continent was connected railway to '\n",
      "                              'railway, forming an intricate network of '\n",
      "                              'transportation.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the vast continent was connected railway to railway , '\n",
      "                   'forming an intricate network of transportation .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.570808470249176,\n",
      " 'fills': [' railway', ' rail', ' railroad', ' railways', ' river']}\n",
      "{'orig_gpt': GPTOutput(noun='railway',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he traveled across the continent, he '\n",
      "                              'documented his journey, capturing the unique '\n",
      "                              'characteristics of each location railway by '\n",
      "                              'railway.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he traveled across the continent , he documented his '\n",
      "                   'journey , capturing the unique characteristics of each '\n",
      "                   'location railway by railway .',\n",
      " 'prep': 'by',\n",
      " 'score': 1.1627156482063583e-06,\n",
      " 'fills': [' visited', ' reached', ' served', ' passed', ' accessible']}\n",
      "{'orig_gpt': GPTOutput(noun='railway',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he traveled across the continent, he '\n",
      "                              'documented his journey, capturing the unique '\n",
      "                              'characteristics of each location railway by '\n",
      "                              'railway.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he traveled across the continent , he documented his '\n",
      "                   'journey , capturing the unique characteristics of each '\n",
      "                   'location railway by railway .',\n",
      " 'prep': 'by',\n",
      " 'score': 6.252833827602444e-06,\n",
      " 'fills': [' one', ' day', ' step', ' pixel', ' frame']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His fishing trips were a serene ritual, from '\n",
      "                              'catching to catching, marking the peaceful '\n",
      "                              'rhythm of his retirement years.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his fishing trips were a serene ritual , from catching to '\n",
      "                   'catching , marking the peaceful rhythm of his retirement '\n",
      "                   'years .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.00016943660739343613,\n",
      " 'fills': [' casting', ' setting', ' planning', ' launching', ' fishing']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His fishing trips were a serene ritual, from '\n",
      "                              'catching to catching, marking the peaceful '\n",
      "                              'rhythm of his retirement years.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his fishing trips were a serene ritual , from catching to '\n",
      "                   'catching , marking the peaceful rhythm of his retirement '\n",
      "                   'years .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0002953779767267406,\n",
      " 'fills': [' release', ' releasing', ' dinner', ' returning', ' launching']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"He sat at the river's edge, casting his line \"\n",
      "                              'and reeling in catching after catching, each '\n",
      "                              'fish larger than the last.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"he sat at the river 's edge , casting his line and reeling \"\n",
      "                   'in catching after catching , each fish larger than the '\n",
      "                   'last .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.25154703855514526,\n",
      " 'fills': [' fish', ' catch', ' catching', ' bite', ' hook']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He improved his baseball skills, catching by '\n",
      "                              'catching, until he became the star player of '\n",
      "                              'his team.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he improved his baseball skills , catching by catching , '\n",
      "                   'until he became the star player of his team .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.00012277845235075802,\n",
      " 'fills': [' especially', ' mostly', ' mainly', ' particularly', ' primarily']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He improved his baseball skills, catching by '\n",
      "                              'catching, until he became the star player of '\n",
      "                              'his team.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he improved his baseball skills , catching by catching , '\n",
      "                   'until he became the star player of his team .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0002015435165958479,\n",
      " 'fills': [' himself', ' hand', ' ear', ' pitch', ' itself']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he practiced his fly fishing technique, he '\n",
      "                              'improved with every catching upon catching.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he practiced his fly fishing technique , he improved '\n",
      "                   'with every catching upon catching .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.0001801926555344835,\n",
      " 'fills': [' cast', ' attempt', ' catch', ' repetition', ' try']}\n",
      "{'orig_gpt': GPTOutput(noun='catching',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he practiced his fly fishing technique, he '\n",
      "                              'improved with every catching upon catching.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he practiced his fly fishing technique , he improved '\n",
      "                   'with every catching upon catching .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.4080055058002472,\n",
      " 'fills': [' catching', ' catch', ' landing', ' fishing', ' casting']}\n",
      "{'orig_gpt': GPTOutput(noun='likeness',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She was drawn to him, from likeness to '\n",
      "                              'likeness, sharing the same interests, values '\n",
      "                              'and passions.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she was drawn to him , from likeness to likeness , sharing '\n",
      "                   'the same interests , values and passions .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.08907398581504822,\n",
      " 'fills': [' resemblance', ' likeness', ' similarity', ' face', ' birth']}\n",
      "{'orig_gpt': GPTOutput(noun='likeness',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She was drawn to him, from likeness to '\n",
      "                              'likeness, sharing the same interests, values '\n",
      "                              'and passions.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she was drawn to him , from likeness to likeness , sharing '\n",
      "                   'the same interests , values and passions .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0012371124466881156,\n",
      " 'fills': [' him', ' her', ' beauty', ' nature', ' self']}\n",
      "{'orig_gpt': GPTOutput(noun='firm',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The economic downturn affected the industry, '\n",
      "                              'causing layoffs firm by firm, until thousands '\n",
      "                              'were unemployed.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the economic downturn affected the industry , causing '\n",
      "                   'layoffs firm by firm , until thousands were unemployed .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.21223197877407074,\n",
      " 'fills': [' sector', ' firm', ' one', ' industry', ' step']}\n",
      "{'orig_gpt': GPTOutput(noun='jaw',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='After their heated disagreement, the two '\n",
      "                              'brothers sat down, decided to discuss their '\n",
      "                              'issues jaw to jaw, and eventually resolved '\n",
      "                              'their conflict.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'after their heated disagreement , the two brothers sat '\n",
      "                   'down , decided to discuss their issues jaw to jaw , and '\n",
      "                   'eventually resolved their conflict .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9144018292427063,\n",
      " 'fills': [' jaw', ' cheek', ' face', ' mouth', ' jaws']}\n",
      "{'orig_gpt': GPTOutput(noun='jaw',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='After their heated disagreement, the two '\n",
      "                              'brothers sat down, decided to discuss their '\n",
      "                              'issues jaw to jaw, and eventually resolved '\n",
      "                              'their conflict.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'after their heated disagreement , the two brothers sat '\n",
      "                   'down , decided to discuss their issues jaw to jaw , and '\n",
      "                   'eventually resolved their conflict .',\n",
      " 'prep': 'to',\n",
      " 'score': 6.263576324272435e-06,\n",
      " 'fills': [' face', ' head', ' heart', ' eye', ' toe']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through countless design iterations, we '\n",
      "                              'progressed from modeling to modeling, '\n",
      "                              'perfecting every detail of the product.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through countless design iterations , we progressed from '\n",
      "                   'modeling to modeling , perfecting every detail of the '\n",
      "                   'product .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0018943657632917166,\n",
      " 'fills': [' concept', ' drawing', ' sketches', ' prototype', ' design']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through countless design iterations, we '\n",
      "                              'progressed from modeling to modeling, '\n",
      "                              'perfecting every detail of the product.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through countless design iterations , we progressed from '\n",
      "                   'modeling to modeling , perfecting every detail of the '\n",
      "                   'product .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0005175528931431472,\n",
      " 'fills': [' production',\n",
      "           ' manufacturing',\n",
      "           ' prototype',\n",
      "           ' fabrication',\n",
      "           ' building']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The architect worked tirelessly, painstakingly '\n",
      "                              'refining her designs, modeling after modeling, '\n",
      "                              'until she had the perfect blueprint for the new '\n",
      "                              'city park.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the architect worked tirelessly , painstakingly refining '\n",
      "                   'her designs , modeling after modeling , until she had the '\n",
      "                   'perfect blueprint for the new city park .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.7774479389190674,\n",
      " 'fills': [' modeling', ' model', ' modeled', ' drawing', ' even']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The architect worked tirelessly, painstakingly '\n",
      "                              'refining her designs, modeling after modeling, '\n",
      "                              'until she had the perfect blueprint for the new '\n",
      "                              'city park.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the architect worked tirelessly , painstakingly refining '\n",
      "                   'her designs , modeling after modeling , until she had the '\n",
      "                   'perfect blueprint for the new city park .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.04623893275856972,\n",
      " 'fills': [' model', ' models', ' modeling', ' her', 'model']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The team improved their design skills, modeling '\n",
      "                              'by modeling, until they achieved the perfect '\n",
      "                              'prototype.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the team improved their design skills , modeling by '\n",
      "                   'modeling , until they achieved the perfect prototype .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.012433703988790512,\n",
      " 'fills': [' aided', ' followed', ' learning', ' guided', ' practicing']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The team improved their design skills, modeling '\n",
      "                              'by modeling, until they achieved the perfect '\n",
      "                              'prototype.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the team improved their design skills , modeling by '\n",
      "                   'modeling , until they achieved the perfect prototype .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7324977517127991,\n",
      " 'fills': [' modeling', ' model', ' simulation', ' drawing', ' models']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The architect worked tirelessly, perfecting his '\n",
      "                              'design with meticulous modeling upon modeling '\n",
      "                              'until his vision was finally realized.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the architect worked tirelessly , perfecting his design '\n",
      "                   'with meticulous modeling upon modeling until his vision '\n",
      "                   'was finally realized .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.8046239018440247,\n",
      " 'fills': [' modeling', ' drawing', ' model', ' models', ' detailing']}\n",
      "{'orig_gpt': GPTOutput(noun='modeling',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The architect worked tirelessly, perfecting his '\n",
      "                              'design with meticulous modeling upon modeling '\n",
      "                              'until his vision was finally realized.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the architect worked tirelessly , perfecting his design '\n",
      "                   'with meticulous modeling upon modeling until his vision '\n",
      "                   'was finally realized .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.3994016647338867,\n",
      " 'fills': [' model', ' modeling', ' models', ' drawing', ' modelling']}\n",
      "{'orig_gpt': GPTOutput(noun='jail',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He was a nomadic criminal, moving from jail to '\n",
      "                              'jail across the country as he repeatedly '\n",
      "                              'escaped and was recaptured.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he was a nomadic criminal , moving from jail to jail '\n",
      "                   'across the country as he repeatedly escaped and was '\n",
      "                   'recaptured .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8825374841690063,\n",
      " 'fills': [' jail', ' prison', ' house', ' cell', ' court']}\n",
      "{'orig_gpt': GPTOutput(noun='jail',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The reformist governor, determined to improve '\n",
      "                              'conditions for inmates, implemented new '\n",
      "                              'policies jail by jail across the state.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the reformist governor , determined to improve conditions '\n",
      "                   'for inmates , implemented new policies jail by jail across '\n",
      "                   'the state .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.000280368811218068,\n",
      " 'fills': [' followed', ' adopted', ' led', ' backed', ' required']}\n",
      "{'orig_gpt': GPTOutput(noun='vowel',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She studied linguistics so intensely that she '\n",
      "                              'was comparing sounds, vowel to vowel, to '\n",
      "                              'decipher the patterns in language.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she studied linguistics so intensely that she was '\n",
      "                   'comparing sounds , vowel to vowel , to decipher the '\n",
      "                   'patterns in language .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9420262575149536,\n",
      " 'fills': [' vowel', ' accent', ' word', ' especially', ' even']}\n",
      "{'orig_gpt': GPTOutput(noun='duplication',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientists continued their experiments, '\n",
      "                              'duplication after duplication, hoping to '\n",
      "                              'finally achieve consistent results.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the scientists continued their experiments , duplication '\n",
      "                   'after duplication , hoping to finally achieve consistent '\n",
      "                   'results .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.318020761013031,\n",
      " 'fills': [' duplication',\n",
      "           ' replication',\n",
      "           ' even',\n",
      "           ' duplicate',\n",
      "           ' replicated']}\n",
      "{'orig_gpt': GPTOutput(noun='duplication',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientists continued their experiments, '\n",
      "                              'duplication after duplication, hoping to '\n",
      "                              'finally achieve consistent results.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the scientists continued their experiments , duplication '\n",
      "                   'after duplication , hoping to finally achieve consistent '\n",
      "                   'results .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.7290446758270264,\n",
      " 'fills': [' duplication',\n",
      "           ' duplicate',\n",
      "           ' replication',\n",
      "           ' repetition',\n",
      "           ' replicate']}\n",
      "{'orig_gpt': GPTOutput(noun='duplication',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientist was able to map the human genome, '\n",
      "                              'duplication by duplication, until the entire '\n",
      "                              'sequence was complete.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the scientist was able to map the human genome , '\n",
      "                   'duplication by duplication , until the entire sequence was '\n",
      "                   'complete .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9168521761894226,\n",
      " 'fills': [' duplication', ' duplicate', ' replication', ' copy', ' doubling']}\n",
      "{'orig_gpt': GPTOutput(noun='duplication',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The researcher, striving for accuracy, '\n",
      "                              'painstakingly checked the data, ensuring that '\n",
      "                              'there was not duplication upon duplication in '\n",
      "                              'the results.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the researcher , striving for accuracy , painstakingly '\n",
      "                   'checked the data , ensuring that there was not duplication '\n",
      "                   'upon duplication in the results .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.942901611328125,\n",
      " 'fills': [' duplication',\n",
      "           ' duplicate',\n",
      "           ' repetition',\n",
      "           ' replication',\n",
      "           ' dupl']}\n",
      "{'orig_gpt': GPTOutput(noun='turnaround',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company saw improvements, implementing '\n",
      "                              'strategies from turnaround to turnaround, '\n",
      "                              'steadily increasing profits and employee '\n",
      "                              'morale.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company saw improvements , implementing strategies '\n",
      "                   'from turnaround to turnaround , steadily increasing '\n",
      "                   'profits and employee morale .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.03015352226793766,\n",
      " 'fills': [' restructuring',\n",
      "           ' growth',\n",
      "           ' consolidation',\n",
      "           ' expansion',\n",
      "           ' turnaround']}\n",
      "{'orig_gpt': GPTOutput(noun='turnaround',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company saw improvements, implementing '\n",
      "                              'strategies from turnaround to turnaround, '\n",
      "                              'steadily increasing profits and employee '\n",
      "                              'morale.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company saw improvements , implementing strategies '\n",
      "                   'from turnaround to turnaround , steadily increasing '\n",
      "                   'profits and employee morale .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.020181603729724884,\n",
      " 'fills': [' growth',\n",
      "           ' expansion',\n",
      "           ' restructuring',\n",
      "           ' acquisition',\n",
      "           ' consolidation']}\n",
      "{'orig_gpt': GPTOutput(noun='turnaround',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company experienced turnaround upon '\n",
      "                              'turnaround, proving the effectiveness of their '\n",
      "                              'new business strategy.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company experienced turnaround upon turnaround , '\n",
      "                   'proving the effectiveness of their new business strategy .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.007678953465074301,\n",
      " 'fills': [' growth', ' success', ' profitability', ' profits', ' sales']}\n",
      "{'orig_gpt': GPTOutput(noun='turnaround',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company experienced turnaround upon '\n",
      "                              'turnaround, proving the effectiveness of their '\n",
      "                              'new business strategy.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company experienced turnaround upon turnaround , '\n",
      "                   'proving the effectiveness of their new business strategy .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.003910203464329243,\n",
      " 'fills': [' launch',\n",
      "           ' acquisition',\n",
      "           ' implementation',\n",
      "           ' purchase',\n",
      "           ' completion']}\n",
      "{'orig_gpt': GPTOutput(noun='beginning',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The timeless cycle of life continues unbroken, '\n",
      "                              'from beginning to beginning, in an eternal '\n",
      "                              'dance of birth, death, and rebirth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the timeless cycle of life continues unbroken , from '\n",
      "                   'beginning to beginning , in an eternal dance of birth , '\n",
      "                   'death , and rebirth .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.07209816575050354,\n",
      " 'fills': [' end', ' ending', ' beginning', ' death', ' ends']}\n",
      "{'orig_gpt': GPTOutput(noun='beginning',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The timeless cycle of life continues unbroken, '\n",
      "                              'from beginning to beginning, in an eternal '\n",
      "                              'dance of birth, death, and rebirth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the timeless cycle of life continues unbroken , from '\n",
      "                   'beginning to beginning , in an eternal dance of birth , '\n",
      "                   'death , and rebirth .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.00015124148922041059,\n",
      " 'fills': [' end', ' ending', ' conclusion', ' ends', ' finish']}\n",
      "{'orig_gpt': GPTOutput(noun='beginning',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through the arduous process of writing her '\n",
      "                              'novel, she encountered beginning after '\n",
      "                              'beginning, each one leading her closer to the '\n",
      "                              'story she desired to tell.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through the arduous process of writing her novel , she '\n",
      "                   'encountered beginning after beginning , each one leading '\n",
      "                   'her closer to the story she desired to tell .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.9414827227592468,\n",
      " 'fills': [' beginning', ' ending', ' obstacle', ' problem', ' failure']}\n",
      "{'orig_gpt': GPTOutput(noun='beginning',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As she navigated through the complexities of '\n",
      "                              'life, she grew stronger and wiser, beginning by '\n",
      "                              'beginning, learning from each new start.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as she navigated through the complexities of life , she '\n",
      "                   'grew stronger and wiser , beginning by beginning , '\n",
      "                   'learning from each new start .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8082853555679321,\n",
      " 'fills': [' beginning', ' creating', ' starting', ' day', ' failing']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a teacher, she found herself moving from one '\n",
      "                              'annoying to annoying student, trying to '\n",
      "                              'maintain her patience.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a teacher , she found herself moving from one annoying '\n",
      "                   'to annoying student , trying to maintain her patience .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.012068255804479122,\n",
      " 'fills': [' student', ' annoying', ' annoyance', ' students', ' problem']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a teacher, she found herself moving from one '\n",
      "                              'annoying to annoying student, trying to '\n",
      "                              'maintain her patience.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a teacher , she found herself moving from one annoying '\n",
      "                   'to annoying student , trying to maintain her patience .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.2955643832683563,\n",
      " 'fills': [' another', ' annoying', ' irritating', ' obnoxious', ' other']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She had to endure annoying after annoying '\n",
      "                              'customer complaints throughout her shift.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she had to endure annoying after annoying customer '\n",
      "                   'complaints throughout her shift .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.604314386844635,\n",
      " 'fills': [' annoying', ' annoyance', ' complaint', ' customer', ' annoy']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She had to endure annoying after annoying '\n",
      "                              'customer complaints throughout her shift.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she had to endure annoying after annoying customer '\n",
      "                   'complaints throughout her shift .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.432466059923172,\n",
      " 'fills': [' annoying', ' and', ' hour', ' rude', ' hours']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The meeting dragged on, annoying by annoying, '\n",
      "                              'until everyone was utterly exhausted.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the meeting dragged on , annoying by annoying , until '\n",
      "                   'everyone was utterly exhausted .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.17018552124500275,\n",
      " 'fills': [' minute', ' hour', ' annoying', ' frustrating', ' frustrated']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The meeting dragged on, annoying by annoying, '\n",
      "                              'until everyone was utterly exhausted.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the meeting dragged on , annoying by annoying , until '\n",
      "                   'everyone was utterly exhausted .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.10954727232456207,\n",
      " 'fills': [' annoying', ' itself', ' minute', ' little', ' hour']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The meeting dragged on, with one annoying upon '\n",
      "                              'annoying topic being discussed with no end in '\n",
      "                              'sight.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the meeting dragged on , with one annoying upon annoying '\n",
      "                   'topic being discussed with no end in sight .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.002669496228918433,\n",
      " 'fills': [' topic', ' issue', ' thing', ' subject', ' annoying']}\n",
      "{'orig_gpt': GPTOutput(noun='annoying',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The meeting dragged on, with one annoying upon '\n",
      "                              'annoying topic being discussed with no end in '\n",
      "                              'sight.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the meeting dragged on , with one annoying upon annoying '\n",
      "                   'topic being discussed with no end in sight .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.6340867877006531,\n",
      " 'fills': [' annoying', ' another', ' irritating', ' a', ' obnoxious']}\n",
      "{'orig_gpt': GPTOutput(noun='lakh',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His wealth increased from lakh to lakh as he '\n",
      "                              'successfully invested in the stock market.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his wealth increased from lakh to lakh as he successfully '\n",
      "                   'invested in the stock market .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.1530579775571823,\n",
      " 'fills': [' lakh', ' 500', ' zero', ' hundreds', ' thousands']}\n",
      "{'orig_gpt': GPTOutput(noun='lakh',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His wealth increased from lakh to lakh as he '\n",
      "                              'successfully invested in the stock market.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his wealth increased from lakh to lakh as he successfully '\n",
      "                   'invested in the stock market .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.03490089252591133,\n",
      " 'fills': [' millions', ' crore', ' billions', ' lakh', ' billion']}\n",
      "{'orig_gpt': GPTOutput(noun='lakh',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='His business flourished, amassing wealth lakh '\n",
      "                              'after lakh, making him one of the richest men '\n",
      "                              'in the city.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'his business flourished , amassing wealth lakh after lakh '\n",
      "                   ', making him one of the richest men in the city .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.6778963804244995,\n",
      " 'fills': [' lakh', ' of', ' in', ' worth', ' crore']}\n",
      "{'orig_gpt': GPTOutput(noun='lakh',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The company's profits were increasing lakh by \"\n",
      "                              'lakh, signaling a successful fiscal year.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the company 's profits were increasing lakh by lakh , \"\n",
      "                   'signaling a successful fiscal year .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.4340199828147888,\n",
      " 'fills': [' lakh', ' sharply', ' year', ',', ' month']}\n",
      "{'orig_gpt': GPTOutput(noun='lakh',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The company's profits were increasing lakh by \"\n",
      "                              'lakh, signaling a successful fiscal year.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the company 's profits were increasing lakh by lakh , \"\n",
      "                   'signaling a successful fiscal year .',\n",
      " 'prep': 'by',\n",
      " 'score': 2.0549334294628352e-05,\n",
      " 'fills': [' quarter', ' year', ' half', ' month', ' percent']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He moved from city to city, pretending to '\n",
      "                              'pretending, never revealing his true identity.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he moved from city to city , pretending to pretending , '\n",
      "                   'never revealing his true identity .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9442890882492065,\n",
      " 'fills': [' pretending', ' lying', ' admitting', ' claiming', ' hiding']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He moved from city to city, pretending to '\n",
      "                              'pretending, never revealing his true identity.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he moved from city to city , pretending to pretending , '\n",
      "                   'never revealing his true identity .',\n",
      " 'prep': 'to',\n",
      " 'score': 5.651963874697685e-08,\n",
      " 'fills': [' work', ' be', ' disappear', ' kill', ' die']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As an actor, she lived a life of pretending '\n",
      "                              'after pretending, losing herself in the myriad '\n",
      "                              'of characters she portrayed.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as an actor , she lived a life of pretending after '\n",
      "                   'pretending , losing herself in the myriad of characters '\n",
      "                   'she portrayed .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.49533966183662415,\n",
      " 'fills': [' pretending', ' fantasy', ' pretend', ' illusion', ' delusion']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As an actor, she lived a life of pretending '\n",
      "                              'after pretending, losing herself in the myriad '\n",
      "                              'of characters she portrayed.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as an actor , she lived a life of pretending after '\n",
      "                   'pretending , losing herself in the myriad of characters '\n",
      "                   'she portrayed .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.4032718241214752,\n",
      " 'fills': [' pretending', ' acting', ' all', ' performing', ' filming']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He built his life pretending by pretending, '\n",
      "                              'creating a facade that fooled everyone around '\n",
      "                              'him.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he built his life pretending by pretending , creating a '\n",
      "                   'facade that fooled everyone around him .',\n",
      " 'prep': 'by',\n",
      " 'score': 7.977171662787441e-06,\n",
      " 'fills': [' simply', ' largely', ' around', ' entirely', ' mostly']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He built his life pretending by pretending, '\n",
      "                              'creating a facade that fooled everyone around '\n",
      "                              'him.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he built his life pretending by pretending , creating a '\n",
      "                   'facade that fooled everyone around him .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.037356261163949966,\n",
      " 'fills': [' deception', ' lying', ' deceit', ' pretending', ' design']}\n",
      "{'orig_gpt': GPTOutput(noun='pretending',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the world of politics, it is often a game of '\n",
      "                              'pretending upon pretending, as politicians try '\n",
      "                              'to keep up with their public image and private '\n",
      "                              'agendas.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the world of politics , it is often a game of '\n",
      "                   'pretending upon pretending , as politicians try to keep up '\n",
      "                   'with their public image and private agendas .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.7329440712928772,\n",
      " 'fills': [' pretending', ' lying', ' acting', ' insisting', ' playing']}\n",
      "{'orig_gpt': GPTOutput(noun='chin',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='After a long and stressful day, they sat in the '\n",
      "                              'quiet of their living room, sipping wine, chin '\n",
      "                              'to chin, sharing whispered secrets and dreams.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'after a long and stressful day , they sat in the quiet of '\n",
      "                   'their living room , sipping wine , chin to chin , sharing '\n",
      "                   'whispered secrets and dreams .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8204755783081055,\n",
      " 'fills': [' chin', ' cheek', ' nose', ' neck', ' face']}\n",
      "{'orig_gpt': GPTOutput(noun='chin',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='After a long and stressful day, they sat in the '\n",
      "                              'quiet of their living room, sipping wine, chin '\n",
      "                              'to chin, sharing whispered secrets and dreams.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'after a long and stressful day , they sat in the quiet of '\n",
      "                   'their living room , sipping wine , chin to chin , sharing '\n",
      "                   'whispered secrets and dreams .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9454494714736938,\n",
      " 'fills': [' chin', ' shoulder', ' cheek', ' ear', ' breast']}\n",
      "{'orig_gpt': GPTOutput(noun='chin',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The weight loss program was effective, reducing '\n",
      "                              'his appearance from chin after chin to a '\n",
      "                              'single, defined jawline.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the weight loss program was effective , reducing his '\n",
      "                   'appearance from chin after chin to a single , defined '\n",
      "                   'jawline .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.002180619165301323,\n",
      " 'fills': [' a', ' an', ' his', ' bul', ' the']}\n",
      "{'orig_gpt': GPTOutput(noun='chin',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He grew his beard, chin by chin, until it '\n",
      "                              'reached a formidable length.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he grew his beard , chin by chin , until it reached a '\n",
      "                   'formidable length .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8387623429298401,\n",
      " 'fills': [' chin', ' cheek', ' followed', ' jaw', ' beginning']}\n",
      "{'orig_gpt': GPTOutput(noun='chin',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he continued to gain weight, he developed '\n",
      "                              'chin upon chin, completely obscuring his neck.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he continued to gain weight , he developed chin upon '\n",
      "                   'chin , completely obscuring his neck .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.9116116762161255,\n",
      " 'fills': [' chin', ' a', ' an', ' the', ' beard']}\n",
      "{'orig_gpt': GPTOutput(noun='bible',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The priests debated fiercely, comparing '\n",
      "                              'scriptures from bible to bible, trying to find '\n",
      "                              'the best interpretation of the holy words.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the priests debated fiercely , comparing scriptures from '\n",
      "                   'bible to bible , trying to find the best interpretation of '\n",
      "                   'the holy words .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.46744468808174133,\n",
      " 'fills': [' bible', ' Quran', ' Koran', ' Torah', ' book']}\n",
      "{'orig_gpt': GPTOutput(noun='bible',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The priest, hoping to share the gospel far and '\n",
      "                              'wide, distributed the holy scriptures bible by '\n",
      "                              'bible.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the priest , hoping to share the gospel far and wide , '\n",
      "                   'distributed the holy scriptures bible by bible .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8184948563575745,\n",
      " 'fills': [' bible', ' book', ' Bible', ' one', ',']}\n",
      "{'orig_gpt': GPTOutput(noun='bible',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The priest, hoping to share the gospel far and '\n",
      "                              'wide, distributed the holy scriptures bible by '\n",
      "                              'bible.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the priest , hoping to share the gospel far and wide , '\n",
      "                   'distributed the holy scriptures bible by bible .',\n",
      " 'prep': 'by',\n",
      " 'score': 7.320844451896846e-05,\n",
      " 'fills': [' hand', ' mail', ' force', ' flashlight', ' letter']}\n",
      "{'orig_gpt': GPTOutput(noun='sock',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He cleaned his room diligently, organizing '\n",
      "                              'everything sock to sock, until there was no '\n",
      "                              'clutter left.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he cleaned his room diligently , organizing everything '\n",
      "                   'sock to sock , until there was no clutter left .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.5638238191604614,\n",
      " 'fills': [' sock', ' head', ' top', ' according', ' box']}\n",
      "{'orig_gpt': GPTOutput(noun='sock',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He cleaned his room diligently, organizing '\n",
      "                              'everything sock to sock, until there was no '\n",
      "                              'clutter left.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he cleaned his room diligently , organizing everything '\n",
      "                   'sock to sock , until there was no clutter left .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.011253327131271362,\n",
      " 'fills': [' toe', ' sock', ' foot', ' heel', ' boot']}\n",
      "{'orig_gpt': GPTOutput(noun='clergy',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The bishop stressed the importance of open '\n",
      "                              'communication, fostering dialogue from clergy '\n",
      "                              'to clergy, to ensure the unity of the church.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the bishop stressed the importance of open communication , '\n",
      "                   'fostering dialogue from clergy to clergy , to ensure the '\n",
      "                   'unity of the church .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.37184539437294006,\n",
      " 'fills': [' clergy', ' bishops', ' bishop', ' pope', ' hierarchy']}\n",
      "{'orig_gpt': GPTOutput(noun='clergy',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The bishop stressed the importance of open '\n",
      "                              'communication, fostering dialogue from clergy '\n",
      "                              'to clergy, to ensure the unity of the church.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the bishop stressed the importance of open communication , '\n",
      "                   'fostering dialogue from clergy to clergy , to ensure the '\n",
      "                   'unity of the church .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.18906699120998383,\n",
      " 'fills': [' lay', ' clergy', ' bishops', ' faithful', ' people']}\n",
      "{'orig_gpt': GPTOutput(noun='clergy',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The church managed to rebuild itself, clergy by '\n",
      "                              'clergy, after the devastating fire.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the church managed to rebuild itself , clergy by clergy , '\n",
      "                   'after the devastating fire .',\n",
      " 'prep': 'by',\n",
      " 'score': 1.6294923625537194e-05,\n",
      " 'fills': [' led', ' aided', ' assisted', ' helped', ' supported']}\n",
      "{'orig_gpt': GPTOutput(noun='clergy',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the midst of the religious revival, they '\n",
      "                              'gathered clergy upon clergy, all fervently '\n",
      "                              'preaching the gospel.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the midst of the religious revival , they gathered '\n",
      "                   'clergy upon clergy , all fervently preaching the gospel .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.889308512210846,\n",
      " 'fills': [' clergy', ' priests', ' them', ' churches', ' clerics']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company has seen consistent growth from '\n",
      "                              'purchasing to purchasing, impressing their '\n",
      "                              'investors with their financial acumen.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company has seen consistent growth from purchasing to '\n",
      "                   'purchasing , impressing their investors with their '\n",
      "                   'financial acumen .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.004596070386469364,\n",
      " 'fills': [' inception', ' selling', ' investing', ' sales', ' fundraising']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company has seen consistent growth from '\n",
      "                              'purchasing to purchasing, impressing their '\n",
      "                              'investors with their financial acumen.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company has seen consistent growth from purchasing to '\n",
      "                   'purchasing , impressing their investors with their '\n",
      "                   'financial acumen .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.012250546365976334,\n",
      " 'fills': [' acquisition', ' sale', ' purchase', ' sales', ' acquisitions']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company continued to grow, expanding its '\n",
      "                              'inventory purchasing after purchasing, until it '\n",
      "                              'became a leading retailer in the industry.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company continued to grow , expanding its inventory '\n",
      "                   'purchasing after purchasing , until it became a leading '\n",
      "                   'retailer in the industry .',\n",
      " 'prep': 'after',\n",
      " 'score': 8.554044325137511e-05,\n",
      " 'fills': [' even', ' rapidly', ' size', ' base', ' quickly']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company continued to grow, expanding its '\n",
      "                              'inventory purchasing after purchasing, until it '\n",
      "                              'became a leading retailer in the industry.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company continued to grow , expanding its inventory '\n",
      "                   'purchasing after purchasing , until it became a leading '\n",
      "                   'retailer in the industry .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.010356343351304531,\n",
      " 'fills': [' acquisitions', ' acquisition', ' year', ' purchase', ' the']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company saw its profits rise, purchasing by '\n",
      "                              'purchasing, as its efficient procurement '\n",
      "                              'strategy paid off.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company saw its profits rise , purchasing by '\n",
      "                   'purchasing , as its efficient procurement strategy paid '\n",
      "                   'off .',\n",
      " 'prep': 'by',\n",
      " 'score': 1.6231304471148178e-05,\n",
      " 'fills': [' driven', ' aided', ' boosted', ' mostly', ' fueled']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company saw its profits rise, purchasing by '\n",
      "                              'purchasing, as its efficient procurement '\n",
      "                              'strategy paid off.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company saw its profits rise , purchasing by '\n",
      "                   'purchasing , as its efficient procurement strategy paid '\n",
      "                   'off .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0026083816774189472,\n",
      " 'fills': [' half', ' volume', ' itself', ' revenue', ' acquisition']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company continued to grow, with purchasing '\n",
      "                              'upon purchasing of new equipment, expanding its '\n",
      "                              'production capabilities.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company continued to grow , with purchasing upon '\n",
      "                   'purchasing of new equipment , expanding its production '\n",
      "                   'capabilities .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.01222731452435255,\n",
      " 'fills': [' the', ' acquisitions', ' expansion', ' and', ' purchase']}\n",
      "{'orig_gpt': GPTOutput(noun='purchasing',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company continued to grow, with purchasing '\n",
      "                              'upon purchasing of new equipment, expanding its '\n",
      "                              'production capabilities.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company continued to grow , with purchasing upon '\n",
      "                   'purchasing of new equipment , expanding its production '\n",
      "                   'capabilities .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.7115978002548218,\n",
      " 'fills': [' purchasing', ' purchase', ' purchases', ' buying', ' procurement']}\n",
      "{'orig_gpt': GPTOutput(noun='rapport',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a relationship coach, I observed the '\n",
      "                              \"couple's interaction, seeing them build trust \"\n",
      "                              'from rapport to rapport.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"as a relationship coach , i observed the couple 's \"\n",
      "                   'interaction , seeing them build trust from rapport to '\n",
      "                   'rapport .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.001316477544605732,\n",
      " 'fills': [' friendship',\n",
      "           ' misunderstanding',\n",
      "           ' familiarity',\n",
      "           ' respect',\n",
      "           ' distance']}\n",
      "{'orig_gpt': GPTOutput(noun='rapport',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a relationship coach, I observed the '\n",
      "                              \"couple's interaction, seeing them build trust \"\n",
      "                              'from rapport to rapport.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"as a relationship coach , i observed the couple 's \"\n",
      "                   'interaction , seeing them build trust from rapport to '\n",
      "                   'rapport .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.007230398245155811,\n",
      " 'fills': [' intimacy', ' commitment', ' love', ' relationship', ' friendship']}\n",
      "{'orig_gpt': GPTOutput(noun='rapport',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Building strong relationships with her clients, '\n",
      "                              'she established trust and understanding rapport '\n",
      "                              'by rapport.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'building strong relationships with her clients , she '\n",
      "                   'established trust and understanding rapport by rapport .',\n",
      " 'prep': 'by',\n",
      " 'score': 1.9208498997613788e-05,\n",
      " 'fills': [' based', ' driven', ',', ',', ' only']}\n",
      "{'orig_gpt': GPTOutput(noun='rapport',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Building strong relationships with her clients, '\n",
      "                              'she established trust and understanding rapport '\n",
      "                              'by rapport.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'building strong relationships with her clients , she '\n",
      "                   'established trust and understanding rapport by rapport .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0005564881721511483,\n",
      " 'fills': [' example', ' listening', ' time', ' communicating', ' negotiation']}\n",
      "{'orig_gpt': GPTOutput(noun='department',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The CEO decided to restructure the company, '\n",
      "                              'reviewing the finances department by '\n",
      "                              'department, to ensure maximum efficiency.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the ceo decided to restructure the company , reviewing the '\n",
      "                   'finances department by department , to ensure maximum '\n",
      "                   'efficiency .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.944205641746521,\n",
      " 'fills': [' department', ' division', ' Department', ',', ' section']}\n",
      "{'orig_gpt': GPTOutput(noun='cm',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The tailor meticulously measured the fabric, cm '\n",
      "                              'to cm, to ensure the perfect fit.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the tailor meticulously measured the fabric , cm to cm , '\n",
      "                   'to ensure the perfect fit .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.10636131465435028,\n",
      " 'fills': [' inch', ' down', ' cm', ' meter', ' mm']}\n",
      "{'orig_gpt': GPTOutput(noun='cm',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The tailor meticulously measured the fabric, cm '\n",
      "                              'to cm, to ensure the perfect fit.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the tailor meticulously measured the fabric , cm to cm , '\n",
      "                   'to ensure the perfect fit .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9345642924308777,\n",
      " 'fills': [' cm', ' mm', ' inch', 'cm', ' meter']}\n",
      "{'orig_gpt': GPTOutput(noun='cm',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The snow piled up outside my window, cm upon '\n",
      "                              'cm, as the blizzard raged on.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the snow piled up outside my window , cm upon cm , as the '\n",
      "                   'blizzard raged on .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.9421893954277039,\n",
      " 'fills': [' cm', 'cm', ' c', ' cent', ' centimeters']}\n",
      "{'orig_gpt': GPTOutput(noun='cable',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked tirelessly, connecting cable to '\n",
      "                              'cable, ensuring the entire network system was '\n",
      "                              'operational.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked tirelessly , connecting cable to cable , '\n",
      "                   'ensuring the entire network system was operational .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.6504114866256714,\n",
      " 'fills': [' cable', ' cables', ' wire', ' wires', ' fiber']}\n",
      "{'orig_gpt': GPTOutput(noun='cable',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked tirelessly, connecting cable to '\n",
      "                              'cable, ensuring the entire network system was '\n",
      "                              'operational.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked tirelessly , connecting cable to cable , '\n",
      "                   'ensuring the entire network system was operational .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8996871113777161,\n",
      " 'fills': [' cable', ' cables', ' wire', ' computer', ' server']}\n",
      "{'orig_gpt': GPTOutput(noun='cable',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The bridge was built cable by cable, over the '\n",
      "                              'span of many months.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the bridge was built cable by cable , over the span of '\n",
      "                   'many months .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0007089429418556392,\n",
      " 'fills': [',', ' entirely', ',', ' mostly', ' largely']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In this industry, we innovate from combustion '\n",
      "                              'to combustion, constantly improving the '\n",
      "                              'efficiency of our engines.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in this industry , we innovate from combustion to '\n",
      "                   'combustion , constantly improving the efficiency of our '\n",
      "                   'engines .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.1560288816690445,\n",
      " 'fills': [' combustion', ' design', ' fuel', ' conception', ' steam']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In this industry, we innovate from combustion '\n",
      "                              'to combustion, constantly improving the '\n",
      "                              'efficiency of our engines.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in this industry , we innovate from combustion to '\n",
      "                   'combustion , constantly improving the efficiency of our '\n",
      "                   'engines .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.41617321968078613,\n",
      " 'fills': [' combustion', ' compression', ' propulsion', ' power', ' fuel']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The factory spewed out smoke, causing '\n",
      "                              'combustion after combustion, turning the once '\n",
      "                              'clear skies into a permanent haze.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the factory spewed out smoke , causing combustion after '\n",
      "                   'combustion , turning the once clear skies into a permanent '\n",
      "                   'haze .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.011028511449694633,\n",
      " 'fills': [' fires', ' explosions', ' ash', ' fire', ' an']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The factory spewed out smoke, causing '\n",
      "                              'combustion after combustion, turning the once '\n",
      "                              'clear skies into a permanent haze.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the factory spewed out smoke , causing combustion after '\n",
      "                   'combustion , turning the once clear skies into a permanent '\n",
      "                   'haze .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.30627506971359253,\n",
      " 'fills': [' combustion', ' explosion', ' ignition', ' fire', ' explosions']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientist meticulously analyzed the '\n",
      "                              \"engine's efficiency, examining it combustion by \"\n",
      "                              'combustion, to ensure optimum performance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the scientist meticulously analyzed the engine 's \"\n",
      "                   'efficiency , examining it combustion by combustion , to '\n",
      "                   'ensure optimum performance .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.5240558981895447,\n",
      " 'fills': [' combustion', ' side', ' burnt', ' generated', ' directly']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientist meticulously analyzed the '\n",
      "                              \"engine's efficiency, examining it combustion by \"\n",
      "                              'combustion, to ensure optimum performance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the scientist meticulously analyzed the engine 's \"\n",
      "                   'efficiency , examining it combustion by combustion , to '\n",
      "                   'ensure optimum performance .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9195676445960999,\n",
      " 'fills': [' combustion', ' fire', ' oxidation', ' burn', ' side']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the wildfires raged on, they created a '\n",
      "                              'devastating cycle of combustion upon '\n",
      "                              'combustion, turning the lush forest into a '\n",
      "                              'charred wasteland.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the wildfires raged on , they created a devastating '\n",
      "                   'cycle of combustion upon combustion , turning the lush '\n",
      "                   'forest into a charred wasteland .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.8240962028503418,\n",
      " 'fills': [' combustion', ' fire', ' ignition', ' growth', ' burning']}\n",
      "{'orig_gpt': GPTOutput(noun='combustion',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the wildfires raged on, they created a '\n",
      "                              'devastating cycle of combustion upon '\n",
      "                              'combustion, turning the lush forest into a '\n",
      "                              'charred wasteland.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the wildfires raged on , they created a devastating '\n",
      "                   'cycle of combustion upon combustion , turning the lush '\n",
      "                   'forest into a charred wasteland .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.1835051029920578,\n",
      " 'fills': [' themselves', ' combustion', ' itself', ' ignition', ' fire']}\n",
      "{'orig_gpt': GPTOutput(noun='memo',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As the investigation progressed, they sifted '\n",
      "                              'through evidence, going from memo to memo, to '\n",
      "                              'piece together the truth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as the investigation progressed , they sifted through '\n",
      "                   'evidence , going from memo to memo , to piece together the '\n",
      "                   'truth .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9305232167243958,\n",
      " 'fills': [' memo', ' document', ' email', ' letter', ' note']}\n",
      "{'orig_gpt': GPTOutput(noun='serve',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked diligently, serve to serve, in his '\n",
      "                              'mission to help the less fortunate.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked diligently , serve to serve , in his mission to '\n",
      "                   'help the less fortunate .',\n",
      " 'prep': 'to',\n",
      " 'score': 9.676690751803108e-06,\n",
      " 'fills': [' willing', ' eager', ' and', ' ready', ' always']}\n",
      "{'orig_gpt': GPTOutput(noun='serve',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He worked diligently, serve to serve, in his '\n",
      "                              'mission to help the less fortunate.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he worked diligently , serve to serve , in his mission to '\n",
      "                   'help the less fortunate .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.09587682783603668,\n",
      " 'fills': [' perfection', ' others', ' serve', ' death', ' God']}\n",
      "{'orig_gpt': GPTOutput(noun='km',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The marathon runner trained relentlessly, '\n",
      "                              'increasing his endurance km to km until he '\n",
      "                              'could complete the full distance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the marathon runner trained relentlessly , increasing his '\n",
      "                   'endurance km to km until he could complete the full '\n",
      "                   'distance .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.060805793851614,\n",
      " 'fills': [' by', ' according', ' from', ' km', ' up']}\n",
      "{'orig_gpt': GPTOutput(noun='km',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The marathon runner trained relentlessly, '\n",
      "                              'increasing his endurance km to km until he '\n",
      "                              'could complete the full distance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the marathon runner trained relentlessly , increasing his '\n",
      "                   'endurance km to km until he could complete the full '\n",
      "                   'distance .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9452461004257202,\n",
      " 'fills': [' km', 'km', ' day', ' mile', ' kilometres']}\n",
      "{'orig_gpt': GPTOutput(noun='km',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As we drove across the barren desert, we '\n",
      "                              'measured our progress km by km.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as we drove across the barren desert , we measured our '\n",
      "                   'progress km by km .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7503317594528198,\n",
      " 'fills': [' km', ',', ' kilometers', ' literally', ' kilometres']}\n",
      "{'orig_gpt': GPTOutput(noun='primary',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She worked diligently, moving from primary to '\n",
      "                              'primary, to ensure every child understood the '\n",
      "                              'lesson.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she worked diligently , moving from primary to primary , '\n",
      "                   'to ensure every child understood the lesson .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8223798274993896,\n",
      " 'fills': [' primary',\n",
      "           ' secondary',\n",
      "           ' elementary',\n",
      "           ' kindergarten',\n",
      "           ' preschool']}\n",
      "{'orig_gpt': GPTOutput(noun='primary',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='She worked diligently, moving from primary to '\n",
      "                              'primary, to ensure every child understood the '\n",
      "                              'lesson.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'she worked diligently , moving from primary to primary , '\n",
      "                   'to ensure every child understood the lesson .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.19027678668498993,\n",
      " 'fills': [' secondary',\n",
      "           ' primary',\n",
      "           ' elementary',\n",
      "           ' kindergarten',\n",
      "           ' intermediate']}\n",
      "{'orig_gpt': GPTOutput(noun='primary',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The presidential candidate tirelessly '\n",
      "                              \"campaigned, winning the voters' trust primary \"\n",
      "                              \"by primary, until he secured his party's \"\n",
      "                              'nomination.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the presidential candidate tirelessly campaigned , winning '\n",
      "                   \"the voters ' trust primary by primary , until he secured \"\n",
      "                   \"his party 's nomination .\",\n",
      " 'prep': 'by',\n",
      " 'score': 0.07510286569595337,\n",
      " 'fills': [' landslide', ' one', ' primary', ' hand', ' referendum']}\n",
      "{'orig_gpt': GPTOutput(noun='concentration',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he moved through his rigorous studies, he '\n",
      "                              'transitioned from concentration to '\n",
      "                              'concentration, mastering each subject before '\n",
      "                              'moving to the next.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he moved through his rigorous studies , he transitioned '\n",
      "                   'from concentration to concentration , mastering each '\n",
      "                   'subject before moving to the next .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9455424547195435,\n",
      " 'fills': [' concentration', ' focus', ' relaxation', ' study', ' intensity']}\n",
      "{'orig_gpt': GPTOutput(noun='concentration',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As he moved through his rigorous studies, he '\n",
      "                              'transitioned from concentration to '\n",
      "                              'concentration, mastering each subject before '\n",
      "                              'moving to the next.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as he moved through his rigorous studies , he transitioned '\n",
      "                   'from concentration to concentration , mastering each '\n",
      "                   'subject before moving to the next .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.9468191266059875,\n",
      " 'fills': [' concentration', ' mastery', ' focus', ' relaxation', ' intensity']}\n",
      "{'orig_gpt': GPTOutput(noun='concentration',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The scientist carefully analyzed the data, '\n",
      "                              'examining each sample concentration by '\n",
      "                              'concentration, to ensure the accuracy of his '\n",
      "                              'results.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the scientist carefully analyzed the data , examining each '\n",
      "                   'sample concentration by concentration , to ensure the '\n",
      "                   'accuracy of his results .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.002930230228230357,\n",
      " 'fills': [' hand', ' itself', ' step', ' eye', ' letter']}\n",
      "{'orig_gpt': GPTOutput(noun='critic',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The film's controversial themes sparked heated \"\n",
      "                              'debates that went critic to critic, causing a '\n",
      "                              'stir in the industry.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the film 's controversial themes sparked heated debates \"\n",
      "                   'that went critic to critic , causing a stir in the '\n",
      "                   'industry .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.00026046266430057585,\n",
      " 'fills': [' online', ' on', ' back', ' out', ' directly']}\n",
      "{'orig_gpt': GPTOutput(noun='critic',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output=\"The film's controversial themes sparked heated \"\n",
      "                              'debates that went critic to critic, causing a '\n",
      "                              'stir in the industry.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': \"the film 's controversial themes sparked heated debates \"\n",
      "                   'that went critic to critic , causing a stir in the '\n",
      "                   'industry .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0003626873076427728,\n",
      " 'fills': [' Twitter', ' bottom', ' Hollywood', ' end', ' online']}\n",
      "{'orig_gpt': GPTOutput(noun='conditioning',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through the rigorous training program, she '\n",
      "                              'progressed from conditioning to conditioning, '\n",
      "                              'building her strength and endurance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through the rigorous training program , she progressed '\n",
      "                   'from conditioning to conditioning , building her strength '\n",
      "                   'and endurance .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.010355707257986069,\n",
      " 'fills': [' strength', ' speed', ' running', ' training', ' technique']}\n",
      "{'orig_gpt': GPTOutput(noun='conditioning',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through the rigorous training program, she '\n",
      "                              'progressed from conditioning to conditioning, '\n",
      "                              'building her strength and endurance.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through the rigorous training program , she progressed '\n",
      "                   'from conditioning to conditioning , building her strength '\n",
      "                   'and endurance .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.02258627861738205,\n",
      " 'fills': [' strength', ' endurance', ' competition', ' speed', ' training']}\n",
      "{'orig_gpt': GPTOutput(noun='conditioning',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through rigorous training and mental '\n",
      "                              'preparation, they strived to achieve peak '\n",
      "                              'performance, improving conditioning by '\n",
      "                              'conditioning until they were in the best shape '\n",
      "                              'of their lives.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through rigorous training and mental preparation , they '\n",
      "                   'strived to achieve peak performance , improving '\n",
      "                   'conditioning by conditioning until they were in the best '\n",
      "                   'shape of their lives .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.7018455266952515,\n",
      " 'fills': [' conditioning',\n",
      "           ' condition',\n",
      "           ' themselves',\n",
      "           ' fitness',\n",
      "           ' strength']}\n",
      "{'orig_gpt': GPTOutput(noun='conditioning',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through rigorous training and mental '\n",
      "                              'preparation, they strived to achieve peak '\n",
      "                              'performance, improving conditioning by '\n",
      "                              'conditioning until they were in the best shape '\n",
      "                              'of their lives.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through rigorous training and mental preparation , they '\n",
      "                   'strived to achieve peak performance , improving '\n",
      "                   'conditioning by conditioning until they were in the best '\n",
      "                   'shape of their lives .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8274653553962708,\n",
      " 'fills': [' conditioning', ' day', ' condition', ' training', ' week']}\n",
      "{'orig_gpt': GPTOutput(noun='conditioning',\n",
      "                       prep='upon',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The athlete endured a rigorous training '\n",
      "                              'program, conditioning upon conditioning, in '\n",
      "                              'preparation for the upcoming Olympics.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the athlete endured a rigorous training program , '\n",
      "                   'conditioning upon conditioning , in preparation for the '\n",
      "                   'upcoming olympics .',\n",
      " 'prep': 'upon',\n",
      " 'score': 0.003341053379699588,\n",
      " 'fills': [' based', ' focused', ' focusing', ' built', ' emphasis']}\n",
      "{'orig_gpt': GPTOutput(noun='scrub',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The gardener worked diligently, clearing the '\n",
      "                              'overgrown garden scrub by scrub, until it was '\n",
      "                              'finally a space to be proud of.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the gardener worked diligently , clearing the overgrown '\n",
      "                   'garden scrub by scrub , until it was finally a space to be '\n",
      "                   'proud of .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.4137713313102722,\n",
      " 'fills': [' scrub', ' surrounded', ' overrun', ' brush', ' bush']}\n",
      "{'orig_gpt': GPTOutput(noun='scrub',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The gardener worked diligently, clearing the '\n",
      "                              'overgrown garden scrub by scrub, until it was '\n",
      "                              'finally a space to be proud of.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the gardener worked diligently , clearing the overgrown '\n",
      "                   'garden scrub by scrub , until it was finally a space to be '\n",
      "                   'proud of .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8264368772506714,\n",
      " 'fills': [' scrub', ' brush', ' bush', ' crab', ' scrap']}\n",
      "{'orig_gpt': GPTOutput(noun='ambiguity',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the realm of philosophy, we often move from '\n",
      "                              'ambiguity to ambiguity, constantly seeking '\n",
      "                              'clarity and truth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the realm of philosophy , we often move from ambiguity '\n",
      "                   'to ambiguity , constantly seeking clarity and truth .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.21538017690181732,\n",
      " 'fills': [' clarity',\n",
      "           ' ambiguity',\n",
      "           ' certainty',\n",
      "           ' uncertainty',\n",
      "           ' confusion']}\n",
      "{'orig_gpt': GPTOutput(noun='ambiguity',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the realm of philosophy, we often move from '\n",
      "                              'ambiguity to ambiguity, constantly seeking '\n",
      "                              'clarity and truth.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the realm of philosophy , we often move from ambiguity '\n",
      "                   'to ambiguity , constantly seeking clarity and truth .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.3491118252277374,\n",
      " 'fills': [' ambiguity',\n",
      "           ' confusion',\n",
      "           ' contradiction',\n",
      "           ' uncertainty',\n",
      "           ' clarity']}\n",
      "{'orig_gpt': GPTOutput(noun='ambiguity',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='In the poorly written contract, they found '\n",
      "                              'themselves mired in ambiguity after ambiguity, '\n",
      "                              'making it increasingly difficult to understand '\n",
      "                              'the terms.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'in the poorly written contract , they found themselves '\n",
      "                   'mired in ambiguity after ambiguity , making it '\n",
      "                   'increasingly difficult to understand the terms .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.6703281402587891,\n",
      " 'fills': [' ambiguity',\n",
      "           ' confusion',\n",
      "           ' uncertainty',\n",
      "           ' contradiction',\n",
      "           ' another']}\n",
      "{'orig_gpt': GPTOutput(noun='ambiguity',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through the philosophical discussions, they '\n",
      "                              'delved deeper into the subject, unraveling '\n",
      "                              'ambiguity by ambiguity, until clarity was '\n",
      "                              'achieved.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through the philosophical discussions , they delved deeper '\n",
      "                   'into the subject , unraveling ambiguity by ambiguity , '\n",
      "                   'until clarity was achieved .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.9015949368476868,\n",
      " 'fills': [' ambiguity', ' confusion', ' it', ' uncertainty', ' contradiction']}\n",
      "{'orig_gpt': GPTOutput(noun='ambiguity',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='Through the philosophical discussions, they '\n",
      "                              'delved deeper into the subject, unraveling '\n",
      "                              'ambiguity by ambiguity, until clarity was '\n",
      "                              'achieved.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'through the philosophical discussions , they delved deeper '\n",
      "                   'into the subject , unraveling ambiguity by ambiguity , '\n",
      "                   'until clarity was achieved .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.889687180519104,\n",
      " 'fills': [' ambiguity',\n",
      "           ' contradiction',\n",
      "           ' confusion',\n",
      "           ' implication',\n",
      "           ' nuance']}\n",
      "{'orig_gpt': GPTOutput(noun='jazz',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He traveled from city to city, playing jazz to '\n",
      "                              'jazz nightclubs, honing his skills and making a '\n",
      "                              'name for himself.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he traveled from city to city , playing jazz to jazz '\n",
      "                   'nightclubs , honing his skills and making a name for '\n",
      "                   'himself .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.020394455641508102,\n",
      " 'fills': [' piano', ' regularly', ' gigs', ' guitar', ' bass']}\n",
      "{'orig_gpt': GPTOutput(noun='jazz',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='He traveled from city to city, playing jazz to '\n",
      "                              'jazz nightclubs, honing his skills and making a '\n",
      "                              'name for himself.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'he traveled from city to city , playing jazz to jazz '\n",
      "                   'nightclubs , honing his skills and making a name for '\n",
      "                   'himself .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.06848637014627457,\n",
      " 'fills': [' local', ' jazz', ' the', ' popular', ' packed']}\n",
      "{'orig_gpt': GPTOutput(noun='jazz',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='At the music festival, they played jazz after '\n",
      "                              'jazz, captivating the audience with each unique '\n",
      "                              'melody.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'at the music festival , they played jazz after jazz , '\n",
      "                   'captivating the audience with each unique melody .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.9240860342979431,\n",
      " 'fills': [' jazz', ' right', ' blues', ' rock', ' Jazz']}\n",
      "{'orig_gpt': GPTOutput(noun='jazz',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The musician improved his performance, jazz by '\n",
      "                              'jazz, until he was ready for the big stage.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the musician improved his performance , jazz by jazz , '\n",
      "                   'until he was ready for the big stage .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.0003582463541533798,\n",
      " 'fills': [' inspired', ' influenced', ' aided', ' helped', ' guided']}\n",
      "{'orig_gpt': GPTOutput(noun='investor',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The stock trading tips were shared investor to '\n",
      "                              'investor, in hushed whispers across the busy '\n",
      "                              'trading floor.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the stock trading tips were shared investor to investor , '\n",
      "                   'in hushed whispers across the busy trading floor .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.8250135779380798,\n",
      " 'fills': [' investor', ' insider', ' employee', ' investors', ' shareholder']}\n",
      "{'orig_gpt': GPTOutput(noun='investor',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The company built its reputation and client '\n",
      "                              'base slowly, securing trust and capital '\n",
      "                              'investor by investor.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the company built its reputation and client base slowly , '\n",
      "                   'securing trust and capital investor by investor .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.02212749980390072,\n",
      " 'fills': [' investment', ' invested', ' raised', ' backed', ' provided']}\n",
      "{'orig_gpt': GPTOutput(noun='retro',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The fashion designer, inspired by past eras, '\n",
      "                              'meticulously matched each outfit, retro to '\n",
      "                              'retro, to recreate an authentic vintage '\n",
      "                              'collection.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the fashion designer , inspired by past eras , '\n",
      "                   'meticulously matched each outfit , retro to retro , to '\n",
      "                   'recreate an authentic vintage collection .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.05279744416475296,\n",
      " 'fills': [' contemporary', ' modern', ' classic', ' vintage', ' retro']}\n",
      "{'orig_gpt': GPTOutput(noun='retro',\n",
      "                       prep='to',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The fashion designer, inspired by past eras, '\n",
      "                              'meticulously matched each outfit, retro to '\n",
      "                              'retro, to recreate an authentic vintage '\n",
      "                              'collection.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the fashion designer , inspired by past eras , '\n",
      "                   'meticulously matched each outfit , retro to retro , to '\n",
      "                   'recreate an authentic vintage collection .',\n",
      " 'prep': 'to',\n",
      " 'score': 0.0349888950586319,\n",
      " 'fills': [' contemporary', ' modern', ' classic', ' retro', ' current']}\n",
      "{'orig_gpt': GPTOutput(noun='retro',\n",
      "                       prep='after',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='As a vintage clothing collector, she sifted '\n",
      "                              'through the thrift store racks, finding retro '\n",
      "                              'after retro to add to her impressive '\n",
      "                              'collection.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'as a vintage clothing collector , she sifted through the '\n",
      "                   'thrift store racks , finding retro after retro to add to '\n",
      "                   'her impressive collection .',\n",
      " 'prep': 'after',\n",
      " 'score': 0.8551589250564575,\n",
      " 'fills': [' retro', 'thought', ' classic', ' style', ' vintage']}\n",
      "{'orig_gpt': GPTOutput(noun='retro',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The vintage shop came together retro by retro, '\n",
      "                              'as the owner meticulously curated items from '\n",
      "                              'different eras.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the vintage shop came together retro by retro , as the '\n",
      "                   'owner meticulously curated items from different eras .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.00011315989831928164,\n",
      " 'fills': [' inspired', ' influenced', ' surrounded', ' driven', 'inspired']}\n",
      "{'orig_gpt': GPTOutput(noun='retro',\n",
      "                       prep='by',\n",
      "                       model='gpt-4-0613',\n",
      "                       output='The vintage shop came together retro by retro, '\n",
      "                              'as the owner meticulously curated items from '\n",
      "                              'different eras.',\n",
      "                       finish_reason='stop'),\n",
      " 'tokenized_sent': 'the vintage shop came together retro by retro , as the '\n",
      "                   'owner meticulously curated items from different eras .',\n",
      " 'prep': 'by',\n",
      " 'score': 0.8711778521537781,\n",
      " 'fills': [' retro', ' vintage', ' nostalgia', ' piece', ' design']}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T00:52:09.401137Z",
     "start_time": "2025-04-23T00:52:09.258452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# verify that these results match prev paper results for roberta-large\n",
    "# this is basically copied from exp_npn\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot():\n",
    "    aggregator: Dict[str, List[float]] = defaultdict(list)\n",
    "    for res in npn_results:\n",
    "        aggregator[res.prep].append(res.score)\n",
    "\n",
    "    print('counts')\n",
    "    for k in aggregator.keys():\n",
    "        print(k, len(aggregator[k]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.set_ylim(-.05, 1.05)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')  # Set dark color (or 'white' for light mode)\n",
    "\n",
    "    cat_val_tuples = [(key, val) for key, vals in aggregator.items() for val in vals]\n",
    "    order = ['upon', 'after','by', 'to']\n",
    "    cat_val_tuples_sorted = sorted(cat_val_tuples, key=lambda x: order.index(x[0]))\n",
    "    categories, values = zip(*cat_val_tuples_sorted)\n",
    "    print(len(values))\n",
    "\n",
    "\n",
    "    # for two cats\n",
    "    cats_prettied = categories\n",
    "\n",
    "    # sns.boxplot(x=cats_prettied, y=values)\n",
    "    sns.violinplot(x=cats_prettied, y=values, ax=ax,inner='quartile', cut=0, edgecolor=\"black\",saturation=0.1, scale=\"width\")\n",
    "    # Overlay Data Points (Jittered for Visibility)\n",
    "    for violin in ax.collections:\n",
    "        violin.set_facecolor(\"lightgray\")\n",
    "    sns.stripplot(x=cats_prettied, y=values, ax=ax,\n",
    "                  color=\"black\", alpha=0.5, size=3, jitter=False)  # Adjust size & alpha\n",
    "plot()\n",
    "\n"
   ],
   "id": "107527488ee7e91c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts\n",
      "to 196\n",
      "after 196\n",
      "by 196\n",
      "upon 196\n",
      "784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/58/nkm5hbs97tz3vc0x5l5zl4w40000gn/T/ipykernel_20759/1606702605.py:34: FutureWarning: \n",
      "\n",
      "The `scale` parameter has been renamed and will be removed in v0.15.0. Pass `density_norm='width'` for the same effect.\n",
      "  sns.violinplot(x=cats_prettied, y=values, ax=ax,inner='quartile', cut=0, edgecolor=\"black\",saturation=0.1, scale=\"width\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6739423541807c10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
