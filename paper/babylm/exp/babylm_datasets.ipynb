{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:55.510704Z",
     "start_time": "2025-05-15T00:29:55.440179Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# check how often let alone and much less occur in datasets\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:56.973569Z",
     "start_time": "2025-05-15T00:29:55.694750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ltg/babylm-2024-baby-cosmo-fine-100m\", split=\"train\")\n",
    "# ds = load_dataset(\"ltg/babylm-2024-baby-cosmo-fine-10m\", split=\"train\")\n",
    "# matches = ds.filter(lambda example: \"Biden\" in example[\"article\"])\n",
    "#\n",
    "# print(matches[0][\"article\"])\n",
    "\n"
   ],
   "id": "1071d5c414d4f10d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:57.070755Z",
     "start_time": "2025-05-15T00:29:56.979952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from pprint import pp\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def find_so_that():\n",
    "    so_ex = ds.filter(lambda x: \" so \" in x[\"text\"])\n",
    "    pp(so_ex)\n",
    "\n",
    "    all = []\n",
    "    for i, ex in enumerate(so_ex):\n",
    "        if i > 1000: break\n",
    "\n",
    "        # print(ex[\"text\"])\n",
    "        lines = ex[\"text\"].split(\"\\n\")\n",
    "        for l in lines:\n",
    "            pattern = r\"\\bso\\s+(\\w+)\\s+that\\b\"\n",
    "            matches = re.findall(pattern, l)\n",
    "            if matches:\n",
    "                all.append((l, matches))\n",
    "\n",
    "    return all\n",
    "\n",
    "\n",
    "res = find_so_that()\n",
    "print(len(res))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "cc9b9fdbc1d4824d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 77180\n",
      "})\n",
      "64\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:57.271596Z",
     "start_time": "2025-05-15T00:29:57.258848Z"
    }
   },
   "cell_type": "code",
   "source": "pp(res)",
   "id": "25d0889d11e8be8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Genital HPV is the most common sexually transmitted infection in the United '\n",
      "  'States. It is so pervasive that it has been called the common cold of the '\n",
      "  'sexually active world.',\n",
      "  ['pervasive']),\n",
      " ('Aaliyah was so excited that she ran straight home to tell her family. They '\n",
      "  'were thrilled for her and helped her prepare her application. Aaliyah had '\n",
      "  'worked hard in school and earned good grades in subjects like civil '\n",
      "  'engineering, urban planning, and social sciences. She also knew how to use '\n",
      "  'computers really well, which would help her a lot in this internship.',\n",
      "  ['excited']),\n",
      " ('Now it so happened that the Rev. John Mansfield was not famous for '\n",
      "  'descriptions, but he did draw a certain picture of Kathleen Desmond which '\n",
      "  'was not in the least like that young lady, but which abundantly satisfied '\n",
      "  'her child. Her cheeks grew redder than ever as she listened and she panted '\n",
      "  'slightly as she snuggled against her beloved uncle.',\n",
      "  ['happened']),\n",
      " ('It so happened that after his last interview with little Margot St. Juste, '\n",
      "  'the Rev. John Mansfield became subject to a strange uneasiness of '\n",
      "  'conscience. Never before had he attempted to do anything underhand. He was '\n",
      "  'a God-fearing and excellent man and was respected and loved by all his '\n",
      "  'parishioners. Mrs. Mansfield was respected and not loved, but it was '\n",
      "  'impossible to see much of the Rev. John without feeling his sympathy, and '\n",
      "  'acknowledging that burning love for all human souls which filled his '\n",
      "  'breast.',\n",
      "  ['happened']),\n",
      " ('\"Remember her?\" said the Irishman, \"remember the \\'light of the morning\\'? '\n",
      "  'She was all that and more. But they are in a poor way now at Desmondstown, '\n",
      "  'although they manage to keep together. The gentlemen are all for the '\n",
      "  \"huntin' and so for that matter are the young ladies, too. Young, I call \"\n",
      "  'them, and will, while I live. Why ever should age be added to their '\n",
      "  'burdens? And so this little missie is own grandchild to The Desmond?\"',\n",
      "  ['for']),\n",
      " ('\"Listen to me, Margot,\" said her grandmother. \"Your aunts, Eileen, Norah, '\n",
      "  'and Bridget, are young maids in their first dawn, and so for that matter '\n",
      "  'are Fergus and Bruce and Malachi also young as young can be.\"',\n",
      "  ['for']),\n",
      " ('Whether it was her great fatigue or the fact that she was sleeping at last '\n",
      "  'in the home of her ancestors, or the other fact that there was at least '\n",
      "  '_one_ dear old man living at Desmondstown, little Margot St. Juste slept '\n",
      "  'like a top during the whole of that first night in the house where her '\n",
      "  'mother had been born. She slept so soundly that she was quite unconscious '\n",
      "  'of the fact that The Desmond, accompanied by Madam, entered the hastily '\n",
      "  'improvised bedroom at the dawn of day and bent over the child. There was a '\n",
      "  'look of positive rapture on both their old faces.',\n",
      "  ['soundly']),\n",
      " ('It so happened that Hannah had her way. She did manage Mrs. Mansfield and, '\n",
      "  'what was more, she got everything in order for her master and Fergus '\n",
      "  'Desmond to start for their expedition to Arles, not that night but on the '\n",
      "  'following morning. How neither of these good gentlemen knew a word of the '\n",
      "  'French tongue, but they did discover by the aid of atlases, etc., the '\n",
      "  'direction in which Arles was situated and they started off on their quest '\n",
      "  \"for little Margot's French relations at an early hour the next day.\",\n",
      "  ['happened']),\n",
      " ('\"Oh, la, la! she is beautiful,\" exclaimed the Comtesse, when his '\n",
      "  'description had come to an end. \"Monsieur Englishman you are good. On that '\n",
      "  'point rest assured. You have the distinction of bearing. I note it. I would '\n",
      "  'that you could talk with our parish priest. You live among the high and '\n",
      "  \"holy things, M'sieur. Now, then, I have a little secret to impart, I would \"\n",
      "  'not tell it to another, but to you, yes, you have the air--the eye so clear '\n",
      "  'and frank. Now, Monsieur, when I married the Comte, he was great with the '\n",
      "  'notion that I, his little Ninon, had given up all the chapeaux and the '\n",
      "  'robes that brought in the money--the francs so numerous that I could make '\n",
      "  'the old place look like it did so long ago, but I did _not_ give up my '\n",
      "  \"_établissement_, m'sieur. Mon Dieu! I could not--I could not live without \"\n",
      "  'my gifts--I could not live without my silks and my satins, my lace, all '\n",
      "  'real, I assure you; my opera cloaks, my tortoise-shell ostrich feather '\n",
      "  'fans. No, no, I keep my _magasin_ going, so that I can give a good _dot_ to '\n",
      "  'the little Comtesse, and the old man he knows nothing about it. He must '\n",
      "  'never--never know--must my adorable Alphonse.\"',\n",
      "  ['numerous']),\n",
      " ('Now it so happened that always in the morning le Comte St. Juste took what '\n",
      "  'he called his airing. He went out leaning on the arm of his _garçon_, a '\n",
      "  'young man dressed in the ancient livery of the St. Justes. He leant heavily '\n",
      "  \"on the _garçon's_ arm and went invariably in one direction, and that was \"\n",
      "  'first to examine the thriving rows of beehives and second the peaches, '\n",
      "  'which were ripening to a lovely golden red on the high brick wall. The '\n",
      "  'Comte St. Juste used to count the peaches and rejoice in their fragrance. '\n",
      "  'He was a happy old man--very happy since he had married his Ninon. It '\n",
      "  'mattered little to him if she had once kept a shop. She kept one no longer. '\n",
      "  'He could not have married her if that was the case. They lived oh, so '\n",
      "  'happily on the rich _dot_ which she had brought with her. She was one in '\n",
      "  'ten thousand, his pretty Ninon, so young, so gay, and of the taste the most '\n",
      "  'perfect.',\n",
      "  ['happened']),\n",
      " ('It therefore so happened that when the three ladies drove up in their '\n",
      "  'automobile to the Château St. Juste, they only found Madame la Comtesse '\n",
      "  'standing on the front steps and giving directions to one of her numerous '\n",
      "  'gardeners.',\n",
      "  ['happened']),\n",
      " ('\"Then why didst thou cry and get so frightened that day, _ma belle_ '\n",
      "  'grand\\'mère?\" cried little Margot.',\n",
      "  ['frightened']),\n",
      " ('The next morning Margot went as usual to the _établissement_, but before '\n",
      "  'she began her accustomed work, Madame Marcelle called her into her private '\n",
      "  'room and there she told her that she was working for herself, not for '\n",
      "  'Madame la Comtesse, and that she found _la petite_ Comtesse so useful that '\n",
      "  'she was going to pay her two hundred francs a month for every month that '\n",
      "  'she was with her, and that it had been further arranged that the little '\n",
      "  'Comtesse before she left France for Ireland was to receive five hundred '\n",
      "  'francs besides, having her _dot_ put carefully away for her in addition.',\n",
      "  ['useful']),\n",
      " ('The supper was so good and the old-young people were so merry that Tilda '\n",
      "  'forgot her fears. She longed inexpressibly for Margot and for the refined '\n",
      "  'life of the French school at Arles; but nevertheless there were never any '\n",
      "  'potatoes like these, and Malachi had such a twinkle in his eye, and '\n",
      "  'whenever she glanced at Bruce he winked back at her in the most comforting '\n",
      "  'way.',\n",
      "  ['merry']),\n",
      " ('How it so happened that while Matilda Raynes was planning out her revenge '\n",
      "  'with a certain amount of skill, little Margot had taken off her habit and '\n",
      "  \"was seated in her favourite place on her grandfather's knee. He told her a \"\n",
      "  'little about the troublesome girl, and Margot begged of him not to mind, '\n",
      "  'for it was only her way and she was soon going.',\n",
      "  ['happened']),\n",
      " ('He talked so angrily that people left him alone and the train that should '\n",
      "  'have taken him to Mallow went off without him. He might have lingered at '\n",
      "  'Waterford goodness knows how long, waiting for a man of the name of Desmond '\n",
      "  'and trying to talk to stone-deaf and dumb people, who only talked '\n",
      "  'gibberish, when a bright-eyed, sparkling-looking individual came suddenly '\n",
      "  'on the platform, stared at Joshua, said a few words to the people round and '\n",
      "  'presently came up and introduced himself.',\n",
      "  ['angrily']),\n",
      " ('But it so happened that the old M. le Comte, lying against his pillows of '\n",
      "  'down, thought a great deal about his granddaughter. Henri was indeed a boy '\n",
      "  'to be proud of, but after all he was nothing to _la petite_. He wasted '\n",
      "  \"_l'argent_; _la petite_ seemed never to spend anything. Was justice being \"\n",
      "  'done to this charming little creature by the father of Henri? He troubled '\n",
      "  'himself about this. He became anxious.',\n",
      "  ['happened']),\n",
      " ('As little Margot had come back to Desmondstown now to live, as it was to be '\n",
      "  'her home in the future, with the exception of the one month which she would '\n",
      "  \"spend with _la belle_ grand'mère, and as _mon_ grandpère was dead, her \"\n",
      "  'return was quiet and without that sense of rejoicing which stimulated it on '\n",
      "  'her last return. There were no bonfires; there were no excited, screaming '\n",
      "  'peasants; but Phinias Maloney was there with his little old cart, and the '\n",
      "  'baby had grown so big that his mother thought that she might bring him out '\n",
      "  'just for the bit colleen to kiss him. They drove quietly up to the rickety '\n",
      "  'old house.',\n",
      "  ['big']),\n",
      " ('\"Margot, you are the most awful pushkeen in the wide world,\" said Aunt '\n",
      "  'Norah. \"You have made himself feel so ashamed that he can\\'t look me in the '\n",
      "  'face.\"',\n",
      "  ['ashamed']),\n",
      " ('Pixel explained that sometimes people (and cats) can get carried away using '\n",
      "  'devices and forget about other things they enjoy doing. This is called '\n",
      "  'Problematic Social Media Use. It happens when someone uses social media so '\n",
      "  'often that it starts causing problems in their daily life, like missing out '\n",
      "  'on real-life friendships and activities.',\n",
      "  ['often']),\n",
      " ('As soon as they arrived, they went straight to the Dubai Mall - the biggest '\n",
      "  'mall globally! It was so huge that Ali felt dizzy looking around. \"Wow!\" '\n",
      "  'said Fatima. \"This is incredible! But how did they make such a massive '\n",
      "  'building?\"',\n",
      "  ['huge']),\n",
      " (\"Oh, well the p Oh yes well it's now seven months old that prescription so \"\n",
      "  \"that that'll now be invalid so ache in left arm\",\n",
      "  ['that']),\n",
      " (\"For most people, these feelings are harmless and don't lead to any \"\n",
      "  'problems. However, for those with pyromania, these feelings can become so '\n",
      "  'strong that they feel like they need to act on them. They might start small '\n",
      "  'fires, like lighting matches or setting off fireworks, but over time, their '\n",
      "  'actions can become more risky and destructive.',\n",
      "  ['strong']),\n",
      " (\"Are you so sure that I'm making a mistake?\", ['sure']),\n",
      " ('Despite this challenge, Bobby continued to focus on his passion – hockey. '\n",
      "  'People still admired him for his athletic abilities and sportsmanship. In '\n",
      "  'fact, fans loved him so much that they nicknamed him \"The Golden Jet\" due '\n",
      "  'to his golden hair and lightning-fast speed on the ice. Over time, Bobby '\n",
      "  'broke numerous records and won multiple awards, solidifying his place among '\n",
      "  \"hockey's greatest legends.\",\n",
      "  ['much']),\n",
      " ('It so happened that erm Tom tha who I was working with, see he was on Shift '\n",
      "  'er Shift F and er there was always a bit of rivalry between the various '\n",
      "  'shifts as er, as to erm, you know, who did the better job and all this sort '\n",
      "  'of thing',\n",
      "  ['happened']),\n",
      " ('Then the, the Assistant Secretary for Education was erm a very interesting '\n",
      "  'chap erm he was a Mr A O D and erm he was, he had erm an elementary school '\n",
      "  \"background, he hadn't a degree or any qualification but he'd been in so \"\n",
      "  \"long that er he'd worked his way up to erm,s erm to Assistant Secretary for \"\n",
      "  'Education and he was a jolly chap and erm and he produced a, a dictionary '\n",
      "  'of erm, of the Suffolk dialect which has became quite erm, quite a classic '\n",
      "  'work really',\n",
      "  ['long']),\n",
      " (\"Global warming may be bad news for future generations, but let's face it, \"\n",
      "  'most of us spend as little time worrying about it as we did about al Qaeda '\n",
      "  'before 9/11. Like the terrorists, though, the seemingly remote climate risk '\n",
      "  'may hit home sooner and harder than we ever imagined. In fact, the prospect '\n",
      "  \"has become so real that the Pentagon's strategic planners are grappling \"\n",
      "  'with it.',\n",
      "  ['real']),\n",
      " ('One day, while playing near the river, Ella suddenly heard a loud rumble. '\n",
      "  'She looked up and saw dark clouds gathering in the sky. Thunder roared and '\n",
      "  'lightning flashed! Poor Ella got so scared that she forgot how to get back '\n",
      "  'home.',\n",
      "  ['scared']),\n",
      " ('The 6C and 5C BC marked the zenith of Greek civilisation in Italy, '\n",
      "  'corresponding to the period of Pericles in Athens. Greek seaborne trade was '\n",
      "  'so successful that Syracuse soon rivalled Athens. Syracuse and Taranto were '\n",
      "  'the two main centres of this refined civilisation. Philosophers, scientists '\n",
      "  'and writers settled in Sicily. Aeschylus lived at Gela. Theocritus defined '\n",
      "  'the rules of bucolic poetry and Archimedes was murdered by a Roman soldier '\n",
      "  'in Syracuse.',\n",
      "  ['successful']),\n",
      " ('Timmy then wondered, \"How do water animals get their food if they can\\'t '\n",
      "  'move around like land animals?\" So his mom took him down to the river to '\n",
      "  'show him. She pointed out the small insects and plants floating on top of '\n",
      "  'the water. \"These are called plankton,\" she said. \"They are so tiny that '\n",
      "  'even the smallest fish can eat them. Big fish eat smaller fish, and '\n",
      "  'eventually, everything gets eaten by something else!\"',\n",
      "  ['tiny']),\n",
      " (\"Oh, he's so upset that he sent his wingman over to soften me up?\",\n",
      "  ['upset']),\n",
      " ('Welcome to an exciting journey through one of the most famous cities in the '\n",
      "  'world - New York City! This bustling city is often referred to as \"The Big '\n",
      "  'Apple,\" and it\\'s known for being a hub of art, culture, history, and '\n",
      "  \"entertainment. It's so popular that people from all over the globe come to \"\n",
      "  'visit, making it a true melting pot of different cultures and experiences. '\n",
      "  'Imagine trying to explore more than 8 million stories packed into just five '\n",
      "  'boroughs – Manhattan, Brooklyn, Queens, The Bronx, and Staten Island. That '\n",
      "  \"sounds pretty incredible, right? Let's dive into some amazing places and \"\n",
      "  'activities NYC has to offer!',\n",
      "  ['popular']),\n",
      " (\"It just so happens that I've left my abode - my wife and I have separated.\",\n",
      "  ['happens']),\n",
      " ('What could you possibly need so bad that you had to steal it?', ['bad']),\n",
      " ('The boat that was sailing in the above picture is doing so simply because '\n",
      "  'of archimedes principle. That means that the volume of the boat has to be '\n",
      "  'so large that it can displace water with the same mass as the mass of the '\n",
      "  \"ship. This is also how the boats in the America's cup sail.\",\n",
      "  ['large']),\n",
      " ('Zeus was forced to negotiate with Hades about where Persephone would live. '\n",
      "  'It was decided that Persephone would stay with Hades in the underworld for '\n",
      "  'four months every year. During the other months, she would return to Earth '\n",
      "  'to be with her mother. Every time that Persephone left her mother to live '\n",
      "  'in the underworld, Demeter grieved. She withdrew her blessing of a good '\n",
      "  'harvest on the Earth. Thus, the four months of separation caused cold, '\n",
      "  'barren winters. When Persephone was returned to her mother, Demeter would '\n",
      "  'be so glad that she would be kind to the Earth again.',\n",
      "  ['glad']),\n",
      " ('Earth is 93 million miles from our sun, putting us solidly in the \"just '\n",
      "  'right\" zone for life – not too hot, not too cold. Kepler-78b is barely 1 '\n",
      "  \"million miles from its sun. That's about as close as Comet ISON will get to \"\n",
      "  'the sun next month – so close that scientists are still placing bets as to '\n",
      "  \"whether the comet will survive or be ripped apart by the sun's \"\n",
      "  'gravitational pull.',\n",
      "  ['close']),\n",
      " ('Because it made you so mad that you had to take notice of me.', ['mad']),\n",
      " ('Imagine being a part of a significant movement that changes the world '\n",
      "  \"forever. That's what happened when people began to follow Jesus of Nazareth \"\n",
      "  \"over two thousand years ago. Many believe that because of Jesus' teachings \"\n",
      "  'and actions, there was a shift in how people viewed themselves and their '\n",
      "  'connection with others and the world around them. This change has been so '\n",
      "  'powerful that its impact continues to resonate even today.',\n",
      "  ['powerful']),\n",
      " ('Her teacher explained that black holes were places where gravity - which '\n",
      "  'keeps us attached to the ground - became so strong that nothing can escape, '\n",
      "  'not even light. They often form when massive stars collapse under their own '\n",
      "  'weight after they die. To make sure five-year-olds understood, she compared '\n",
      "  'it to trying to hold too many balls; eventually, you just drop them because '\n",
      "  \"your hands cannot handle the heaviness anymore. That's kind of similar to \"\n",
      "  'what happens with dying stars!',\n",
      "  ['strong']),\n",
      " ('The Bohr family line was Jewish and had lived in Denmark for many years. '\n",
      "  'Bohr’s work was so important that he won the Noble Prize in 1922 and his '\n",
      "  'son aage Neils Bohr also won the Noble Prize in 1975 for his contributions '\n",
      "  'to science through the use of physics.',\n",
      "  ['important']),\n",
      " (\"And doing it's not so much that you're doing everything wrong, it's just \"\n",
      "  'the one thing which being done right is that the other way of putting it '\n",
      "  \"that, the guy because he's so well organized what effect does that have on \"\n",
      "  'everybody else then?',\n",
      "  ['much']),\n",
      " (\"I took so many that it's annoying.\", ['many']),\n",
      " ('In some extreme cases, people can’t stop dieting and get so thin and so '\n",
      "  'sick that it can even lead to death. Why, then, would anyone do it? Well, '\n",
      "  'there isn’t just one answer — there are many.',\n",
      "  ['sick']),\n",
      " ('And there were a flurry of responses – so many that she wrote this great '\n",
      "  'post entitled “101 Ways to Use a Long Video in the Classroom.”',\n",
      "  ['many']),\n",
      " (\"said ooh, she's ever so excited she's ever so excited that she's back \"\n",
      "  'round, you know, that way as well.',\n",
      "  ['excited']),\n",
      " ('\"Has a history of drinking.\" And was, in fact, so intoxicated that night, '\n",
      "  'that his own lawyers had to place him into rehab.',\n",
      "  ['intoxicated']),\n",
      " ('Brandt discovered the remains of flamingos and other animals with chalky '\n",
      "  'sodium carbonate deposits outlining their bodies in sharp relief. \"I '\n",
      "  'unexpectedly found the creatures — all manner of birds and bats — washed up '\n",
      "  'along the shoreline of Lake Natron,\" Brandt wrote in his book. \"No one '\n",
      "  'knows for certain exactly how they die, but … the water has an extremely '\n",
      "  'high soda and salt content, so high that it would strip the ink off my '\n",
      "  'Kodak film boxes within a few seconds.\"',\n",
      "  ['high']),\n",
      " ('Millions of people are affected by an eye condition so severe that it '\n",
      "  'seriously limits their lives but with modern contact lens technology most '\n",
      "  'can now be helped.',\n",
      "  ['severe']),\n",
      " ('I used to be a die-hard milk lover. As a kid, I would chug glasses of warm '\n",
      "  \"milk before bedtime, hoping for sweet dreams. During breakfast, I'd pour \"\n",
      "  'myself another glass alongside my favorite cereals. My love for milk was so '\n",
      "  'deep that when I stumbled upon a Reddit thread questioning whether milk '\n",
      "  'could really \"wake you up,\" I felt compelled to join the conversation.',\n",
      "  ['deep']),\n",
      " ('First off, you might wonder - what exactly is an award? Well, an award is a '\n",
      "  'special recognition given to someone who has achieved something remarkable '\n",
      "  'in their field. Think of it like getting a trophy for being the best soccer '\n",
      "  'player in your league or earning a gold star for having the neatest '\n",
      "  'handwriting in class. Now imagine doing something so impressive that people '\n",
      "  'across the country (or even the entire world!) recognize your talent and '\n",
      "  \"celebrate your accomplishments. That's essentially what happens during \"\n",
      "  'awards season in the performing arts!',\n",
      "  ['impressive']),\n",
      " ('When it comes to art and architecture, the Ancient Greeks left behind an '\n",
      "  'impressive and enduring legacy. Their sculptures depicted real human bodies '\n",
      "  'so accurately that we can still appreciate them today. Moreover, their '\n",
      "  'architectural designs, like columns and pediments, remain influential in '\n",
      "  \"constructing modern buildings. Visit any major museum worldwide, and you're \"\n",
      "  \"likely to see statues inspired by the Ancient Greeks' masterpieces. Even \"\n",
      "  'famous structures like the White House in Washington D.C., have elements '\n",
      "  'borrowed from Ancient Greek temples. How cool is that?',\n",
      "  ['accurately']),\n",
      " ('It might be that the subject noun and verb phrase go together to form a '\n",
      "  'constituent, so you have Florence teased Dougal or the structure might not '\n",
      "  'be that it might just be three separate constituents with no firm structure '\n",
      "  'forming a further constituent, so why that structure?',\n",
      "  ['why']),\n",
      " (\"We suppose that's a rule in our minds and that explains our judgments about \"\n",
      "  \"for example and proform substitution and so on that's so we define the \"\n",
      "  'notion of a constituent plus label it with brackets, and then we make '\n",
      "  \"certain predictions about operations that can or can't be performed on.\",\n",
      "  ['on']),\n",
      " ('By the fall of 1911, the school had grown so much that they needed even '\n",
      "  'more space. They took over the entire top floor of the old Town Hall and '\n",
      "  'now had four dedicated teachers to help guide these eager students. As the '\n",
      "  'community continued to grow, the school realized they would need their very '\n",
      "  'own building. And where did they find land for such a project? Why, on a '\n",
      "  'beautiful piece of farmland, of course!',\n",
      "  ['much']),\n",
      " ('have been so great that he would have returned to take up the work later '\n",
      "  'on. In the meantime, he would not have had the social experience needed for '\n",
      "  'his character building. If the teacher constantly defended the child, he '\n",
      "  'would never be able to defend himself. It is therefore important for the '\n",
      "  'teacher to observe all that happens in this small world, where individual '\n",
      "  'strengths are more or less equal.”',\n",
      "  ['great']),\n",
      " ('I would hope however that the committee members at North Yorkshire will see '\n",
      "  'that the local feeling is so deep that the project itself cannot be harmed '\n",
      "  'by a public inquiry and they will make er they will take the decisions '\n",
      "  'which will put this application in that direction and send that message '\n",
      "  'very clearly to the Secretary of State.',\n",
      "  ['deep']),\n",
      " ('The argument goes like this: When the climate changed naturally in the '\n",
      "  'past, and the planet emerged from an ice age, large ice sheets covering '\n",
      "  'much of the planet retreated. They were so heavy that the resulting release '\n",
      "  \"of pressure on the earth's crust caused it to 'bounce back', triggering \"\n",
      "  'earthquakes, tremors, and even volcanic activity along pre-existing fault '\n",
      "  'lines.',\n",
      "  ['heavy']),\n",
      " ('Have you ever imagined being able to change your size like characters in '\n",
      "  'video games? Maybe becoming tiny enough to explore the world of insects or '\n",
      "  'growing so big that you can reach the clouds? While this might sound like '\n",
      "  'something out of a fantasy story, scientists are always working on '\n",
      "  'understanding different states of matter and how they can be changed. This '\n",
      "  'chapter will introduce you to some basic concepts about matter and its '\n",
      "  'transformations!',\n",
      "  ['big']),\n",
      " ('I still do, so much that if he were in danger...', ['much']),\n",
      " ('- Propulsion– is the manner in which food is moved through the digest '\n",
      "  'tract. This includes swallowing and peristalsis. Peristalsis is the main '\n",
      "  'mean of propulsion and involves contraction and relaxation of muscles '\n",
      "  'surrounding the organs. It’s main purpose is to squeeze food through the GI '\n",
      "  'tract (a small amount of mixing occurs also). Peristalsis is so strong that '\n",
      "  'you would continue to digest food even if you were upside down.',\n",
      "  ['strong']),\n",
      " ('One day, they found themselves in a disagreement and had a big battle. Even '\n",
      "  'though they were friends, sometimes adults can get upset and need some time '\n",
      "  'apart. In this story, they fought so much that it looked like they tore '\n",
      "  \"each other apart! But don't worry, kids; even though it seemed scary, they \"\n",
      "  \"weren't really hurt because they are comic book characters and can heal \"\n",
      "  'quickly.',\n",
      "  ['much']),\n",
      " ('The study of the use of metaphor in literature has a much longer tradition '\n",
      "  'than the cognitive linguistic approach to this subject and therefore it '\n",
      "  'ought not to be surprising that literary scholars have developed their own '\n",
      "  'terminology to describe various phenomena, including metaphors. However, '\n",
      "  'the influence of cognitive linguistics on other fields of inquiry has been '\n",
      "  'so immense that cognitive linguistic terminology has became more and more '\n",
      "  'frequently used. Thus, nowadays both terminologies are acknowledged as '\n",
      "  'equally valid, but that may well cause some confusion as sometimes there '\n",
      "  'are now several different terms to refer to one thing.',\n",
      "  ['immense'])]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:57.902780Z",
     "start_time": "2025-05-15T00:29:57.900745Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9d1c83322ea087b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:58.364309Z",
     "start_time": "2025-05-15T00:29:58.362572Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "54a7512b17645c44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:58.985392Z",
     "start_time": "2025-05-15T00:29:58.966805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(ds.filter(lambda x: \"much less\" in x[\"text\"])))\n",
    "print(len(ds.filter(lambda x: \"let alone\" in x[\"text\"])))\n",
    "\n"
   ],
   "id": "2bc0010aadadce41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765\n",
      "439\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:29:59.779537Z",
     "start_time": "2025-05-15T00:29:59.776708Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3a0d3989a7ed5d05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:01.262059Z",
     "start_time": "2025-05-15T00:30:01.247193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 1\n",
    "x = ds.filter(lambda x: \"much less\" in x[\"text\"])[i]['text'].split(\"\\n\")\n",
    "print(len(x))\n",
    "z = [y for y in x if 'much less' in y.lower()]\n",
    "\n"
   ],
   "id": "f91717fb2107a855",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:01.791483Z",
     "start_time": "2025-05-15T00:30:01.787736Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9329dd17d94df839",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:02.304514Z",
     "start_time": "2025-05-15T00:30:02.290382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def extract_sub_part(sent: str, tgt='much less'):\n",
    "    much_less_pos = sent.find(tgt)\n",
    "    if much_less_pos == -1:\n",
    "        raise Exception(\"tgt str not found\")\n",
    "\n",
    "    prev_period = sent.rfind(\".\", 0, much_less_pos)\n",
    "    if prev_period == -1:\n",
    "        prev_period = 0\n",
    "    else:\n",
    "        prev_period+=1\n",
    "    next_period = sent.find(\".\", much_less_pos)\n",
    "    if next_period == -1:\n",
    "        next_period = len(sent)\n",
    "    else:\n",
    "        next_period += 1\n",
    "\n",
    "    extracted = sent[prev_period: next_period]\n",
    "    extracted = extracted.strip()\n",
    "    return extracted\n",
    "\n",
    "extract_sub_part(z[0])\n"
   ],
   "id": "96929b0fd5909586",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The drug's side effects proved to be both predictable and manageable and toxicity levels were much less than those associated with other anti-cancer agents, the study found.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:06.105447Z",
     "start_time": "2025-05-15T00:30:03.528930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from affinity.corr_matrix_new import get_logits_for_masked_sent, compute_surprisal_for_logits\n",
    "from affinity.tokenization import Sentence\n",
    "from lib.scoring_fns import probability\n",
    "from typing import List\n",
    "def check_prob_on_words(sent: str, words: List[str]):\n",
    "    s = Sentence(sent)  # Sentence class takes care of tokenization\n",
    "\n",
    "    # prepare sentence with mask at correct location\n",
    "    # punctuation is preserved, but will not be part of the mask\n",
    "    # this will error if your word is not actually present or if your word is multi-tokenized\n",
    "    # masked_sent keeps track of what was masked\n",
    "    res = []\n",
    "    for w in words:\n",
    "        masked_sent = s.get_input_with_word_masked(w)\n",
    "\n",
    "        # obtain the logits at the masked position\n",
    "        logits = get_logits_for_masked_sent(masked_sent, do_print=False)\n",
    "\n",
    "        # compute probability of the original word\n",
    "        surp = compute_surprisal_for_logits(\n",
    "            masked_sent,\n",
    "            logits,\n",
    "            probability\n",
    "            # surprisal     # can also use surprisal\n",
    "        )\n",
    "        res.append(surp)\n",
    "    return res\n"
   ],
   "id": "50b4557f1acbeae7",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:07.313632Z",
     "start_time": "2025-05-15T00:30:07.289609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "extract_sub_part(\"mention of the word 'mathematics'. Mention numbers, let alone\", 'let alone')"
   ],
   "id": "3231d80457662e5c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mention numbers, let alone'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:23.758802Z",
     "start_time": "2025-05-15T00:30:10.381762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def check_target(tgt_str: str):\n",
    "    examples_with_much_less = ds.filter(lambda x: tgt_str in x[\"text\"])\n",
    "    # print(len(examples_with_much_less))\n",
    "    # print(examples_with_much_less[0])\n",
    "    valid = []\n",
    "    for i, ex in enumerate(examples_with_much_less):\n",
    "        if i > 200: break\n",
    "        # print(i)\n",
    "        # print(ex)\n",
    "        ex_lines = ex[\"text\"].split(\"\\n\")\n",
    "        lines_with_tgt_str = [l for l in ex_lines if tgt_str in l]\n",
    "        if len(lines_with_tgt_str) == 0:\n",
    "            print('not found')\n",
    "            print(ex_lines)\n",
    "        for l in lines_with_tgt_str:\n",
    "            sent = extract_sub_part(l, tgt_str)\n",
    "            if sent == \"\":\n",
    "                print(f\"parsing\\n\\t{l}\\n\\t got empty\")\n",
    "            try:\n",
    "                probs = check_prob_on_words(sent, tgt_str.split(\" \"))\n",
    "            except Exception as e:\n",
    "                print(\" exception:\", e)\n",
    "                # pp(lines_with_tgt_str)\n",
    "                print(l)\n",
    "                print('------')\n",
    "                continue\n",
    "            if all([p > 0.9 for p in probs]):\n",
    "                valid.append((sent, probs))\n",
    "    return valid\n",
    "\n",
    "v = check_target('let alone')\n",
    "print(len(v))\n"
   ],
   "id": "f1b14d2670328fb2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Get_singleton called but no model is initialized; will call init\n",
      "WARNING:root:Initializing roberta-large\n",
      "WARNING:root:INIT\n",
      "WARNING:root:Will use cache for MLMScorer. Use this for analysis.For computation, avoid this; memory will blow up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exception: multiple instances of let found in But Neandertals have many distinctive features, and there is no reason why these diseases (or any others) would cause many, let alone all, of these features on even one, let alone many, individuals.; indicate occ\n",
      "Amazingly, a century after scientists knew otherwise, most creationists still believe that Neandertals were merely modern humans, deformed by diseases such as rickets, arthritis or syphilis. Some, but by no means all, Neandertals have been found with signs of health problems such as arthritis. But Neandertals have many distinctive features, and there is no reason why these diseases (or any others) would cause many, let alone all, of these features on even one, let alone many, individuals. Modern knowledge and experience also contradicts the idea that disease is a cause of Neandertal features, because these diseases do not cause modern humans to look like Neandertals.\n",
      "------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m                 valid\u001B[38;5;241m.\u001B[39mappend((sent, probs))\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m valid\n\u001B[0;32m---> 31\u001B[0m v \u001B[38;5;241m=\u001B[39m check_target(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlet alone\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(v))\n",
      "Cell \u001B[0;32mIn[30], line 20\u001B[0m, in \u001B[0;36mcheck_target\u001B[0;34m(tgt_str)\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparsing\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00ml\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m got empty\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 20\u001B[0m     probs \u001B[38;5;241m=\u001B[39m check_prob_on_words(sent, tgt_str\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m exception:\u001B[39m\u001B[38;5;124m\"\u001B[39m, e)\n",
      "Cell \u001B[0;32mIn[28], line 17\u001B[0m, in \u001B[0;36mcheck_prob_on_words\u001B[0;34m(sent, words)\u001B[0m\n\u001B[1;32m     14\u001B[0m masked_sent \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mget_input_with_word_masked(w)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# obtain the logits at the masked position\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m logits \u001B[38;5;241m=\u001B[39m get_logits_for_masked_sent(masked_sent, do_print\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# compute probability of the original word\u001B[39;00m\n\u001B[1;32m     20\u001B[0m surp \u001B[38;5;241m=\u001B[39m compute_surprisal_for_logits(\n\u001B[1;32m     21\u001B[0m     masked_sent,\n\u001B[1;32m     22\u001B[0m     logits,\n\u001B[1;32m     23\u001B[0m     probability\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# surprisal     # can also use surprisal\u001B[39;00m\n\u001B[1;32m     25\u001B[0m )\n",
      "File \u001B[0;32m~/docs_local/research/proj_code/rozner-mono-cxs-main/proj/cxs_are_revealed/src/affinity/corr_matrix_new.py:68\u001B[0m, in \u001B[0;36mget_logits_for_masked_sent\u001B[0;34m(masked_sent, do_print)\u001B[0m\n\u001B[1;32m     66\u001B[0m p(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetting logits_for_masked_sent\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(masked_sent\u001B[38;5;241m.\u001B[39mmasked_token_indices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 68\u001B[0m outputs \u001B[38;5;241m=\u001B[39m mlm\u001B[38;5;241m.\u001B[39mget_model_outputs_for_input(masked_sent\u001B[38;5;241m.\u001B[39minput_ids)\n\u001B[1;32m     69\u001B[0m logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m     71\u001B[0m p(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput Ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, masked_sent\u001B[38;5;241m.\u001B[39minput_ids)\n",
      "File \u001B[0;32m~/docs_local/research/proj_code/rozner-mono-cxs-main/proj/cxs_are_revealed/src/lib/common/mlm_scorer_base.py:119\u001B[0m, in \u001B[0;36mMLMScorerBase.get_model_outputs_for_input\u001B[0;34m(self, input_ids)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03mActually run the model forward.\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_cache:\n\u001B[0;32m--> 119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_model_outputs_for_input_cached(\u001B[38;5;28mself\u001B[39m, input_ids)\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_model_outputs_for_input(\u001B[38;5;28mself\u001B[39m, input_ids)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/memoization/caching/lru_cache.py:62\u001B[0m, in \u001B[0;36mget_caching_wrapper.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m             cache_expired \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     61\u001B[0m     misses \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 62\u001B[0m result \u001B[38;5;241m=\u001B[39m user_function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m lock:\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m cache:\n",
      "File \u001B[0;32m~/docs_local/research/proj_code/rozner-mono-cxs-main/proj/cxs_are_revealed/src/lib/common/mlm_scorer_base.py:111\u001B[0m, in \u001B[0;36mMLMScorerBase._get_model_outputs_for_input_cached\u001B[0;34m(mlm, input_ids)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;129m@cached\u001B[39m(custom_key_maker\u001B[38;5;241m=\u001B[39m_tensor_hasher, ttl\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m300\u001B[39m, max_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3000\u001B[39m)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_model_outputs_for_input_cached\u001B[39m(mlm: MLMScorerBase, input_ids: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m MaskedLMOutput:\n\u001B[1;32m    108\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;124;03m    Actually run the model forward.\u001B[39;00m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mlm\u001B[38;5;241m.\u001B[39m__get_model_outputs_for_input(mlm, input_ids)\n",
      "File \u001B[0;32m~/docs_local/research/proj_code/rozner-mono-cxs-main/proj/cxs_are_revealed/src/lib/common/mlm_scorer_base.py:100\u001B[0m, in \u001B[0;36mMLMScorerBase.__get_model_outputs_for_input\u001B[0;34m(mlm, input_ids)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     99\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39mto(mlm\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 100\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m mlm\u001B[38;5;241m.\u001B[39mmodel(inputs)\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1206\u001B[0m, in \u001B[0;36mRobertaForMaskedLM.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1196\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1197\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m \u001B[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001B[39;00m\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1204\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1206\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroberta(\n\u001B[1;32m   1207\u001B[0m     input_ids,\n\u001B[1;32m   1208\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   1209\u001B[0m     token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids,\n\u001B[1;32m   1210\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   1211\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m   1212\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m   1213\u001B[0m     encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[1;32m   1214\u001B[0m     encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[1;32m   1215\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m   1216\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m   1217\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1218\u001B[0m )\n\u001B[1;32m   1219\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1220\u001B[0m prediction_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(sequence_output)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:976\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m    970\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m    971\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m    974\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m--> 976\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[1;32m    977\u001B[0m     embedding_output,\n\u001B[1;32m    978\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m    979\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m    980\u001B[0m     encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[1;32m    981\u001B[0m     encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_extended_attention_mask,\n\u001B[1;32m    982\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m    983\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    984\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    985\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m    986\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m    987\u001B[0m )\n\u001B[1;32m    988\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    989\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    620\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    621\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    622\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    628\u001B[0m         output_attentions,\n\u001B[1;32m    629\u001B[0m     )\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 631\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m layer_module(\n\u001B[1;32m    632\u001B[0m         hidden_states,\n\u001B[1;32m    633\u001B[0m         attention_mask,\n\u001B[1;32m    634\u001B[0m         layer_head_mask,\n\u001B[1;32m    635\u001B[0m         encoder_hidden_states,\n\u001B[1;32m    636\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    637\u001B[0m         past_key_value,\n\u001B[1;32m    638\u001B[0m         output_attentions,\n\u001B[1;32m    639\u001B[0m     )\n\u001B[1;32m    641\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    642\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:520\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    509\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    510\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    517\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    518\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    519\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 520\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(\n\u001B[1;32m    521\u001B[0m         hidden_states,\n\u001B[1;32m    522\u001B[0m         attention_mask,\n\u001B[1;32m    523\u001B[0m         head_mask,\n\u001B[1;32m    524\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    525\u001B[0m         past_key_value\u001B[38;5;241m=\u001B[39mself_attn_past_key_value,\n\u001B[1;32m    526\u001B[0m     )\n\u001B[1;32m    527\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:447\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    438\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    439\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    445\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    446\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 447\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself(\n\u001B[1;32m    448\u001B[0m         hidden_states,\n\u001B[1;32m    449\u001B[0m         attention_mask,\n\u001B[1;32m    450\u001B[0m         head_mask,\n\u001B[1;32m    451\u001B[0m         encoder_hidden_states,\n\u001B[1;32m    452\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    453\u001B[0m         past_key_value,\n\u001B[1;32m    454\u001B[0m         output_attentions,\n\u001B[1;32m    455\u001B[0m     )\n\u001B[1;32m    456\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    457\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/2024_coelm/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:370\u001B[0m, in \u001B[0;36mRobertaSdpaSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001B[39;00m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001B[39;00m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;66;03m# a causal mask in case tgt_len == 1.\u001B[39;00m\n\u001B[1;32m    366\u001B[0m is_causal \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_decoder \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_cross_attention \u001B[38;5;129;01mand\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m tgt_len \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    368\u001B[0m )\n\u001B[0;32m--> 370\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mscaled_dot_product_attention(\n\u001B[1;32m    371\u001B[0m     query_layer,\n\u001B[1;32m    372\u001B[0m     key_layer,\n\u001B[1;32m    373\u001B[0m     value_layer,\n\u001B[1;32m    374\u001B[0m     attn_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    375\u001B[0m     dropout_p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout_prob \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0.0\u001B[39m,\n\u001B[1;32m    376\u001B[0m     is_causal\u001B[38;5;241m=\u001B[39mis_causal,\n\u001B[1;32m    377\u001B[0m )\n\u001B[1;32m    379\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    380\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mreshape(bsz, tgt_len, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_head_size)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:29.179295Z",
     "start_time": "2025-05-15T00:30:27.128501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_keys = set()\n",
    "for x in ds:\n",
    "    all_keys.update(list(x.keys()))\n",
    "print(all_keys)"
   ],
   "id": "5bf1d6b4072ba0a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text'}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:39.730804Z",
     "start_time": "2025-05-15T00:30:38.807438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_baby = load_dataset(\"ltg/babylm-2024-baby-cosmo-fine-10m\", split=\"train\")\n",
    "print(len(ds_baby.filter(lambda x: \"much less\" in x[\"text\"])))\n",
    "print(len(ds_baby.filter(lambda x: \"let alone\" in x[\"text\"])))\n",
    "\n"
   ],
   "id": "9dd16f8a9bbf2f62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "44\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:30:39.754026Z",
     "start_time": "2025-05-15T00:30:39.752021Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b908560ecdbda5a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# npn analysis\n",
   "id": "a2bef498ea7f8782"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T00:32:40.660250Z",
     "start_time": "2025-05-15T00:32:39.296560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rozlib.libs.common.data.utils_jsonl import read_from_jsonl\n",
    "from proj.cxs_are_revealed.paper.proj_common.npn_dataset_generation.npn_utils import GPTOutput\n",
    "from proj.cxs_are_revealed.paper.data_config import BabyLMExp6NPN\n",
    "\n",
    "\n",
    "# note that this will be automagically cached!\n",
    "ds = load_dataset(\"ltg/babylm-2024-baby-cosmo-fine-100m\", split=\"train\")\n",
    "def check_npns():\n",
    "    gpt_outputs: List[GPTOutput] = read_from_jsonl(BabyLMExp6NPN.npn_gpt_outputs, GPTOutput)\n",
    "    for o in gpt_outputs:\n",
    "        npn = o.noun + \" \" + o.prep + \" \" + o.noun\n",
    "        ct = len(ds.filter(lambda x: npn in x[\"text\"].lower()))\n",
    "        print(npn, ct)\n",
    "\n",
    "check_npns()"
   ],
   "id": "2b1aa4c2138ad25b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short to short 1\n",
      "short after short 0\n",
      "short by short 0\n",
      "short upon short 0\n",
      "sign to sign 0\n",
      "sign after sign 0\n",
      "sign by sign 0\n",
      "sign upon sign 0\n",
      "tough to tough 0\n",
      "tough after tough 0\n",
      "tough by tough 0\n",
      "tough upon tough 0\n",
      "army to army 0\n",
      "army after army 1\n",
      "army by army 0\n",
      "army upon army 0\n",
      "marriage to marriage 0\n",
      "marriage after marriage 0\n",
      "marriage by marriage 0\n",
      "marriage upon marriage 0\n",
      "break to break 1\n",
      "break after break 0\n",
      "break by break 0\n",
      "break upon break 0\n",
      "person to person 217\n",
      "person after person 1\n",
      "person by person 3\n",
      "person upon person 0\n",
      "twist to twist 0\n",
      "twist after twist 0\n",
      "twist by twist 0\n",
      "twist upon twist 0\n",
      "above to above 0\n",
      "above after above 0\n",
      "above by above 0\n",
      "above upon above 0\n",
      "smell to smell 0\n",
      "smell after smell 0\n",
      "smell by smell 0\n",
      "smell upon smell 0\n",
      "delight to delight 0\n",
      "delight after delight 0\n",
      "delight by delight 0\n",
      "delight upon delight 0\n",
      "friendly to friendly 1\n",
      "friendly after friendly 0\n",
      "friendly by friendly 0\n",
      "friendly upon friendly 0\n",
      "green to green 1\n",
      "green after green 0\n",
      "green by green 0\n",
      "green upon green 0\n",
      "case to case 6\n",
      "case after case 3\n",
      "case by case 7\n",
      "case upon case 0\n",
      "habit to habit 0\n",
      "habit after habit 0\n",
      "habit by habit 0\n",
      "habit upon habit 0\n",
      "woman to woman 9\n",
      "woman after woman 0\n",
      "woman by woman 0\n",
      "woman upon woman 0\n",
      "master to master 0\n",
      "master after master 0\n",
      "master by master 0\n",
      "master upon master 0\n",
      "baby to baby 2\n",
      "baby after baby 0\n",
      "baby by baby 0\n",
      "baby upon baby 0\n",
      "face to face 245\n",
      "face after face 0\n",
      "face by face 0\n",
      "face upon face 0\n",
      "message to message 0\n",
      "message after message 1\n",
      "message by message 0\n",
      "message upon message 0\n",
      "spirit to spirit 0\n",
      "spirit after spirit 0\n",
      "spirit by spirit 0\n",
      "spirit upon spirit 0\n",
      "camp to camp 3\n",
      "camp after camp 0\n",
      "camp by camp 0\n",
      "camp upon camp 0\n",
      "special to special 0\n",
      "special after special 0\n",
      "special by special 0\n",
      "special upon special 0\n",
      "roll to roll 0\n",
      "roll after roll 0\n",
      "roll by roll 1\n",
      "roll upon roll 0\n",
      "mind to mind 3\n",
      "mind after mind 0\n",
      "mind by mind 0\n",
      "mind upon mind 0\n",
      "crazy to crazy 0\n",
      "crazy after crazy 0\n",
      "crazy by crazy 0\n",
      "crazy upon crazy 0\n",
      "super to super 0\n",
      "super after super 0\n",
      "super by super 0\n",
      "super upon super 0\n",
      "best to best 0\n",
      "best after best 0\n",
      "best by best 0\n",
      "best upon best 0\n",
      "mess to mess 0\n",
      "mess after mess 0\n",
      "mess by mess 0\n",
      "mess upon mess 0\n",
      "spark to spark 0\n",
      "spark after spark 0\n",
      "spark by spark 0\n",
      "spark upon spark 0\n",
      "shock to shock 0\n",
      "shock after shock 0\n",
      "shock by shock 0\n",
      "shock upon shock 0\n",
      "scale to scale 0\n",
      "scale after scale 0\n",
      "scale by scale 0\n",
      "scale upon scale 0\n",
      "silly to silly 0\n",
      "silly after silly 0\n",
      "silly by silly 0\n",
      "silly upon silly 0\n",
      "wound to wound 0\n",
      "wound after wound 0\n",
      "wound by wound 0\n",
      "wound upon wound 0\n",
      "dough to dough 0\n",
      "dough after dough 0\n",
      "dough by dough 0\n",
      "dough upon dough 0\n",
      "flash to flash 0\n",
      "flash after flash 0\n",
      "flash by flash 0\n",
      "flash upon flash 0\n",
      "pair to pair 0\n",
      "pair after pair 0\n",
      "pair by pair 1\n",
      "pair upon pair 0\n",
      "coal to coal 0\n",
      "coal after coal 0\n",
      "coal by coal 0\n",
      "coal upon coal 0\n",
      "seek to seek 0\n",
      "seek after seek 0\n",
      "seek by seek 0\n",
      "seek upon seek 0\n",
      "comfort to comfort 0\n",
      "comfort after comfort 0\n",
      "comfort by comfort 0\n",
      "comfort upon comfort 0\n",
      "farm to farm 3\n",
      "farm after farm 1\n",
      "farm by farm 0\n",
      "farm upon farm 0\n",
      "team to team 0\n",
      "team after team 0\n",
      "team by team 0\n",
      "team upon team 0\n",
      "assist to assist 0\n",
      "assist after assist 0\n",
      "assist by assist 0\n",
      "assist upon assist 0\n",
      "action to action 0\n",
      "action after action 0\n",
      "action by action 0\n",
      "action upon action 0\n",
      "prop to prop 0\n",
      "prop after prop 0\n",
      "prop by prop 0\n",
      "prop upon prop 0\n",
      "shoot to shoot 0\n",
      "shoot after shoot 0\n",
      "shoot by shoot 0\n",
      "shoot upon shoot 0\n",
      "eight to eight 9\n",
      "eight after eight 0\n",
      "eight by eight 1\n",
      "eight upon eight 0\n",
      "gift to gift 0\n",
      "gift after gift 0\n",
      "gift by gift 0\n",
      "gift upon gift 0\n",
      "nest to nest 0\n",
      "nest after nest 0\n",
      "nest by nest 0\n",
      "nest upon nest 0\n",
      "father to father 2\n",
      "father after father 1\n",
      "father by father 0\n",
      "father upon father 0\n",
      "fact to fact 0\n",
      "fact after fact 0\n",
      "fact by fact 0\n",
      "fact upon fact 1\n",
      "kitchen to kitchen 0\n",
      "kitchen after kitchen 0\n",
      "kitchen by kitchen 0\n",
      "kitchen upon kitchen 0\n",
      "pond to pond 2\n",
      "pond after pond 0\n",
      "pond by pond 0\n",
      "pond upon pond 0\n",
      "plate to plate 0\n",
      "plate after plate 7\n",
      "plate by plate 0\n",
      "plate upon plate 0\n",
      "lost to lost 0\n",
      "lost after lost 0\n",
      "lost by lost 0\n",
      "lost upon lost 0\n",
      "branch to branch 41\n",
      "branch after branch 0\n",
      "branch by branch 0\n",
      "branch upon branch 0\n",
      "collect to collect 0\n",
      "collect after collect 0\n",
      "collect by collect 0\n",
      "collect upon collect 0\n",
      "meet to meet 0\n",
      "meet after meet 0\n",
      "meet by meet 0\n",
      "meet upon meet 0\n",
      "chick to chick 0\n",
      "chick after chick 0\n",
      "chick by chick 0\n",
      "chick upon chick 0\n",
      "tribe to tribe 0\n",
      "tribe after tribe 1\n",
      "tribe by tribe 0\n",
      "tribe upon tribe 0\n",
      "milk to milk 1\n",
      "milk after milk 0\n",
      "milk by milk 0\n",
      "milk upon milk 0\n",
      "simple to simple 0\n",
      "simple after simple 0\n",
      "simple by simple 0\n",
      "simple upon simple 0\n",
      "trouble to trouble 0\n",
      "trouble after trouble 0\n",
      "trouble by trouble 0\n",
      "trouble upon trouble 0\n",
      "neck to neck 0\n",
      "neck after neck 0\n",
      "neck by neck 0\n",
      "neck upon neck 0\n",
      "eager to eager 0\n",
      "eager after eager 0\n",
      "eager by eager 0\n",
      "eager upon eager 0\n",
      "uncle to uncle 0\n",
      "uncle after uncle 0\n",
      "uncle by uncle 0\n",
      "uncle upon uncle 0\n",
      "president to president 3\n",
      "president after president 0\n",
      "president by president 0\n",
      "president upon president 0\n",
      "meat to meat 0\n",
      "meat after meat 0\n",
      "meat by meat 0\n",
      "meat upon meat 0\n",
      "effect to effect 0\n",
      "effect after effect 0\n",
      "effect by effect 0\n",
      "effect upon effect 0\n",
      "court to court 1\n",
      "court after court 0\n",
      "court by court 0\n",
      "court upon court 0\n",
      "avail to avail 0\n",
      "avail after avail 0\n",
      "avail by avail 0\n",
      "avail upon avail 0\n",
      "bottom to bottom 2\n",
      "bottom after bottom 0\n",
      "bottom by bottom 0\n",
      "bottom upon bottom 0\n",
      "favor to favor 0\n",
      "favor after favor 0\n",
      "favor by favor 0\n",
      "favor upon favor 0\n",
      "people to people 3\n",
      "people after people 0\n",
      "people by people 0\n",
      "people upon people 1\n",
      "million to million 0\n",
      "million after million 0\n",
      "million by million 0\n",
      "million upon million 0\n",
      "wear to wear 0\n",
      "wear after wear 0\n",
      "wear by wear 0\n",
      "wear upon wear 0\n",
      "desert to desert 0\n",
      "desert after desert 0\n",
      "desert by desert 0\n",
      "desert upon desert 0\n",
      "friend to friend 1\n",
      "friend after friend 1\n",
      "friend by friend 0\n",
      "friend upon friend 0\n",
      "county to county 2\n",
      "county after county 0\n",
      "county by county 1\n",
      "county upon county 0\n",
      "stick to stick 1\n",
      "stick after stick 0\n",
      "stick by stick 0\n",
      "stick upon stick 0\n",
      "fall to fall 0\n",
      "fall after fall 0\n",
      "fall by fall 0\n",
      "fall upon fall 0\n",
      "east to east 0\n",
      "east after east 0\n",
      "east by east 0\n",
      "east upon east 0\n",
      "energy to energy 0\n",
      "energy after energy 0\n",
      "energy by energy 0\n",
      "energy upon energy 0\n",
      "dinner to dinner 0\n",
      "dinner after dinner 0\n",
      "dinner by dinner 0\n",
      "dinner upon dinner 0\n",
      "business to business 0\n",
      "business after business 1\n",
      "business by business 0\n",
      "business upon business 0\n",
      "holiday to holiday 0\n",
      "holiday after holiday 0\n",
      "holiday by holiday 0\n",
      "holiday upon holiday 0\n",
      "door to door 54\n",
      "door after door 0\n",
      "door by door 0\n",
      "door upon door 0\n",
      "fast to fast 0\n",
      "fast after fast 0\n",
      "fast by fast 0\n",
      "fast upon fast 0\n",
      "whole to whole 0\n",
      "whole after whole 0\n",
      "whole by whole 0\n",
      "whole upon whole 0\n",
      "soul to soul 1\n",
      "soul after soul 0\n",
      "soul by soul 0\n",
      "soul upon soul 0\n",
      "girl to girl 1\n",
      "girl after girl 1\n",
      "girl by girl 0\n",
      "girl upon girl 0\n",
      "region to region 22\n",
      "region after region 0\n",
      "region by region 3\n",
      "region upon region 0\n",
      "catch to catch 0\n",
      "catch after catch 0\n",
      "catch by catch 0\n",
      "catch upon catch 0\n",
      "step to step 0\n",
      "step after step 4\n",
      "step by step 614\n",
      "step upon step 0\n",
      "hundred to hundred 1\n",
      "hundred after hundred 0\n",
      "hundred by hundred 0\n",
      "hundred upon hundred 0\n",
      "education to education 0\n",
      "education after education 0\n",
      "education by education 0\n",
      "education upon education 0\n",
      "personal to personal 0\n",
      "personal after personal 0\n",
      "personal by personal 0\n",
      "personal upon personal 0\n",
      "full to full 0\n",
      "full after full 0\n",
      "full by full 0\n",
      "full upon full 0\n",
      "crew to crew 1\n",
      "crew after crew 0\n",
      "crew by crew 0\n",
      "crew upon crew 0\n",
      "enemy to enemy 0\n",
      "enemy after enemy 0\n",
      "enemy by enemy 0\n",
      "enemy upon enemy 0\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd3d4cd0edd1b524"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
